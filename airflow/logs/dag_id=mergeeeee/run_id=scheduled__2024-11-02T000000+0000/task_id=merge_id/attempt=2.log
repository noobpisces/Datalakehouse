[2024-11-03T11:31:19.665+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mergeeeee.merge_id scheduled__2024-11-02T00:00:00+00:00 [queued]>
[2024-11-03T11:31:19.676+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mergeeeee.merge_id scheduled__2024-11-02T00:00:00+00:00 [queued]>
[2024-11-03T11:31:19.676+0000] {taskinstance.py:1359} INFO - Starting attempt 2 of 2
[2024-11-03T11:31:19.688+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): merge_id> on 2024-11-02 00:00:00+00:00
[2024-11-03T11:31:19.694+0000] {standard_task_runner.py:57} INFO - Started process 20137 to run task
[2024-11-03T11:31:19.697+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'mergeeeee', 'merge_id', 'scheduled__2024-11-02T00:00:00+00:00', '--job-id', '183', '--raw', '--subdir', 'DAGS_FOLDER/merge_hihi.py', '--cfg-path', '/tmp/tmpxv5q2frw']
[2024-11-03T11:31:19.700+0000] {standard_task_runner.py:85} INFO - Job 183: Subtask merge_id
[2024-11-03T11:31:19.751+0000] {task_command.py:415} INFO - Running <TaskInstance: mergeeeee.merge_id scheduled__2024-11-02T00:00:00+00:00 [running]> on host eb88dbfa1959
[2024-11-03T11:31:19.820+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='mergeeeee' AIRFLOW_CTX_TASK_ID='merge_id' AIRFLOW_CTX_EXECUTION_DATE='2024-11-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-02T00:00:00+00:00'
[2024-11-03T11:31:19.829+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2024-11-03T11:31:19.831+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --jars /opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/s3-2.18.41.jar,/opt/***/jars/aws-java-sdk-1.12.367.jar,/opt/***/jars/delta-core_2.12-2.4.0.jar,/opt/***/jars/delta-storage-2.2.0.jar, --packages org.apache.hadoop:hadoop-aws:3.3.4 --num-executors 2 --total-executor-cores 2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name arrow-spark --deploy-mode client /opt/***/jobs/python/test_merge.py s3a://lakehouse/bronze/keywords.parquet s3a://lakehouse/bronze/movies.parquet s3a://lakehouse/bronze/credits.parquet s3a://lakehouse/silver/merged_data s3a://lakehouse/gold/gold_data
[2024-11-03T11:31:19.919+0000] {spark_submit.py:579} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-11-03T11:31:21.767+0000] {spark_submit.py:579} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-03T11:31:21.935+0000] {spark_submit.py:579} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-03T11:31:21.936+0000] {spark_submit.py:579} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-03T11:31:21.944+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-11-03T11:31:21.946+0000] {spark_submit.py:579} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-3a3a025d-8027-4d07-9862-86e5a93bae5b;1.0
[2024-11-03T11:31:21.946+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-03T11:31:22.121+0000] {spark_submit.py:579} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in spark-list
[2024-11-03T11:31:22.151+0000] {spark_submit.py:579} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2024-11-03T11:31:22.176+0000] {spark_submit.py:579} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2024-11-03T11:31:22.201+0000] {spark_submit.py:579} INFO - :: resolution report :: resolve 245ms :: artifacts dl 11ms
[2024-11-03T11:31:22.202+0000] {spark_submit.py:579} INFO - :: modules in use:
[2024-11-03T11:31:22.204+0000] {spark_submit.py:579} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2024-11-03T11:31:22.205+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from spark-list in [default]
[2024-11-03T11:31:22.206+0000] {spark_submit.py:579} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2024-11-03T11:31:22.207+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-03T11:31:22.208+0000] {spark_submit.py:579} INFO - |                  |            modules            ||   artifacts   |
[2024-11-03T11:31:22.209+0000] {spark_submit.py:579} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-03T11:31:22.210+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-03T11:31:22.211+0000] {spark_submit.py:579} INFO - |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2024-11-03T11:31:22.212+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-03T11:31:22.212+0000] {spark_submit.py:579} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-3a3a025d-8027-4d07-9862-86e5a93bae5b
[2024-11-03T11:31:22.213+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-03T11:31:22.222+0000] {spark_submit.py:579} INFO - 0 artifacts copied, 3 already retrieved (0kB/10ms)
[2024-11-03T11:31:22.437+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-03T11:31:24.289+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkContext: Running Spark version 3.4.3
[2024-11-03T11:31:24.317+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceUtils: ==============================================================
[2024-11-03T11:31:24.318+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-03T11:31:24.318+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceUtils: ==============================================================
[2024-11-03T11:31:24.319+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkContext: Submitted application: MergeData
[2024-11-03T11:31:24.344+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-03T11:31:24.357+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2024-11-03T11:31:24.361+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-03T11:31:24.435+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SecurityManager: Changing view acls to: ***
[2024-11-03T11:31:24.437+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SecurityManager: Changing modify acls to: ***
[2024-11-03T11:31:24.438+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SecurityManager: Changing view acls groups to:
[2024-11-03T11:31:24.440+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SecurityManager: Changing modify acls groups to:
[2024-11-03T11:31:24.441+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-03T11:31:24.795+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO Utils: Successfully started service 'sparkDriver' on port 45529.
[2024-11-03T11:31:24.833+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkEnv: Registering MapOutputTracker
[2024-11-03T11:31:24.874+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-03T11:31:24.893+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-03T11:31:24.894+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-03T11:31:24.899+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-03T11:31:24.922+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e6489671-52aa-4deb-8e56-c4639890eca2
[2024-11-03T11:31:24.937+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-03T11:31:24.959+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:24 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-03T11:31:25.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-03T11:31:25.177+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-03T11:31:25.221+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///opt/***/jars/hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:45529/jars/hadoop-aws-3.3.4.jar with timestamp 1730633484280
[2024-11-03T11:31:25.224+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///opt/***/jars/s3-2.18.41.jar at spark://eb88dbfa1959:45529/jars/s3-2.18.41.jar with timestamp 1730633484280
[2024-11-03T11:31:25.226+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///opt/***/jars/aws-java-sdk-1.12.367.jar at spark://eb88dbfa1959:45529/jars/aws-java-sdk-1.12.367.jar with timestamp 1730633484280
[2024-11-03T11:31:25.228+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///opt/***/jars/delta-core_2.12-2.4.0.jar at spark://eb88dbfa1959:45529/jars/delta-core_2.12-2.4.0.jar with timestamp 1730633484280
[2024-11-03T11:31:25.229+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///opt/***/jars/delta-storage-2.2.0.jar at spark://eb88dbfa1959:45529/jars/delta-storage-2.2.0.jar with timestamp 1730633484280
[2024-11-03T11:31:25.230+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:45529/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1730633484280
[2024-11-03T11:31:25.231+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://eb88dbfa1959:45529/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1730633484280
[2024-11-03T11:31:25.231+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://eb88dbfa1959:45529/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1730633484280
[2024-11-03T11:31:25.233+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:45529/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1730633484280
[2024-11-03T11:31:25.235+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-3af8ea18-8bc9-4a0b-ac5c-336c18568c93/userFiles-2ad3eb66-be69-4e7a-a945-1684797f88b8/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-11-03T11:31:25.247+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://eb88dbfa1959:45529/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1730633484280
[2024-11-03T11:31:25.248+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-3af8ea18-8bc9-4a0b-ac5c-336c18568c93/userFiles-2ad3eb66-be69-4e7a-a945-1684797f88b8/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2024-11-03T11:31:25.686+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://eb88dbfa1959:45529/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1730633484280
[2024-11-03T11:31:25.687+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-3af8ea18-8bc9-4a0b-ac5c-336c18568c93/userFiles-2ad3eb66-be69-4e7a-a945-1684797f88b8/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-11-03T11:31:25.892+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-03T11:31:26.009+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 72 ms (0 ms spent in bootstraps)
[2024-11-03T11:31:26.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241103113126-0033
[2024-11-03T11:31:26.174+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241103113126-0033/0 on worker-20241103094523-172.18.0.8-41531 (172.18.0.8:41531) with 2 core(s)
[2024-11-03T11:31:26.179+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20241103113126-0033/0 on hostPort 172.18.0.8:41531 with 2 core(s), 2.0 GiB RAM
[2024-11-03T11:31:26.191+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43169.
[2024-11-03T11:31:26.192+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO NettyBlockTransferService: Server created on eb88dbfa1959:43169
[2024-11-03T11:31:26.197+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-03T11:31:26.219+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, eb88dbfa1959, 43169, None)
[2024-11-03T11:31:26.227+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO BlockManagerMasterEndpoint: Registering block manager eb88dbfa1959:43169 with 434.4 MiB RAM, BlockManagerId(driver, eb88dbfa1959, 43169, None)
[2024-11-03T11:31:26.238+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, eb88dbfa1959, 43169, None)
[2024-11-03T11:31:26.241+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, eb88dbfa1959, 43169, None)
[2024-11-03T11:31:26.326+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241103113126-0033/0 is now RUNNING
[2024-11-03T11:31:26.701+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-03T11:31:27.322+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-03T11:31:27.329+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:27 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-03T11:31:28.916+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2024-11-03T11:31:28.932+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:28 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2024-11-03T11:31:28.933+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:28 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2024-11-03T11:31:29.642+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:29 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:47292) with ID 0,  ResourceProfileId 0
[2024-11-03T11:31:29.759+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:29 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:37445 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.8, 37445, None)
[2024-11-03T11:31:30.783+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:30 INFO InMemoryFileIndex: It took 100 ms to list leaf files for 1 paths.
[2024-11-03T11:31:33.428+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:33 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2024-11-03T11:31:34.003+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:34.046+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-03T11:31:34.047+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:31:34.047+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:34.049+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:34.055+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:31:34.115+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 107.0 KiB, free 434.3 MiB)
[2024-11-03T11:31:34.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.3 MiB)
[2024-11-03T11:31:34.177+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on eb88dbfa1959:43169 (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-03T11:31:34.183+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:34.207+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:31:34.208+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-03T11:31:34.271+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7499 bytes)
[2024-11-03T11:31:34.534+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:37445 (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:36.456+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2207 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:31:36.459+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-03T11:31:36.467+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 2.393 s
[2024-11-03T11:31:36.472+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:31:36.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-03T11:31:36.476+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:36 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 2.468236 s
[2024-11-03T11:31:37.025+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on eb88dbfa1959:43169 in memory (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-03T11:31:37.031+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.8:37445 in memory (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:37.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO InMemoryFileIndex: It took 13 ms to list leaf files for 1 paths.
[2024-11-03T11:31:37.649+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:37.651+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Got job 1 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-03T11:31:37.652+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Final stage: ResultStage 1 (load at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:31:37.653+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:37.653+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:37.654+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:31:37.663+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 107.0 KiB, free 434.3 MiB)
[2024-11-03T11:31:37.671+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.3 MiB)
[2024-11-03T11:31:37.672+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on eb88dbfa1959:43169 (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-03T11:31:37.673+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:37.675+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:31:37.676+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-11-03T11:31:37.678+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7500 bytes)
[2024-11-03T11:31:37.714+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:37445 (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:37.790+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 113 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:31:37.792+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-03T11:31:37.793+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: ResultStage 1 (load at NativeMethodAccessorImpl.java:0) finished in 0.136 s
[2024-11-03T11:31:37.793+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:31:37.794+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-03T11:31:37.795+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:37 INFO DAGScheduler: Job 1 finished: load at NativeMethodAccessorImpl.java:0, took 0.144793 s
[2024-11-03T11:31:38.801+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on eb88dbfa1959:43169 in memory (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-03T11:31:38.803+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.8:37445 in memory (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:38.912+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:38 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:31:38.914+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:38 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:31:39.557+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO CodeGenerator: Code generated in 415.962445 ms
[2024-11-03T11:31:39.603+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 206.4 KiB, free 434.2 MiB)
[2024-11-03T11:31:39.625+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 434.2 MiB)
[2024-11-03T11:31:39.627+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on eb88dbfa1959:43169 (size: 36.6 KiB, free: 434.4 MiB)
[2024-11-03T11:31:39.629+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:39.665+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:31:39.735+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:39.737+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-03T11:31:39.738+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:31:39.739+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:39.742+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:39.743+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:31:39.845+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.4 KiB, free 434.1 MiB)
[2024-11-03T11:31:39.856+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2024-11-03T11:31:39.858+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on eb88dbfa1959:43169 (size: 12.2 KiB, free: 434.4 MiB)
[2024-11-03T11:31:39.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:39.861+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:31:39.862+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-11-03T11:31:39.869+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7922 bytes)
[2024-11-03T11:31:39.920+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.8:37445 (size: 12.2 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:40.769+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:40 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.8:37445 (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:42.319+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2454 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:31:42.320+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-03T11:31:42.321+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 2.572 s
[2024-11-03T11:31:42.322+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:31:42.323+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-11-03T11:31:42.323+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 2.587315 s
[2024-11-03T11:31:42.371+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO CodeGenerator: Code generated in 22.154228 ms
[2024-11-03T11:31:42.435+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-03T11:31:42.436+0000] {spark_submit.py:579} INFO - |id   |cast_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
[2024-11-03T11:31:42.436+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-03T11:31:42.437+0000] {spark_submit.py:579} INFO - |862  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.437+0000] {spark_submit.py:579} INFO - |8844 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.437+0000] {spark_submit.py:579} INFO - |15602|[Walter Matthau, Jack Lemmon, Ann-Margret, Sophia Loren, Daryl Hannah, Burgess Meredith, Kevin Pollak]                                                                                                                                                                                                                                                                                                                                                                                                            |
[2024-11-03T11:31:42.438+0000] {spark_submit.py:579} INFO - |31357|[Whitney Houston, Angela Bassett, Loretta Devine, Lela Rochon, Gregory Hines, Dennis Haysbert, Michael Beach, Mykelti Williamson, Lamont Johnson, Wesley Snipes]                                                                                                                                                                                                                                                                                                                                                  |
[2024-11-03T11:31:42.439+0000] {spark_submit.py:579} INFO - |11862|[Steve Martin, Diane Keaton, Martin Short, Kimberly Williams-Paisley, George Newbern, Kieran Culkin, BD Wong, Peter Michael Goetz, Kate McGregor-Stewart, Jane Adams, Eugene Levy, Lori Alan]                                                                                                                                                                                                                                                                                                                     |
[2024-11-03T11:31:42.439+0000] {spark_submit.py:579} INFO - |949  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.439+0000] {spark_submit.py:579} INFO - |11860|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.440+0000] {spark_submit.py:579} INFO - |45325|[Jonathan Taylor Thomas, Brad Renfro, Rachael Leigh Cook, Michael McShane, Amy Wright, Eric Schweig, Tamara Mello]                                                                                                                                                                                                                                                                                                                                                                                                |
[2024-11-03T11:31:42.440+0000] {spark_submit.py:579} INFO - |9091 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.441+0000] {spark_submit.py:579} INFO - |710  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.441+0000] {spark_submit.py:579} INFO - |9087 |[Michael Douglas, Annette Bening, Michael J. Fox, Martin Sheen, Anna Deavere Smith, Shawna Waldron, Samantha Mathis, David Paymer, Richard Dreyfuss, Nina Siemaszko, Wendie Malick, Beau Billingslea, Gail Strickland, Joshua Malina, Clement von Franckenstein, John Mahoney, John Mahon, Gabriel Jarret]                                                                                                                                                                                                        |
[2024-11-03T11:31:42.441+0000] {spark_submit.py:579} INFO - |12110|[Leslie Nielsen, Mel Brooks, Amy Yasbeck, Peter MacNicol, Lysette Anthony, Harvey Korman, Steven Weber, Mark Blankfield, Megan Cavanagh, Gregg Binkley, Anne Bancroft]                                                                                                                                                                                                                                                                                                                                            |
[2024-11-03T11:31:42.441+0000] {spark_submit.py:579} INFO - |21032|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.442+0000] {spark_submit.py:579} INFO - |10858|[Anthony Hopkins, Joan Allen, Powers Boothe, Ed Harris, Bob Hoskins, E.G. Marshall, David Paymer, David Hyde Pierce, Paul Sorvino, Mary Steenburgen, J.T. Walsh, James Woods, Brian Bedford, Kevin Dunn, Fyvush Finkel, Annabeth Gish, Larry Hagman, Madeline Kahn, Dan Hedaya, Bridgette Wilson, Tom Bower, Tony Goldwyn, Edward Herrmann, Tony Lo Bianco, Saul Rubinek, Robert Beltran, John Cunningham, John Diehl, John C. McGinley, Michael Chiklis, Ric Young, Boris Sichkin, Sam Waterston, Marley Shelton]|
[2024-11-03T11:31:42.442+0000] {spark_submit.py:579} INFO - |1408 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.442+0000] {spark_submit.py:579} INFO - |524  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.443+0000] {spark_submit.py:579} INFO - |4584 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.443+0000] {spark_submit.py:579} INFO - |5    |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.444+0000] {spark_submit.py:579} INFO - |9273 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.444+0000] {spark_submit.py:579} INFO - |11517|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-03T11:31:42.444+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-03T11:31:42.445+0000] {spark_submit.py:579} INFO - only showing top 20 rows
[2024-11-03T11:31:42.446+0000] {spark_submit.py:579} INFO - 
[2024-11-03T11:31:42.790+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO DeltaLog: Loading version 2.
[2024-11-03T11:31:42.907+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on eb88dbfa1959:43169 in memory (size: 12.2 KiB, free: 434.4 MiB)
[2024-11-03T11:31:42.910+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.8:37445 in memory (size: 12.2 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:42.921+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on eb88dbfa1959:43169 in memory (size: 36.6 KiB, free: 434.4 MiB)
[2024-11-03T11:31:42.923+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:42 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.8:37445 in memory (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:43.446+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 3, totalFileSize: 6544)
[2024-11-03T11:31:43.629+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:31:43.630+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:31:43.832+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO CodeGenerator: Code generated in 130.579029 ms
[2024-11-03T11:31:43.836+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 204.8 KiB, free 434.2 MiB)
[2024-11-03T11:31:43.851+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)
[2024-11-03T11:31:43.853+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 434.4 MiB)
[2024-11-03T11:31:43.855+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO SparkContext: Created broadcast 4 from toString at String.java:2951
[2024-11-03T11:31:43.887+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6294728 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:31:43.941+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-03T11:31:43.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Got job 3 (toString at String.java:2951) with 2 output partitions
[2024-11-03T11:31:43.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Final stage: ResultStage 3 (toString at String.java:2951)
[2024-11-03T11:31:43.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:43.944+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:43.945+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[12] at toString at String.java:2951), which has no missing parents
[2024-11-03T11:31:43.947+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 50.4 KiB, free 434.1 MiB)
[2024-11-03T11:31:43.955+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 434.1 MiB)
[2024-11-03T11:31:43.956+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on eb88dbfa1959:43169 (size: 15.6 KiB, free: 434.3 MiB)
[2024-11-03T11:31:43.957+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:43.958+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:31:43.958+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2024-11-03T11:31:43.960+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8078 bytes)
[2024-11-03T11:31:43.961+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7955 bytes)
[2024-11-03T11:31:43.992+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:43 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.8:37445 (size: 15.6 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:44.238+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.8:37445 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:31:44.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 512 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:31:44.811+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 852 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:31:44.813+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-11-03T11:31:44.814+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO DAGScheduler: ResultStage 3 (toString at String.java:2951) finished in 0.867 s
[2024-11-03T11:31:44.815+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:31:44.816+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-11-03T11:31:44.817+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO DAGScheduler: Job 3 finished: toString at String.java:2951, took 0.872835 s
[2024-11-03T11:31:44.888+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO CodeGenerator: Code generated in 49.46043 ms
[2024-11-03T11:31:44.894+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:44 INFO Snapshot: [tableId=20a0bfb0-53a8-48c5-97b8-c032a6c76c87] Created snapshot Snapshot(path=s3a://lakehouse/silver/merged_data/_delta_log, version=2, metadata=Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292)), logSegment=LogSegment(s3a://lakehouse/silver/merged_data/_delta_log,2,WrappedArray(S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1939; replication=1; blocksize=33554432; modification_time=1730633270778; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ca3327c2494957c46b6efdff87392535 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=1572; replication=1; blocksize=33554432; modification_time=1730633353899; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=499135ae9e973681b348d18a25ba2f7a versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633363129; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=797865593c6dd74174b6a29b3e403bc4 versionId=null),None,1730633363129), checksumOpt=None)
[2024-11-03T11:31:45.213+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO OptimisticTransaction: [tableId=34f0700a,txnId=616047f6] Updated metadata from - to Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292))
[2024-11-03T11:31:45.224+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on eb88dbfa1959:43169 in memory (size: 15.6 KiB, free: 434.4 MiB)
[2024-11-03T11:31:45.227+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.8:37445 in memory (size: 15.6 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:45.700+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:31:45.700+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#802, Some(Etc/UTC)), lambdafunction(lambda x#814.name, lambda x#814, false)))
[2024-11-03T11:31:45.817+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-03T11:31:45.875+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-03T11:31:45.949+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO CodeGenerator: Code generated in 55.564267 ms
[2024-11-03T11:31:45.961+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO CodeGenerator: Code generated in 9.290575 ms
[2024-11-03T11:31:45.965+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 206.5 KiB, free 434.0 MiB)
[2024-11-03T11:31:46.022+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.9 MiB)
[2024-11-03T11:31:46.058+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.8:37445 in memory (size: 36.0 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:46.102+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on eb88dbfa1959:43169 (size: 36.6 KiB, free: 434.3 MiB)
[2024-11-03T11:31:46.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on eb88dbfa1959:43169 in memory (size: 36.0 KiB, free: 434.4 MiB)
[2024-11-03T11:31:46.104+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO SparkContext: Created broadcast 6 from save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:46.106+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:31:46.374+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Registering RDD 20 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-11-03T11:31:46.382+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Got map stage job 4 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:31:46.382+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:31:46.384+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:46.392+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:46.395+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:31:46.419+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 45.0 KiB, free 434.1 MiB)
[2024-11-03T11:31:46.437+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 434.1 MiB)
[2024-11-03T11:31:46.441+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on eb88dbfa1959:43169 (size: 18.8 KiB, free: 434.3 MiB)
[2024-11-03T11:31:46.442+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:46.444+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:31:46.445+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2024-11-03T11:31:46.450+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-03T11:31:46.451+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-03T11:31:46.504+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.8:37445 (size: 18.8 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:46.955+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.8:37445 (size: 36.6 KiB, free: 1048.7 MiB)
[2024-11-03T11:31:47.262+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:47 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 6) in 812 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:31:53.408+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 6960 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:31:53.409+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-11-03T11:31:53.409+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: ShuffleMapStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 7.008 s
[2024-11-03T11:31:53.410+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:31:53.410+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: running: Set()
[2024-11-03T11:31:53.411+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:31:53.411+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: failed: Set()
[2024-11-03T11:31:53.440+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 5353534, minimum partition size: 1048576
[2024-11-03T11:31:53.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-03T11:31:53.504+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO CodeGenerator: Code generated in 25.332909 ms
[2024-11-03T11:31:53.615+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:31:53.617+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:31:53.618+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:31:53.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2024-11-03T11:31:53.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:53.621+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[22] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:31:53.655+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on eb88dbfa1959:43169 in memory (size: 18.8 KiB, free: 434.4 MiB)
[2024-11-03T11:31:53.657+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.8:37445 in memory (size: 18.8 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:53.668+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 368.5 KiB, free 433.8 MiB)
[2024-11-03T11:31:53.677+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 134.9 KiB, free 433.7 MiB)
[2024-11-03T11:31:53.696+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on eb88dbfa1959:43169 (size: 134.9 KiB, free: 434.2 MiB)
[2024-11-03T11:31:53.697+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:53.698+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:31:53.699+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2024-11-03T11:31:53.718+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:31:53.719+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8) (172.18.0.8, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:31:53.742+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.8:37445 (size: 134.9 KiB, free: 1048.6 MiB)
[2024-11-03T11:31:53.930+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.8:47292
[2024-11-03T11:31:57.438+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 3722 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:31:57.439+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 3720 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:31:57.440+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-11-03T11:31:57.443+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 3.805 s
[2024-11-03T11:31:57.444+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:31:57.445+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2024-11-03T11:31:57.445+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 3.829482 s
[2024-11-03T11:31:57.448+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO FileFormatWriter: Start to commit write Job 7b5b0a92-9f65-4375-8436-f23639a424c1.
[2024-11-03T11:31:57.451+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO FileFormatWriter: Write Job 7b5b0a92-9f65-4375-8436-f23639a424c1 committed. Elapsed time: 2 ms.
[2024-11-03T11:31:57.457+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO FileFormatWriter: Finished processing stats for write job 7b5b0a92-9f65-4375-8436-f23639a424c1.
[2024-11-03T11:31:57.479+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Compute snapshot for version: 2
[2024-11-03T11:31:57.494+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 204.5 KiB, free 433.5 MiB)
[2024-11-03T11:31:57.512+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on eb88dbfa1959:43169 in memory (size: 134.9 KiB, free: 434.4 MiB)
[2024-11-03T11:31:57.514+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)
[2024-11-03T11:31:57.515+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.8:37445 in memory (size: 134.9 KiB, free: 1048.8 MiB)
[2024-11-03T11:31:57.517+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-03T11:31:57.519+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:57 INFO SparkContext: Created broadcast 9 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:31:58.145+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:31:58.146+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:31:58.175+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-11-03T11:31:58.449+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO CodeGenerator: Code generated in 189.065368 ms
[2024-11-03T11:31:58.453+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 204.8 KiB, free 433.7 MiB)
[2024-11-03T11:31:58.467+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.7 MiB)
[2024-11-03T11:31:58.471+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-03T11:31:58.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO SparkContext: Created broadcast 10 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:31:58.476+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6294728 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:31:58.512+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Registering RDD 26 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2024-11-03T11:31:58.513+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Got map stage job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-03T11:31:58.514+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Final stage: ShuffleMapStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:31:58.514+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:31:58.516+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:31:58.517+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[26] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:31:58.535+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 138.3 KiB, free 433.6 MiB)
[2024-11-03T11:31:58.539+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.5 MiB)
[2024-11-03T11:31:58.543+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on eb88dbfa1959:43169 (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-03T11:31:58.544+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:31:58.546+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[26] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:31:58.547+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSchedulerImpl: Adding task set 7.0 with 2 tasks resource profile 0
[2024-11-03T11:31:58.549+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8067 bytes)
[2024-11-03T11:31:58.550+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 10) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7944 bytes)
[2024-11-03T11:31:58.569+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.8:37445 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:31:58.917+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.8:37445 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:31:58.969+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 10) in 419 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:31:58.982+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 433 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:31:58.983+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-11-03T11:31:58.983+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: ShuffleMapStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.465 s
[2024-11-03T11:31:58.984+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:31:58.985+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: running: Set()
[2024-11-03T11:31:58.986+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:31:58.987+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:58 INFO DAGScheduler: failed: Set()
[2024-11-03T11:31:59.048+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:59 INFO BlockManagerInfo: Removed broadcast_11_piece0 on eb88dbfa1959:43169 in memory (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-03T11:31:59.050+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:59 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.8:37445 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:31:59.524+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:59 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 19938 bytes
[2024-11-03T11:31:59.525+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:59 INFO CodeGenerator: Code generated in 369.818434 ms
[2024-11-03T11:31:59.694+0000] {spark_submit.py:579} INFO - 24/11/03 11:31:59 INFO CodeGenerator: Code generated in 114.207935 ms
[2024-11-03T11:32:00.051+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO CodeGenerator: Code generated in 59.982618 ms
[2024-11-03T11:32:00.061+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Registering RDD 36 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2024-11-03T11:32:00.061+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Got map stage job 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-03T11:32:00.062+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Final stage: ShuffleMapStage 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:00.063+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2024-11-03T11:32:00.069+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:00.070+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[36] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:00.208+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 517.0 KiB, free 433.2 MiB)
[2024-11-03T11:32:00.212+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 433.1 MiB)
[2024-11-03T11:32:00.215+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on eb88dbfa1959:43169 (size: 123.3 KiB, free: 434.2 MiB)
[2024-11-03T11:32:00.216+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:00.218+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[36] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-03T11:32:00.219+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 50 tasks resource profile 0
[2024-11-03T11:32:00.221+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO TaskSetManager: Starting task 8.0 in stage 9.0 (TID 11) (172.18.0.8, executor 0, partition 8, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:00.222+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO TaskSetManager: Starting task 32.0 in stage 9.0 (TID 12) (172.18.0.8, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:00.246+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.8:37445 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:00.381+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.8:47292
[2024-11-03T11:32:00.878+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO BlockManagerInfo: Added rdd_33_8 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:00.879+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:00 INFO BlockManagerInfo: Added rdd_33_32 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.099+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 42.0 in stage 9.0 (TID 13) (172.18.0.8, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.101+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 8.0 in stage 9.0 (TID 11) in 881 ms on 172.18.0.8 (executor 0) (1/50)
[2024-11-03T11:32:01.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 44.0 in stage 9.0 (TID 14) (172.18.0.8, executor 0, partition 44, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.105+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 32.0 in stage 9.0 (TID 12) in 883 ms on 172.18.0.8 (executor 0) (2/50)
[2024-11-03T11:32:01.179+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_44 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.214+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 48.0 in stage 9.0 (TID 15) (172.18.0.8, executor 0, partition 48, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.215+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 44.0 in stage 9.0 (TID 14) in 113 ms on 172.18.0.8 (executor 0) (3/50)
[2024-11-03T11:32:01.296+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_48 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.333+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 16) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.334+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 48.0 in stage 9.0 (TID 15) in 119 ms on 172.18.0.8 (executor 0) (4/50)
[2024-11-03T11:32:01.420+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_0 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.455+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 17) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.456+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 16) in 123 ms on 172.18.0.8 (executor 0) (5/50)
[2024-11-03T11:32:01.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_42 in memory on 172.18.0.8:37445 (size: 811.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.517+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 18) (172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.519+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 42.0 in stage 9.0 (TID 13) in 419 ms on 172.18.0.8 (executor 0) (6/50)
[2024-11-03T11:32:01.529+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_1 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.578+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 19) (172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.579+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 17) in 125 ms on 172.18.0.8 (executor 0) (7/50)
[2024-11-03T11:32:01.600+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_2 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.639+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 20) (172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.640+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 18) in 124 ms on 172.18.0.8 (executor 0) (8/50)
[2024-11-03T11:32:01.650+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_3 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.689+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 21) (172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.694+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 19) in 111 ms on 172.18.0.8 (executor 0) (9/50)
[2024-11-03T11:32:01.724+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_4 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.775+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 22) (172.18.0.8, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.776+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 20) in 136 ms on 172.18.0.8 (executor 0) (10/50)
[2024-11-03T11:32:01.783+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_5 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.825+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 23) (172.18.0.8, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.826+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 21) in 137 ms on 172.18.0.8 (executor 0) (11/50)
[2024-11-03T11:32:01.851+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_6 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.907+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 9.0 in stage 9.0 (TID 24) (172.18.0.8, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.908+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 22) in 134 ms on 172.18.0.8 (executor 0) (12/50)
[2024-11-03T11:32:01.909+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO BlockManagerInfo: Added rdd_33_7 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:01.971+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Starting task 10.0 in stage 9.0 (TID 25) (172.18.0.8, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:01.972+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:01 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 23) in 148 ms on 172.18.0.8 (executor 0) (13/50)
[2024-11-03T11:32:02.023+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_9 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.083+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_10 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.090+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 11.0 in stage 9.0 (TID 26) (172.18.0.8, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.092+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 9.0 in stage 9.0 (TID 24) in 185 ms on 172.18.0.8 (executor 0) (14/50)
[2024-11-03T11:32:02.157+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 12.0 in stage 9.0 (TID 27) (172.18.0.8, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.158+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 10.0 in stage 9.0 (TID 25) in 187 ms on 172.18.0.8 (executor 0) (15/50)
[2024-11-03T11:32:02.192+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_11 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.243+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 13.0 in stage 9.0 (TID 28) (172.18.0.8, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.245+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 11.0 in stage 9.0 (TID 26) in 155 ms on 172.18.0.8 (executor 0) (16/50)
[2024-11-03T11:32:02.256+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_12 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.295+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 14.0 in stage 9.0 (TID 29) (172.18.0.8, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.297+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 12.0 in stage 9.0 (TID 27) in 140 ms on 172.18.0.8 (executor 0) (17/50)
[2024-11-03T11:32:02.322+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_13 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.385+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 15.0 in stage 9.0 (TID 30) (172.18.0.8, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.386+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 13.0 in stage 9.0 (TID 28) in 143 ms on 172.18.0.8 (executor 0) (18/50)
[2024-11-03T11:32:02.397+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_14 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.438+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 16.0 in stage 9.0 (TID 31) (172.18.0.8, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.440+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 14.0 in stage 9.0 (TID 29) in 144 ms on 172.18.0.8 (executor 0) (19/50)
[2024-11-03T11:32:02.469+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_15 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.515+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_16 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.517+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 17.0 in stage 9.0 (TID 32) (172.18.0.8, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.518+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 15.0 in stage 9.0 (TID 30) in 134 ms on 172.18.0.8 (executor 0) (20/50)
[2024-11-03T11:32:02.556+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 18.0 in stage 9.0 (TID 33) (172.18.0.8, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.558+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 16.0 in stage 9.0 (TID 31) in 119 ms on 172.18.0.8 (executor 0) (21/50)
[2024-11-03T11:32:02.590+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_17 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_18 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.621+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 19.0 in stage 9.0 (TID 34) (172.18.0.8, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.622+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 17.0 in stage 9.0 (TID 32) in 106 ms on 172.18.0.8 (executor 0) (22/50)
[2024-11-03T11:32:02.661+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 20.0 in stage 9.0 (TID 35) (172.18.0.8, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.662+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 18.0 in stage 9.0 (TID 33) in 106 ms on 172.18.0.8 (executor 0) (23/50)
[2024-11-03T11:32:02.689+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_19 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.727+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 21.0 in stage 9.0 (TID 36) (172.18.0.8, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.728+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 19.0 in stage 9.0 (TID 34) in 107 ms on 172.18.0.8 (executor 0) (24/50)
[2024-11-03T11:32:02.732+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_20 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.764+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 22.0 in stage 9.0 (TID 37) (172.18.0.8, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.765+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 20.0 in stage 9.0 (TID 35) in 104 ms on 172.18.0.8 (executor 0) (25/50)
[2024-11-03T11:32:02.793+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_21 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.832+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_22 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.837+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 23.0 in stage 9.0 (TID 38) (172.18.0.8, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.838+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 21.0 in stage 9.0 (TID 36) in 112 ms on 172.18.0.8 (executor 0) (26/50)
[2024-11-03T11:32:02.875+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 24.0 in stage 9.0 (TID 39) (172.18.0.8, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.876+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 22.0 in stage 9.0 (TID 37) in 112 ms on 172.18.0.8 (executor 0) (27/50)
[2024-11-03T11:32:02.901+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_23 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.933+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 25.0 in stage 9.0 (TID 40) (172.18.0.8, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.934+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_24 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:02.935+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 23.0 in stage 9.0 (TID 38) in 97 ms on 172.18.0.8 (executor 0) (28/50)
[2024-11-03T11:32:02.963+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Starting task 26.0 in stage 9.0 (TID 41) (172.18.0.8, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:02.964+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO TaskSetManager: Finished task 24.0 in stage 9.0 (TID 39) in 90 ms on 172.18.0.8 (executor 0) (29/50)
[2024-11-03T11:32:02.988+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:02 INFO BlockManagerInfo: Added rdd_33_25 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.013+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 27.0 in stage 9.0 (TID 42) (172.18.0.8, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.014+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 25.0 in stage 9.0 (TID 40) in 80 ms on 172.18.0.8 (executor 0) (30/50)
[2024-11-03T11:32:03.023+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_26 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.049+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 28.0 in stage 9.0 (TID 43) (172.18.0.8, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.050+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 26.0 in stage 9.0 (TID 41) in 86 ms on 172.18.0.8 (executor 0) (31/50)
[2024-11-03T11:32:03.067+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_27 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.094+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 29.0 in stage 9.0 (TID 44) (172.18.0.8, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.095+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 27.0 in stage 9.0 (TID 42) in 82 ms on 172.18.0.8 (executor 0) (32/50)
[2024-11-03T11:32:03.100+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_28 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.126+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 30.0 in stage 9.0 (TID 45) (172.18.0.8, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.127+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 28.0 in stage 9.0 (TID 43) in 79 ms on 172.18.0.8 (executor 0) (33/50)
[2024-11-03T11:32:03.143+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_29 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.168+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 31.0 in stage 9.0 (TID 46) (172.18.0.8, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.169+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 29.0 in stage 9.0 (TID 44) in 75 ms on 172.18.0.8 (executor 0) (34/50)
[2024-11-03T11:32:03.174+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_30 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.203+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 33.0 in stage 9.0 (TID 47) (172.18.0.8, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.204+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 30.0 in stage 9.0 (TID 45) in 77 ms on 172.18.0.8 (executor 0) (35/50)
[2024-11-03T11:32:03.221+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_31 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.248+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 34.0 in stage 9.0 (TID 48) (172.18.0.8, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.249+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 31.0 in stage 9.0 (TID 46) in 80 ms on 172.18.0.8 (executor 0) (36/50)
[2024-11-03T11:32:03.255+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_33 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.281+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 35.0 in stage 9.0 (TID 49) (172.18.0.8, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.282+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 33.0 in stage 9.0 (TID 47) in 80 ms on 172.18.0.8 (executor 0) (37/50)
[2024-11-03T11:32:03.295+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_34 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.331+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_35 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.373+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 36.0 in stage 9.0 (TID 50) (172.18.0.8, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.374+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 34.0 in stage 9.0 (TID 48) in 127 ms on 172.18.0.8 (executor 0) (38/50)
[2024-11-03T11:32:03.385+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 37.0 in stage 9.0 (TID 51) (172.18.0.8, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.386+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 35.0 in stage 9.0 (TID 49) in 105 ms on 172.18.0.8 (executor 0) (39/50)
[2024-11-03T11:32:03.430+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_36 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.441+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_37 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.465+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 38.0 in stage 9.0 (TID 52) (172.18.0.8, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.465+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 36.0 in stage 9.0 (TID 50) in 92 ms on 172.18.0.8 (executor 0) (40/50)
[2024-11-03T11:32:03.470+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 39.0 in stage 9.0 (TID 53) (172.18.0.8, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.471+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 37.0 in stage 9.0 (TID 51) in 87 ms on 172.18.0.8 (executor 0) (41/50)
[2024-11-03T11:32:03.529+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_39 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.530+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_38 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.559+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 40.0 in stage 9.0 (TID 54) (172.18.0.8, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.560+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 39.0 in stage 9.0 (TID 53) in 89 ms on 172.18.0.8 (executor 0) (42/50)
[2024-11-03T11:32:03.563+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 41.0 in stage 9.0 (TID 55) (172.18.0.8, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.564+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 38.0 in stage 9.0 (TID 52) in 100 ms on 172.18.0.8 (executor 0) (43/50)
[2024-11-03T11:32:03.617+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_40 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_41 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.648+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 43.0 in stage 9.0 (TID 56) (172.18.0.8, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.649+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 40.0 in stage 9.0 (TID 54) in 91 ms on 172.18.0.8 (executor 0) (44/50)
[2024-11-03T11:32:03.652+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 45.0 in stage 9.0 (TID 57) (172.18.0.8, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.653+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 41.0 in stage 9.0 (TID 55) in 89 ms on 172.18.0.8 (executor 0) (45/50)
[2024-11-03T11:32:03.695+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_43 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.699+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_45 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.722+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 46.0 in stage 9.0 (TID 58) (172.18.0.8, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.723+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 43.0 in stage 9.0 (TID 56) in 75 ms on 172.18.0.8 (executor 0) (46/50)
[2024-11-03T11:32:03.725+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 47.0 in stage 9.0 (TID 59) (172.18.0.8, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.726+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 45.0 in stage 9.0 (TID 57) in 73 ms on 172.18.0.8 (executor 0) (47/50)
[2024-11-03T11:32:03.773+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_46 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.781+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_47 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.811+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 49.0 in stage 9.0 (TID 60) (172.18.0.8, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:03.812+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 46.0 in stage 9.0 (TID 58) in 90 ms on 172.18.0.8 (executor 0) (48/50)
[2024-11-03T11:32:03.817+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 47.0 in stage 9.0 (TID 59) in 92 ms on 172.18.0.8 (executor 0) (49/50)
[2024-11-03T11:32:03.879+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added rdd_33_49 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:03.908+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Finished task 49.0 in stage 9.0 (TID 60) in 97 ms on 172.18.0.8 (executor 0) (50/50)
[2024-11-03T11:32:03.909+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-11-03T11:32:03.910+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: ShuffleMapStage 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 3.834 s
[2024-11-03T11:32:03.911+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:03.912+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:03.913+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:03.914+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:03.948+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:03.950+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Got job 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-03T11:32:03.951+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:03.952+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
[2024-11-03T11:32:03.952+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:03.953+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:03.957+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 452.2 KiB, free 432.6 MiB)
[2024-11-03T11:32:03.963+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 432.5 MiB)
[2024-11-03T11:32:03.966+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on eb88dbfa1959:43169 (size: 108.1 KiB, free: 434.1 MiB)
[2024-11-03T11:32:03.966+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:03.967+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:32:03.967+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2024-11-03T11:32:03.969+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 61) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:32:03.981+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.8:37445 (size: 108.1 KiB, free: 1048.5 MiB)
[2024-11-03T11:32:03.995+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.8:47292
[2024-11-03T11:32:04.068+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 61) in 100 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:32:04.069+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-11-03T11:32:04.069+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: ResultStage 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.116 s
[2024-11-03T11:32:04.070+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:04.071+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-11-03T11:32:04.071+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Job 8 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.121123 s
[2024-11-03T11:32:04.127+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO CodeGenerator: Code generated in 35.21322 ms
[2024-11-03T11:32:04.129+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Done
[2024-11-03T11:32:04.175+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on eb88dbfa1959:43169 in memory (size: 108.1 KiB, free: 434.2 MiB)
[2024-11-03T11:32:04.177+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.8:37445 in memory (size: 108.1 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:04.186+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on eb88dbfa1959:43169 in memory (size: 123.3 KiB, free: 434.3 MiB)
[2024-11-03T11:32:04.187+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.8:37445 in memory (size: 123.3 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:04.538+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO CodeGenerator: Code generated in 134.537062 ms
[2024-11-03T11:32:04.579+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:04.581+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Got job 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-03T11:32:04.581+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Final stage: ResultStage 14 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:04.582+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
[2024-11-03T11:32:04.583+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:04.585+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[41] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:04.594+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 592.1 KiB, free 433.1 MiB)
[2024-11-03T11:32:04.598+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 138.6 KiB, free 433.0 MiB)
[2024-11-03T11:32:04.599+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on eb88dbfa1959:43169 (size: 138.6 KiB, free: 434.2 MiB)
[2024-11-03T11:32:04.600+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:04.601+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 14 (MapPartitionsRDD[41] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-03T11:32:04.602+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSchedulerImpl: Adding task set 14.0 with 50 tasks resource profile 0
[2024-11-03T11:32:04.617+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 62) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.617+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 63) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.632+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.8:37445 (size: 138.6 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:04.819+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 64) (172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.820+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 3.0 in stage 14.0 (TID 65) (172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.821+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 63) in 204 ms on 172.18.0.8 (executor 0) (1/50)
[2024-11-03T11:32:04.822+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 62) in 205 ms on 172.18.0.8 (executor 0) (2/50)
[2024-11-03T11:32:04.838+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 4.0 in stage 14.0 (TID 66) (172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.840+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 5.0 in stage 14.0 (TID 67) (172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.840+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 64) in 21 ms on 172.18.0.8 (executor 0) (3/50)
[2024-11-03T11:32:04.841+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 3.0 in stage 14.0 (TID 65) in 21 ms on 172.18.0.8 (executor 0) (4/50)
[2024-11-03T11:32:04.855+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 6.0 in stage 14.0 (TID 68) (172.18.0.8, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.856+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 4.0 in stage 14.0 (TID 66) in 17 ms on 172.18.0.8 (executor 0) (5/50)
[2024-11-03T11:32:04.857+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 7.0 in stage 14.0 (TID 69) (172.18.0.8, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.858+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 5.0 in stage 14.0 (TID 67) in 19 ms on 172.18.0.8 (executor 0) (6/50)
[2024-11-03T11:32:04.872+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 8.0 in stage 14.0 (TID 70) (172.18.0.8, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.873+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 6.0 in stage 14.0 (TID 68) in 19 ms on 172.18.0.8 (executor 0) (7/50)
[2024-11-03T11:32:04.874+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 9.0 in stage 14.0 (TID 71) (172.18.0.8, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.875+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 7.0 in stage 14.0 (TID 69) in 19 ms on 172.18.0.8 (executor 0) (8/50)
[2024-11-03T11:32:04.889+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 10.0 in stage 14.0 (TID 72) (172.18.0.8, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.890+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 8.0 in stage 14.0 (TID 70) in 18 ms on 172.18.0.8 (executor 0) (9/50)
[2024-11-03T11:32:04.892+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 11.0 in stage 14.0 (TID 73) (172.18.0.8, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.893+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 9.0 in stage 14.0 (TID 71) in 19 ms on 172.18.0.8 (executor 0) (10/50)
[2024-11-03T11:32:04.905+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 12.0 in stage 14.0 (TID 74) (172.18.0.8, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.906+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 10.0 in stage 14.0 (TID 72) in 18 ms on 172.18.0.8 (executor 0) (11/50)
[2024-11-03T11:32:04.907+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 13.0 in stage 14.0 (TID 75) (172.18.0.8, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.909+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 11.0 in stage 14.0 (TID 73) in 16 ms on 172.18.0.8 (executor 0) (12/50)
[2024-11-03T11:32:04.930+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 14.0 in stage 14.0 (TID 76) (172.18.0.8, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.931+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 12.0 in stage 14.0 (TID 74) in 26 ms on 172.18.0.8 (executor 0) (13/50)
[2024-11-03T11:32:04.932+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 15.0 in stage 14.0 (TID 77) (172.18.0.8, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.933+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 13.0 in stage 14.0 (TID 75) in 26 ms on 172.18.0.8 (executor 0) (14/50)
[2024-11-03T11:32:04.949+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 16.0 in stage 14.0 (TID 78) (172.18.0.8, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.949+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 14.0 in stage 14.0 (TID 76) in 19 ms on 172.18.0.8 (executor 0) (15/50)
[2024-11-03T11:32:04.950+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 17.0 in stage 14.0 (TID 79) (172.18.0.8, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.951+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 15.0 in stage 14.0 (TID 77) in 19 ms on 172.18.0.8 (executor 0) (16/50)
[2024-11-03T11:32:04.965+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 18.0 in stage 14.0 (TID 80) (172.18.0.8, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.967+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 19.0 in stage 14.0 (TID 81) (172.18.0.8, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.968+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 17.0 in stage 14.0 (TID 79) in 17 ms on 172.18.0.8 (executor 0) (17/50)
[2024-11-03T11:32:04.969+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 16.0 in stage 14.0 (TID 78) in 19 ms on 172.18.0.8 (executor 0) (18/50)
[2024-11-03T11:32:04.982+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 20.0 in stage 14.0 (TID 82) (172.18.0.8, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.983+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 19.0 in stage 14.0 (TID 81) in 16 ms on 172.18.0.8 (executor 0) (19/50)
[2024-11-03T11:32:04.985+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 21.0 in stage 14.0 (TID 83) (172.18.0.8, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.986+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 18.0 in stage 14.0 (TID 80) in 21 ms on 172.18.0.8 (executor 0) (20/50)
[2024-11-03T11:32:04.998+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Starting task 22.0 in stage 14.0 (TID 84) (172.18.0.8, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:04.999+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:04 INFO TaskSetManager: Finished task 20.0 in stage 14.0 (TID 82) in 18 ms on 172.18.0.8 (executor 0) (21/50)
[2024-11-03T11:32:05.002+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 23.0 in stage 14.0 (TID 85) (172.18.0.8, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.003+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 21.0 in stage 14.0 (TID 83) in 18 ms on 172.18.0.8 (executor 0) (22/50)
[2024-11-03T11:32:05.017+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 24.0 in stage 14.0 (TID 86) (172.18.0.8, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.018+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 22.0 in stage 14.0 (TID 84) in 19 ms on 172.18.0.8 (executor 0) (23/50)
[2024-11-03T11:32:05.020+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 25.0 in stage 14.0 (TID 87) (172.18.0.8, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.021+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 23.0 in stage 14.0 (TID 85) in 18 ms on 172.18.0.8 (executor 0) (24/50)
[2024-11-03T11:32:05.034+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 26.0 in stage 14.0 (TID 88) (172.18.0.8, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.035+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 24.0 in stage 14.0 (TID 86) in 18 ms on 172.18.0.8 (executor 0) (25/50)
[2024-11-03T11:32:05.036+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 27.0 in stage 14.0 (TID 89) (172.18.0.8, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.037+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 25.0 in stage 14.0 (TID 87) in 18 ms on 172.18.0.8 (executor 0) (26/50)
[2024-11-03T11:32:05.052+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 28.0 in stage 14.0 (TID 90) (172.18.0.8, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.053+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 26.0 in stage 14.0 (TID 88) in 18 ms on 172.18.0.8 (executor 0) (27/50)
[2024-11-03T11:32:05.056+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 29.0 in stage 14.0 (TID 91) (172.18.0.8, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.057+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 27.0 in stage 14.0 (TID 89) in 20 ms on 172.18.0.8 (executor 0) (28/50)
[2024-11-03T11:32:05.069+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 30.0 in stage 14.0 (TID 92) (172.18.0.8, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.070+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 28.0 in stage 14.0 (TID 90) in 19 ms on 172.18.0.8 (executor 0) (29/50)
[2024-11-03T11:32:05.071+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 31.0 in stage 14.0 (TID 93) (172.18.0.8, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.072+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 29.0 in stage 14.0 (TID 91) in 17 ms on 172.18.0.8 (executor 0) (30/50)
[2024-11-03T11:32:05.085+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 32.0 in stage 14.0 (TID 94) (172.18.0.8, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.086+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 33.0 in stage 14.0 (TID 95) (172.18.0.8, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.087+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 30.0 in stage 14.0 (TID 92) in 17 ms on 172.18.0.8 (executor 0) (31/50)
[2024-11-03T11:32:05.088+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 31.0 in stage 14.0 (TID 93) in 16 ms on 172.18.0.8 (executor 0) (32/50)
[2024-11-03T11:32:05.101+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 34.0 in stage 14.0 (TID 96) (172.18.0.8, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.102+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 32.0 in stage 14.0 (TID 94) in 17 ms on 172.18.0.8 (executor 0) (33/50)
[2024-11-03T11:32:05.104+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 35.0 in stage 14.0 (TID 97) (172.18.0.8, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.104+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 33.0 in stage 14.0 (TID 95) in 18 ms on 172.18.0.8 (executor 0) (34/50)
[2024-11-03T11:32:05.125+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 36.0 in stage 14.0 (TID 98) (172.18.0.8, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.126+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 34.0 in stage 14.0 (TID 96) in 26 ms on 172.18.0.8 (executor 0) (35/50)
[2024-11-03T11:32:05.127+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 37.0 in stage 14.0 (TID 99) (172.18.0.8, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.128+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 35.0 in stage 14.0 (TID 97) in 25 ms on 172.18.0.8 (executor 0) (36/50)
[2024-11-03T11:32:05.141+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 38.0 in stage 14.0 (TID 100) (172.18.0.8, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.142+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 36.0 in stage 14.0 (TID 98) in 17 ms on 172.18.0.8 (executor 0) (37/50)
[2024-11-03T11:32:05.145+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 39.0 in stage 14.0 (TID 101) (172.18.0.8, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.146+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 37.0 in stage 14.0 (TID 99) in 19 ms on 172.18.0.8 (executor 0) (38/50)
[2024-11-03T11:32:05.157+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 40.0 in stage 14.0 (TID 102) (172.18.0.8, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.158+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 38.0 in stage 14.0 (TID 100) in 18 ms on 172.18.0.8 (executor 0) (39/50)
[2024-11-03T11:32:05.160+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 41.0 in stage 14.0 (TID 103) (172.18.0.8, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.161+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 39.0 in stage 14.0 (TID 101) in 17 ms on 172.18.0.8 (executor 0) (40/50)
[2024-11-03T11:32:05.174+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 42.0 in stage 14.0 (TID 104) (172.18.0.8, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.175+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 40.0 in stage 14.0 (TID 102) in 18 ms on 172.18.0.8 (executor 0) (41/50)
[2024-11-03T11:32:05.180+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 43.0 in stage 14.0 (TID 105) (172.18.0.8, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.181+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 41.0 in stage 14.0 (TID 103) in 20 ms on 172.18.0.8 (executor 0) (42/50)
[2024-11-03T11:32:05.193+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 44.0 in stage 14.0 (TID 106) (172.18.0.8, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.194+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 42.0 in stage 14.0 (TID 104) in 19 ms on 172.18.0.8 (executor 0) (43/50)
[2024-11-03T11:32:05.197+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 45.0 in stage 14.0 (TID 107) (172.18.0.8, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.198+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 43.0 in stage 14.0 (TID 105) in 19 ms on 172.18.0.8 (executor 0) (44/50)
[2024-11-03T11:32:05.212+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 46.0 in stage 14.0 (TID 108) (172.18.0.8, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.213+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 44.0 in stage 14.0 (TID 106) in 21 ms on 172.18.0.8 (executor 0) (45/50)
[2024-11-03T11:32:05.216+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 47.0 in stage 14.0 (TID 109) (172.18.0.8, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.217+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 45.0 in stage 14.0 (TID 107) in 19 ms on 172.18.0.8 (executor 0) (46/50)
[2024-11-03T11:32:05.227+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 48.0 in stage 14.0 (TID 110) (172.18.0.8, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.228+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 46.0 in stage 14.0 (TID 108) in 15 ms on 172.18.0.8 (executor 0) (47/50)
[2024-11-03T11:32:05.230+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 49.0 in stage 14.0 (TID 111) (172.18.0.8, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:05.231+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 47.0 in stage 14.0 (TID 109) in 16 ms on 172.18.0.8 (executor 0) (48/50)
[2024-11-03T11:32:05.242+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 48.0 in stage 14.0 (TID 110) in 16 ms on 172.18.0.8 (executor 0) (49/50)
[2024-11-03T11:32:05.247+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 49.0 in stage 14.0 (TID 111) in 16 ms on 172.18.0.8 (executor 0) (50/50)
[2024-11-03T11:32:05.248+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-11-03T11:32:05.249+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: ResultStage 14 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.660 s
[2024-11-03T11:32:05.249+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:05.250+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-11-03T11:32:05.250+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Job 9 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.669817 s
[2024-11-03T11:32:05.305+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO OptimisticTransaction: [tableId=34f0700a,txnId=616047f6] Attempting to commit version 3 with 4 actions with Serializable isolation level
[2024-11-03T11:32:05.633+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DeltaLog: Creating a new snapshot v3 for commit version 3
[2024-11-03T11:32:05.634+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DeltaLog: Loading version 3.
[2024-11-03T11:32:05.643+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 4, totalFileSize: 8442)
[2024-11-03T11:32:05.706+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:32:05.707+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:32:05.740+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_14_piece0 on eb88dbfa1959:43169 in memory (size: 138.6 KiB, free: 434.3 MiB)
[2024-11-03T11:32:05.741+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.18.0.8:37445 in memory (size: 138.6 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:05.742+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 205.0 KiB, free 433.5 MiB)
[2024-11-03T11:32:05.755+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
[2024-11-03T11:32:05.757+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on eb88dbfa1959:43169 (size: 36.1 KiB, free: 434.3 MiB)
[2024-11-03T11:32:05.760+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO SparkContext: Created broadcast 15 from toString at String.java:2951
[2024-11-03T11:32:05.761+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8392829 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:05.807+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-03T11:32:05.810+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Got job 10 (toString at String.java:2951) with 2 output partitions
[2024-11-03T11:32:05.811+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Final stage: ResultStage 15 (toString at String.java:2951)
[2024-11-03T11:32:05.812+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:05.812+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:05.813+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[45] at toString at String.java:2951), which has no missing parents
[2024-11-03T11:32:05.815+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 50.4 KiB, free 433.4 MiB)
[2024-11-03T11:32:05.824+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 433.4 MiB)
[2024-11-03T11:32:05.826+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on eb88dbfa1959:43169 (size: 15.5 KiB, free: 434.2 MiB)
[2024-11-03T11:32:05.827+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:05.828+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[45] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:05.829+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0
[2024-11-03T11:32:05.830+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 112) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8078 bytes)
[2024-11-03T11:32:05.831+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 113) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 8078 bytes)
[2024-11-03T11:32:05.845+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.8:37445 (size: 15.5 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:05.863+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.8:37445 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:05.902+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 113) in 72 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:05.903+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 112) in 73 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:05.904+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-11-03T11:32:05.904+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: ResultStage 15 (toString at String.java:2951) finished in 0.090 s
[2024-11-03T11:32:05.905+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:05.905+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2024-11-03T11:32:05.906+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DAGScheduler: Job 10 finished: toString at String.java:2951, took 0.096949 s
[2024-11-03T11:32:05.912+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] Created snapshot Snapshot(path=s3a://lakehouse/silver/merged_data/_delta_log, version=3, metadata=Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292)), logSegment=LogSegment(s3a://lakehouse/silver/merged_data/_delta_log,3,WrappedArray(S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1939; replication=1; blocksize=33554432; modification_time=1730633270778; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ca3327c2494957c46b6efdff87392535 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=1572; replication=1; blocksize=33554432; modification_time=1730633353899; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=499135ae9e973681b348d18a25ba2f7a versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633363129; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=797865593c6dd74174b6a29b3e403bc4 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=1898; replication=1; blocksize=33554432; modification_time=1730633525569; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=32ac357a7d7374f428afc596c592d94e versionId=null),None,1730633525569), checksumOpt=None)
[2024-11-03T11:32:05.913+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/silver/merged_data/_delta_log, version=3, metadata=Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292)), logSegment=LogSegment(s3a://lakehouse/silver/merged_data/_delta_log,3,WrappedArray(S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1939; replication=1; blocksize=33554432; modification_time=1730633270778; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ca3327c2494957c46b6efdff87392535 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=1572; replication=1; blocksize=33554432; modification_time=1730633353899; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=499135ae9e973681b348d18a25ba2f7a versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633363129; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=797865593c6dd74174b6a29b3e403bc4 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=1898; replication=1; blocksize=33554432; modification_time=1730633525569; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=32ac357a7d7374f428afc596c592d94e versionId=null),None,1730633525569), checksumOpt=None)
[2024-11-03T11:32:05.915+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MapPartitionsRDD: Removing RDD 33 from persistence list
[2024-11-03T11:32:05.923+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManager: Removing RDD 33
[2024-11-03T11:32:05.931+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Compute snapshot for version: 3
[2024-11-03T11:32:05.934+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 204.7 KiB, free 433.2 MiB)
[2024-11-03T11:32:05.947+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on eb88dbfa1959:43169 in memory (size: 36.1 KiB, free: 434.3 MiB)
[2024-11-03T11:32:05.948+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.8:37445 in memory (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:05.949+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.4 MiB)
[2024-11-03T11:32:05.950+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:05.951+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO SparkContext: Created broadcast 17 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:05.957+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on eb88dbfa1959:43169 in memory (size: 15.5 KiB, free: 434.3 MiB)
[2024-11-03T11:32:05.958+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:05 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.8:37445 in memory (size: 15.5 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:06.139+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:32:06.140+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:32:06.191+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 205.0 KiB, free 433.3 MiB)
[2024-11-03T11:32:06.211+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.2 MiB)
[2024-11-03T11:32:06.212+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on eb88dbfa1959:43169 (size: 36.1 KiB, free: 434.2 MiB)
[2024-11-03T11:32:06.213+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO SparkContext: Created broadcast 18 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:06.214+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8392829 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:06.224+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Registering RDD 49 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 3
[2024-11-03T11:32:06.225+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Got map stage job 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-03T11:32:06.226+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Final stage: ShuffleMapStage 16 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:06.226+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:06.227+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:06.228+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[49] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:06.232+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 138.3 KiB, free 433.1 MiB)
[2024-11-03T11:32:06.235+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.0 MiB)
[2024-11-03T11:32:06.237+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on eb88dbfa1959:43169 (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:06.238+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:06.239+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[49] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:06.239+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0
[2024-11-03T11:32:06.251+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 114) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8067 bytes)
[2024-11-03T11:32:06.252+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 115) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 8067 bytes)
[2024-11-03T11:32:06.262+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.8:37445 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:06.284+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.8:37445 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:06.329+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 114) in 89 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:06.338+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 115) in 87 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:06.339+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-11-03T11:32:06.340+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: ShuffleMapStage 16 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.112 s
[2024-11-03T11:32:06.341+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:06.342+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:06.342+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:06.342+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:06.397+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Removed broadcast_19_piece0 on eb88dbfa1959:43169 in memory (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:06.398+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.8:37445 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:06.543+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Registering RDD 59 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 4
[2024-11-03T11:32:06.544+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Got map stage job 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-03T11:32:06.544+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Final stage: ShuffleMapStage 18 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:06.545+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
[2024-11-03T11:32:06.546+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:06.547+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[59] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:06.614+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 517.1 KiB, free 432.7 MiB)
[2024-11-03T11:32:06.617+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 432.6 MiB)
[2024-11-03T11:32:06.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on eb88dbfa1959:43169 (size: 123.3 KiB, free: 434.1 MiB)
[2024-11-03T11:32:06.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:06.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[59] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-03T11:32:06.621+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSchedulerImpl: Adding task set 18.0 with 50 tasks resource profile 0
[2024-11-03T11:32:06.622+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 8.0 in stage 18.0 (TID 116) (172.18.0.8, executor 0, partition 8, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.623+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 26.0 in stage 18.0 (TID 117) (172.18.0.8, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.633+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.8:37445 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:06.645+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.8:47292
[2024-11-03T11:32:06.675+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_8 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.675+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_26 in memory on 172.18.0.8:37445 (size: 463.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.695+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 32.0 in stage 18.0 (TID 118) (172.18.0.8, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.695+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 8.0 in stage 18.0 (TID 116) in 74 ms on 172.18.0.8 (executor 0) (1/50)
[2024-11-03T11:32:06.696+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 42.0 in stage 18.0 (TID 119) (172.18.0.8, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.697+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 26.0 in stage 18.0 (TID 117) in 74 ms on 172.18.0.8 (executor 0) (2/50)
[2024-11-03T11:32:06.743+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_32 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.744+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_42 in memory on 172.18.0.8:37445 (size: 828.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.761+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 44.0 in stage 18.0 (TID 120) (172.18.0.8, executor 0, partition 44, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.762+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 32.0 in stage 18.0 (TID 118) in 68 ms on 172.18.0.8 (executor 0) (3/50)
[2024-11-03T11:32:06.763+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 48.0 in stage 18.0 (TID 121) (172.18.0.8, executor 0, partition 48, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.764+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 42.0 in stage 18.0 (TID 119) in 68 ms on 172.18.0.8 (executor 0) (4/50)
[2024-11-03T11:32:06.797+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_48 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.798+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_44 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.823+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 122) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.823+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 44.0 in stage 18.0 (TID 120) in 62 ms on 172.18.0.8 (executor 0) (5/50)
[2024-11-03T11:32:06.824+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 123) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.825+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 48.0 in stage 18.0 (TID 121) in 61 ms on 172.18.0.8 (executor 0) (6/50)
[2024-11-03T11:32:06.859+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_1 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_0 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.876+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 124) (172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.877+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 123) in 53 ms on 172.18.0.8 (executor 0) (7/50)
[2024-11-03T11:32:06.880+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 125) (172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.881+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 122) in 58 ms on 172.18.0.8 (executor 0) (8/50)
[2024-11-03T11:32:06.920+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_2 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.921+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_3 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.942+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 4.0 in stage 18.0 (TID 126) (172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.942+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 125) in 63 ms on 172.18.0.8 (executor 0) (9/50)
[2024-11-03T11:32:06.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Starting task 5.0 in stage 18.0 (TID 127) (172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:06.944+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 124) in 68 ms on 172.18.0.8 (executor 0) (10/50)
[2024-11-03T11:32:06.980+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_5 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:06.984+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:06 INFO BlockManagerInfo: Added rdd_56_4 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.002+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 6.0 in stage 18.0 (TID 128) (172.18.0.8, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.003+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 5.0 in stage 18.0 (TID 127) in 60 ms on 172.18.0.8 (executor 0) (11/50)
[2024-11-03T11:32:07.009+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 7.0 in stage 18.0 (TID 129) (172.18.0.8, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.010+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 4.0 in stage 18.0 (TID 126) in 69 ms on 172.18.0.8 (executor 0) (12/50)
[2024-11-03T11:32:07.040+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_6 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.041+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_7 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.060+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 9.0 in stage 18.0 (TID 130) (172.18.0.8, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.061+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 10.0 in stage 18.0 (TID 131) (172.18.0.8, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.062+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 6.0 in stage 18.0 (TID 128) in 59 ms on 172.18.0.8 (executor 0) (13/50)
[2024-11-03T11:32:07.063+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 7.0 in stage 18.0 (TID 129) in 53 ms on 172.18.0.8 (executor 0) (14/50)
[2024-11-03T11:32:07.093+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_9 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.095+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_10 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.115+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 11.0 in stage 18.0 (TID 132) (172.18.0.8, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.115+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 9.0 in stage 18.0 (TID 130) in 56 ms on 172.18.0.8 (executor 0) (15/50)
[2024-11-03T11:32:07.116+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 12.0 in stage 18.0 (TID 133) (172.18.0.8, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.117+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 10.0 in stage 18.0 (TID 131) in 56 ms on 172.18.0.8 (executor 0) (16/50)
[2024-11-03T11:32:07.151+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_12 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.153+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_11 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.173+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 13.0 in stage 18.0 (TID 134) (172.18.0.8, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.174+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 12.0 in stage 18.0 (TID 133) in 58 ms on 172.18.0.8 (executor 0) (17/50)
[2024-11-03T11:32:07.176+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 14.0 in stage 18.0 (TID 135) (172.18.0.8, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.177+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 11.0 in stage 18.0 (TID 132) in 63 ms on 172.18.0.8 (executor 0) (18/50)
[2024-11-03T11:32:07.213+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_13 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.217+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_14 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.232+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 15.0 in stage 18.0 (TID 136) (172.18.0.8, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.233+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 13.0 in stage 18.0 (TID 134) in 60 ms on 172.18.0.8 (executor 0) (19/50)
[2024-11-03T11:32:07.235+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 16.0 in stage 18.0 (TID 137) (172.18.0.8, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.236+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 14.0 in stage 18.0 (TID 135) in 60 ms on 172.18.0.8 (executor 0) (20/50)
[2024-11-03T11:32:07.265+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_15 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.268+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_16 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.283+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 17.0 in stage 18.0 (TID 138) (172.18.0.8, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.283+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 15.0 in stage 18.0 (TID 136) in 52 ms on 172.18.0.8 (executor 0) (21/50)
[2024-11-03T11:32:07.286+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 18.0 in stage 18.0 (TID 139) (172.18.0.8, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.287+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 16.0 in stage 18.0 (TID 137) in 52 ms on 172.18.0.8 (executor 0) (22/50)
[2024-11-03T11:32:07.335+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_17 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.340+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_18 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.360+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 19.0 in stage 18.0 (TID 140) (172.18.0.8, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.361+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 17.0 in stage 18.0 (TID 138) in 78 ms on 172.18.0.8 (executor 0) (23/50)
[2024-11-03T11:32:07.362+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 20.0 in stage 18.0 (TID 141) (172.18.0.8, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.363+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 18.0 in stage 18.0 (TID 139) in 76 ms on 172.18.0.8 (executor 0) (24/50)
[2024-11-03T11:32:07.399+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_20 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.400+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_19 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.422+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 21.0 in stage 18.0 (TID 142) (172.18.0.8, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.423+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 19.0 in stage 18.0 (TID 140) in 63 ms on 172.18.0.8 (executor 0) (25/50)
[2024-11-03T11:32:07.424+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 22.0 in stage 18.0 (TID 143) (172.18.0.8, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.424+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 20.0 in stage 18.0 (TID 141) in 63 ms on 172.18.0.8 (executor 0) (26/50)
[2024-11-03T11:32:07.458+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_21 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.462+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_22 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.478+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 23.0 in stage 18.0 (TID 144) (172.18.0.8, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.479+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 21.0 in stage 18.0 (TID 142) in 57 ms on 172.18.0.8 (executor 0) (27/50)
[2024-11-03T11:32:07.481+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 24.0 in stage 18.0 (TID 145) (172.18.0.8, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.482+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 22.0 in stage 18.0 (TID 143) in 59 ms on 172.18.0.8 (executor 0) (28/50)
[2024-11-03T11:32:07.513+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_23 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.514+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_24 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.530+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 25.0 in stage 18.0 (TID 146) (172.18.0.8, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.531+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 23.0 in stage 18.0 (TID 144) in 52 ms on 172.18.0.8 (executor 0) (29/50)
[2024-11-03T11:32:07.531+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 27.0 in stage 18.0 (TID 147) (172.18.0.8, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.532+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 24.0 in stage 18.0 (TID 145) in 51 ms on 172.18.0.8 (executor 0) (30/50)
[2024-11-03T11:32:07.565+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_25 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.566+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_27 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.586+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 28.0 in stage 18.0 (TID 148) (172.18.0.8, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.587+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 25.0 in stage 18.0 (TID 146) in 57 ms on 172.18.0.8 (executor 0) (31/50)
[2024-11-03T11:32:07.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_28 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.636+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 29.0 in stage 18.0 (TID 149) (172.18.0.8, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.637+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 28.0 in stage 18.0 (TID 148) in 51 ms on 172.18.0.8 (executor 0) (32/50)
[2024-11-03T11:32:07.669+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_29 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.686+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 30.0 in stage 18.0 (TID 150) (172.18.0.8, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.687+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 29.0 in stage 18.0 (TID 149) in 51 ms on 172.18.0.8 (executor 0) (33/50)
[2024-11-03T11:32:07.717+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_30 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.728+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 31.0 in stage 18.0 (TID 151) (172.18.0.8, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.729+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 27.0 in stage 18.0 (TID 147) in 197 ms on 172.18.0.8 (executor 0) (34/50)
[2024-11-03T11:32:07.747+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 33.0 in stage 18.0 (TID 152) (172.18.0.8, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.748+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 30.0 in stage 18.0 (TID 150) in 62 ms on 172.18.0.8 (executor 0) (35/50)
[2024-11-03T11:32:07.775+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_31 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.790+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_33 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.800+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 34.0 in stage 18.0 (TID 153) (172.18.0.8, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.801+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 31.0 in stage 18.0 (TID 151) in 74 ms on 172.18.0.8 (executor 0) (36/50)
[2024-11-03T11:32:07.808+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 35.0 in stage 18.0 (TID 154) (172.18.0.8, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.809+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 33.0 in stage 18.0 (TID 152) in 62 ms on 172.18.0.8 (executor 0) (37/50)
[2024-11-03T11:32:07.835+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_34 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.840+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_35 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.852+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 36.0 in stage 18.0 (TID 155) (172.18.0.8, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.853+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 34.0 in stage 18.0 (TID 153) in 52 ms on 172.18.0.8 (executor 0) (38/50)
[2024-11-03T11:32:07.865+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 37.0 in stage 18.0 (TID 156) (172.18.0.8, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.866+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 35.0 in stage 18.0 (TID 154) in 58 ms on 172.18.0.8 (executor 0) (39/50)
[2024-11-03T11:32:07.891+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_36 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.902+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_37 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.911+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 38.0 in stage 18.0 (TID 157) (172.18.0.8, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.912+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 36.0 in stage 18.0 (TID 155) in 59 ms on 172.18.0.8 (executor 0) (40/50)
[2024-11-03T11:32:07.924+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 39.0 in stage 18.0 (TID 158) (172.18.0.8, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.925+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 37.0 in stage 18.0 (TID 156) in 61 ms on 172.18.0.8 (executor 0) (41/50)
[2024-11-03T11:32:07.964+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_38 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.979+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO BlockManagerInfo: Added rdd_56_39 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:07.988+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Starting task 40.0 in stage 18.0 (TID 159) (172.18.0.8, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:07.989+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:07 INFO TaskSetManager: Finished task 38.0 in stage 18.0 (TID 157) in 79 ms on 172.18.0.8 (executor 0) (42/50)
[2024-11-03T11:32:08.000+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 41.0 in stage 18.0 (TID 160) (172.18.0.8, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.001+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 39.0 in stage 18.0 (TID 158) in 77 ms on 172.18.0.8 (executor 0) (43/50)
[2024-11-03T11:32:08.036+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_40 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.052+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_41 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.067+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 43.0 in stage 18.0 (TID 161) (172.18.0.8, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.068+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 40.0 in stage 18.0 (TID 159) in 79 ms on 172.18.0.8 (executor 0) (44/50)
[2024-11-03T11:32:08.080+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 45.0 in stage 18.0 (TID 162) (172.18.0.8, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.081+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 41.0 in stage 18.0 (TID 160) in 81 ms on 172.18.0.8 (executor 0) (45/50)
[2024-11-03T11:32:08.123+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_43 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.136+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_45 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.145+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 46.0 in stage 18.0 (TID 163) (172.18.0.8, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.146+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 43.0 in stage 18.0 (TID 161) in 79 ms on 172.18.0.8 (executor 0) (46/50)
[2024-11-03T11:32:08.159+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 47.0 in stage 18.0 (TID 164) (172.18.0.8, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.159+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 45.0 in stage 18.0 (TID 162) in 79 ms on 172.18.0.8 (executor 0) (47/50)
[2024-11-03T11:32:08.187+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_46 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.202+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_47 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.208+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 49.0 in stage 18.0 (TID 165) (172.18.0.8, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:08.209+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 46.0 in stage 18.0 (TID 163) in 63 ms on 172.18.0.8 (executor 0) (48/50)
[2024-11-03T11:32:08.225+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 47.0 in stage 18.0 (TID 164) in 66 ms on 172.18.0.8 (executor 0) (49/50)
[2024-11-03T11:32:08.246+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added rdd_56_49 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-03T11:32:08.265+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 49.0 in stage 18.0 (TID 165) in 57 ms on 172.18.0.8 (executor 0) (50/50)
[2024-11-03T11:32:08.266+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-11-03T11:32:08.267+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: ShuffleMapStage 18 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.718 s
[2024-11-03T11:32:08.267+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:08.268+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:08.269+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:08.269+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:08.281+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Removed broadcast_20_piece0 on eb88dbfa1959:43169 in memory (size: 123.3 KiB, free: 434.2 MiB)
[2024-11-03T11:32:08.282+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.8:37445 in memory (size: 123.3 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:08.306+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:08.312+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Got job 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-03T11:32:08.312+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Final stage: ResultStage 21 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:08.313+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
[2024-11-03T11:32:08.314+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:08.315+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[62] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:08.327+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 452.3 KiB, free 432.8 MiB)
[2024-11-03T11:32:08.331+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 432.7 MiB)
[2024-11-03T11:32:08.334+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on eb88dbfa1959:43169 (size: 108.1 KiB, free: 434.1 MiB)
[2024-11-03T11:32:08.334+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:08.335+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[62] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:32:08.336+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-11-03T11:32:08.337+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 166) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:32:08.348+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.8:37445 (size: 108.1 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:08.364+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.8:47292
[2024-11-03T11:32:08.396+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 166) in 59 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:32:08.397+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-11-03T11:32:08.398+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: ResultStage 21 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.083 s
[2024-11-03T11:32:08.398+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:08.399+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-11-03T11:32:08.400+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO DAGScheduler: Job 13 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.091571 s
[2024-11-03T11:32:08.412+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Done
[2024-11-03T11:32:08.468+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO OptimisticTransaction: [tableId=34f0700a,txnId=616047f6] Committed delta #3 to s3a://lakehouse/silver/merged_data/_delta_log
[2024-11-03T11:32:08.579+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO OptimisticTransaction: [tableId=34f0700a,txnId=e3e22d5b] Updated metadata from - to Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292))
[2024-11-03T11:32:08.834+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-03T11:32:08.834+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#12 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#15)
[2024-11-03T11:32:08.838+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(keywords),IsNotNull(id)
[2024-11-03T11:32:08.840+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(keywords#1),isnotnull(id#0)
[2024-11-03T11:32:08.844+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-03T11:32:08.845+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#802, Some(Etc/UTC)), lambdafunction(lambda x#814.name, lambda x#814, false))),isnotnull(id#804L)
[2024-11-03T11:32:08.916+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-03T11:32:08.944+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-03T11:32:08.959+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO CodeGenerator: Code generated in 9.275076 ms
[2024-11-03T11:32:08.963+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 204.8 KiB, free 432.5 MiB)
[2024-11-03T11:32:08.982+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Removed broadcast_21_piece0 on eb88dbfa1959:43169 in memory (size: 108.1 KiB, free: 434.2 MiB)
[2024-11-03T11:32:08.984+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.8:37445 in memory (size: 108.1 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:08.985+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.0 MiB)
[2024-11-03T11:32:08.987+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:08.988+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:08 INFO SparkContext: Created broadcast 22 from save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:32:09.016+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:09.036+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Removed broadcast_9_piece0 on eb88dbfa1959:43169 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:09.049+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on eb88dbfa1959:43169 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-03T11:32:09.050+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.8:37445 in memory (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:09.061+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManager: Removing RDD 33
[2024-11-03T11:32:09.073+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on eb88dbfa1959:43169 in memory (size: 36.6 KiB, free: 434.3 MiB)
[2024-11-03T11:32:09.074+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.18.0.8:37445 in memory (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-03T11:32:09.080+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Registering RDD 66 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 5
[2024-11-03T11:32:09.081+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Got map stage job 14 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-03T11:32:09.082+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:32:09.083+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:09.084+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:09.085+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-03T11:32:09.086+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[66] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:32:09.087+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 31.5 KiB, free 433.7 MiB)
[2024-11-03T11:32:09.089+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 433.6 MiB)
[2024-11-03T11:32:09.091+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on eb88dbfa1959:43169 (size: 14.8 KiB, free: 434.3 MiB)
[2024-11-03T11:32:09.092+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:09.093+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[66] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:32:09.094+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-11-03T11:32:09.095+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 167) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7912 bytes)
[2024-11-03T11:32:09.102+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO CodeGenerator: Code generated in 10.586266 ms
[2024-11-03T11:32:09.107+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 206.7 KiB, free 433.4 MiB)
[2024-11-03T11:32:09.108+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.8:37445 (size: 14.8 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:09.117+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 433.4 MiB)
[2024-11-03T11:32:09.117+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on eb88dbfa1959:43169 (size: 36.7 KiB, free: 434.2 MiB)
[2024-11-03T11:32:09.118+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO SparkContext: Created broadcast 24 from save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:32:09.120+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:09.138+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Registering RDD 74 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 6
[2024-11-03T11:32:09.139+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Got map stage job 15 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:32:09.140+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:32:09.141+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:09.142+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:09.142+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[74] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:32:09.144+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 45.4 KiB, free 433.4 MiB)
[2024-11-03T11:32:09.148+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 433.3 MiB)
[2024-11-03T11:32:09.149+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on eb88dbfa1959:43169 (size: 18.9 KiB, free: 434.2 MiB)
[2024-11-03T11:32:09.150+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:09.150+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[74] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:09.151+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO TaskSchedulerImpl: Adding task set 23.0 with 2 tasks resource profile 0
[2024-11-03T11:32:09.153+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 168) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-03T11:32:09.164+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.8:37445 (size: 18.9 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:09.183+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.8:37445 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:09.211+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:09 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.8:37445 (size: 36.7 KiB, free: 1048.7 MiB)
[2024-11-03T11:32:11.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 169) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-03T11:32:11.173+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 167) in 2079 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:32:11.174+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-11-03T11:32:11.175+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: ShuffleMapStage 22 (save at NativeMethodAccessorImpl.java:0) finished in 2.090 s
[2024-11-03T11:32:11.176+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:11.176+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: running: Set(ShuffleMapStage 23)
[2024-11-03T11:32:11.177+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:11.178+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:11.194+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-03T11:32:11.194+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#12 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#15)
[2024-11-03T11:32:11.205+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2024-11-03T11:32:11.246+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 169) in 73 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:11.270+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-03T11:32:11.271+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions
[2024-11-03T11:32:11.272+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-11-03T11:32:11.272+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
[2024-11-03T11:32:11.273+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:11.274+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-11-03T11:32:11.281+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.9 KiB, free 433.3 MiB)
[2024-11-03T11:32:11.283+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 433.3 MiB)
[2024-11-03T11:32:11.284+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on eb88dbfa1959:43169 (size: 16.0 KiB, free: 434.2 MiB)
[2024-11-03T11:32:11.285+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:11.286+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[77] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:32:11.286+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2024-11-03T11:32:11.287+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 170) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:32:11.298+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.8:37445 (size: 16.0 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:11.319+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.8:47292
[2024-11-03T11:32:11.464+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 170) in 177 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:32:11.465+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-11-03T11:32:11.465+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.187 s
[2024-11-03T11:32:11.466+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:11.467+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-11-03T11:32:11.468+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.196097 s
[2024-11-03T11:32:11.497+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO CodeGenerator: Code generated in 7.624959 ms
[2024-11-03T11:32:11.571+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1062.8 KiB, free 432.3 MiB)
[2024-11-03T11:32:11.572+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on eb88dbfa1959:43169 (size: 1062.8 KiB, free: 433.2 MiB)
[2024-11-03T11:32:11.572+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO SparkContext: Created broadcast 27 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-03T11:32:11.582+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-03T11:32:11.583+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#12 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#15)
[2024-11-03T11:32:11.625+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO CodeGenerator: Code generated in 9.593521 ms
[2024-11-03T11:32:11.700+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO CodeGenerator: Code generated in 52.813255 ms
[2024-11-03T11:32:11.709+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 208.5 KiB, free 432.1 MiB)
[2024-11-03T11:32:11.718+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 432.0 MiB)
[2024-11-03T11:32:11.719+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on eb88dbfa1959:43169 (size: 37.2 KiB, free: 433.1 MiB)
[2024-11-03T11:32:11.720+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO SparkContext: Created broadcast 28 from save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:32:11.721+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10691623 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:11.735+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Registering RDD 83 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 7
[2024-11-03T11:32:11.736+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Got map stage job 17 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:32:11.736+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:32:11.737+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:11.738+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:11.739+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[83] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:32:11.795+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 60.5 KiB, free 432.0 MiB)
[2024-11-03T11:32:11.797+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 431.9 MiB)
[2024-11-03T11:32:11.797+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on eb88dbfa1959:43169 (size: 22.9 KiB, free: 433.1 MiB)
[2024-11-03T11:32:11.799+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:11.799+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[83] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:11.800+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0
[2024-11-03T11:32:11.801+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 171) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7910 bytes)
[2024-11-03T11:32:11.811+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.18.0.8:37445 (size: 22.9 KiB, free: 1048.6 MiB)
[2024-11-03T11:32:11.963+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.8:37445 (size: 1062.8 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:11.996+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.8:37445 (size: 37.2 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:12.712+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 172) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7910 bytes)
[2024-11-03T11:32:12.714+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 168) in 3561 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:12.718+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-11-03T11:32:12.726+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: ShuffleMapStage 23 (save at NativeMethodAccessorImpl.java:0) finished in 3.572 s
[2024-11-03T11:32:12.727+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:12.733+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: running: Set(ShuffleMapStage 26)
[2024-11-03T11:32:12.734+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:12.735+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:12.737+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_26_piece0 on eb88dbfa1959:43169 in memory (size: 16.0 KiB, free: 433.1 MiB)
[2024-11-03T11:32:12.738+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.8:37445 in memory (size: 16.0 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:12.748+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_23_piece0 on eb88dbfa1959:43169 in memory (size: 14.8 KiB, free: 433.1 MiB)
[2024-11-03T11:32:12.749+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.8:37445 in memory (size: 14.8 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:12.750+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 5353534, minimum partition size: 1048576
[2024-11-03T11:32:12.777+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-03T11:32:12.814+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO CodeGenerator: Code generated in 32.129417 ms
[2024-11-03T11:32:12.822+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 172) in 110 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:12.829+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Registering RDD 86 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 8
[2024-11-03T11:32:12.830+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Got map stage job 18 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:32:12.831+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:32:12.832+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
[2024-11-03T11:32:12.833+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:12.833+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[86] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:32:12.840+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 47.8 KiB, free 432.0 MiB)
[2024-11-03T11:32:12.851+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 432.0 MiB)
[2024-11-03T11:32:12.853+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on eb88dbfa1959:43169 (size: 20.1 KiB, free: 433.1 MiB)
[2024-11-03T11:32:12.854+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:12.855+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[86] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:12.856+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0
[2024-11-03T11:32:12.857+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 173) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:12.883+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.8:37445 (size: 20.1 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:12.892+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.18.0.8:47292
[2024-11-03T11:32:12.926+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_25_piece0 on eb88dbfa1959:43169 in memory (size: 18.9 KiB, free: 433.1 MiB)
[2024-11-03T11:32:12.927+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:12 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.8:37445 in memory (size: 18.9 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:13.098+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 174) (172.18.0.8, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:13.099+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 171) in 1298 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:13.100+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2024-11-03T11:32:13.101+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: ShuffleMapStage 26 (save at NativeMethodAccessorImpl.java:0) finished in 1.363 s
[2024-11-03T11:32:13.102+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:13.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: running: Set(ShuffleMapStage 28)
[2024-11-03T11:32:13.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:13.104+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:13.135+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 173) in 279 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:13.267+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 174) in 170 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:13.268+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-11-03T11:32:13.269+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: ShuffleMapStage 28 (save at NativeMethodAccessorImpl.java:0) finished in 0.433 s
[2024-11-03T11:32:13.269+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:13.270+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:13.270+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:13.271+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:13.282+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO ShufflePartitionsUtil: For shuffle(7, 8), advisory target size: 67108864, actual target size 5372325, minimum partition size: 1048576
[2024-11-03T11:32:13.336+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO CodeGenerator: Code generated in 28.067076 ms
[2024-11-03T11:32:13.344+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO CodeGenerator: Code generated in 5.730392 ms
[2024-11-03T11:32:13.366+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO CodeGenerator: Code generated in 6.336229 ms
[2024-11-03T11:32:13.438+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-03T11:32:13.439+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Got job 19 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-03T11:32:13.440+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Final stage: ResultStage 32 (save at NativeMethodAccessorImpl.java:0)
[2024-11-03T11:32:13.441+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30, ShuffleMapStage 31)
[2024-11-03T11:32:13.442+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:13.442+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[92] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-03T11:32:13.462+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 446.6 KiB, free 431.6 MiB)
[2024-11-03T11:32:13.465+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 159.8 KiB, free 431.4 MiB)
[2024-11-03T11:32:13.466+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on eb88dbfa1959:43169 (size: 159.8 KiB, free: 433.0 MiB)
[2024-11-03T11:32:13.467+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:13.472+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 32 (MapPartitionsRDD[92] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:13.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSchedulerImpl: Adding task set 32.0 with 2 tasks resource profile 0
[2024-11-03T11:32:13.476+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 175) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7649 bytes)
[2024-11-03T11:32:13.477+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Starting task 1.0 in stage 32.0 (TID 176) (172.18.0.8, executor 0, partition 1, NODE_LOCAL, 7649 bytes)
[2024-11-03T11:32:13.487+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.8:37445 (size: 159.8 KiB, free: 1047.4 MiB)
[2024-11-03T11:32:13.552+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.18.0.8:47292
[2024-11-03T11:32:13.588+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.18.0.8:47292
[2024-11-03T11:32:13.824+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Finished task 1.0 in stage 32.0 (TID 176) in 347 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:13.967+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 175) in 493 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:13.968+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2024-11-03T11:32:13.969+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: ResultStage 32 (save at NativeMethodAccessorImpl.java:0) finished in 0.526 s
[2024-11-03T11:32:13.970+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:13.971+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2024-11-03T11:32:13.971+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO DAGScheduler: Job 19 finished: save at NativeMethodAccessorImpl.java:0, took 0.531650 s
[2024-11-03T11:32:13.972+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO FileFormatWriter: Start to commit write Job 20eedbaf-c44f-41e8-bcee-c0ea3ea7a6b5.
[2024-11-03T11:32:13.973+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO FileFormatWriter: Write Job 20eedbaf-c44f-41e8-bcee-c0ea3ea7a6b5 committed. Elapsed time: 0 ms.
[2024-11-03T11:32:13.973+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:13 INFO FileFormatWriter: Finished processing stats for write job 20eedbaf-c44f-41e8-bcee-c0ea3ea7a6b5.
[2024-11-03T11:32:14.098+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_31_piece0 on eb88dbfa1959:43169 in memory (size: 159.8 KiB, free: 433.1 MiB)
[2024-11-03T11:32:14.101+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.8:37445 in memory (size: 159.8 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:14.114+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.8:37445 in memory (size: 20.1 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:14.115+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_30_piece0 on eb88dbfa1959:43169 in memory (size: 20.1 KiB, free: 433.2 MiB)
[2024-11-03T11:32:14.138+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:14.140+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Got job 20 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-03T11:32:14.141+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Final stage: ResultStage 34 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:14.142+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
[2024-11-03T11:32:14.143+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:14.144+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[94] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:14.151+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 592.2 KiB, free 431.5 MiB)
[2024-11-03T11:32:14.154+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 138.6 KiB, free 431.4 MiB)
[2024-11-03T11:32:14.155+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on eb88dbfa1959:43169 (size: 138.6 KiB, free: 433.0 MiB)
[2024-11-03T11:32:14.156+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:14.156+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 34 (MapPartitionsRDD[94] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-03T11:32:14.157+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Adding task set 34.0 with 50 tasks resource profile 0
[2024-11-03T11:32:14.158+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 177) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.159+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 178) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.183+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.8:37445 (size: 138.6 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:14.185+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_29_piece0 on eb88dbfa1959:43169 in memory (size: 22.9 KiB, free: 433.0 MiB)
[2024-11-03T11:32:14.186+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.18.0.8:37445 in memory (size: 22.9 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:14.198+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 2.0 in stage 34.0 (TID 179) (172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.199+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 3.0 in stage 34.0 (TID 180) (172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.200+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 177) in 41 ms on 172.18.0.8 (executor 0) (1/50)
[2024-11-03T11:32:14.201+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 178) in 42 ms on 172.18.0.8 (executor 0) (2/50)
[2024-11-03T11:32:14.215+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 4.0 in stage 34.0 (TID 181) (172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.216+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 5.0 in stage 34.0 (TID 182) (172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.217+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 2.0 in stage 34.0 (TID 179) in 18 ms on 172.18.0.8 (executor 0) (3/50)
[2024-11-03T11:32:14.218+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 3.0 in stage 34.0 (TID 180) in 18 ms on 172.18.0.8 (executor 0) (4/50)
[2024-11-03T11:32:14.229+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 6.0 in stage 34.0 (TID 183) (172.18.0.8, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.230+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 4.0 in stage 34.0 (TID 181) in 16 ms on 172.18.0.8 (executor 0) (5/50)
[2024-11-03T11:32:14.233+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 7.0 in stage 34.0 (TID 184) (172.18.0.8, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.233+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 5.0 in stage 34.0 (TID 182) in 17 ms on 172.18.0.8 (executor 0) (6/50)
[2024-11-03T11:32:14.246+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 8.0 in stage 34.0 (TID 185) (172.18.0.8, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.246+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 6.0 in stage 34.0 (TID 183) in 17 ms on 172.18.0.8 (executor 0) (7/50)
[2024-11-03T11:32:14.247+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 9.0 in stage 34.0 (TID 186) (172.18.0.8, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.248+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 7.0 in stage 34.0 (TID 184) in 16 ms on 172.18.0.8 (executor 0) (8/50)
[2024-11-03T11:32:14.260+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 10.0 in stage 34.0 (TID 187) (172.18.0.8, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.261+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 8.0 in stage 34.0 (TID 185) in 15 ms on 172.18.0.8 (executor 0) (9/50)
[2024-11-03T11:32:14.261+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 11.0 in stage 34.0 (TID 188) (172.18.0.8, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.262+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 9.0 in stage 34.0 (TID 186) in 15 ms on 172.18.0.8 (executor 0) (10/50)
[2024-11-03T11:32:14.276+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 12.0 in stage 34.0 (TID 189) (172.18.0.8, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.277+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 10.0 in stage 34.0 (TID 187) in 16 ms on 172.18.0.8 (executor 0) (11/50)
[2024-11-03T11:32:14.278+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 13.0 in stage 34.0 (TID 190) (172.18.0.8, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.279+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 11.0 in stage 34.0 (TID 188) in 17 ms on 172.18.0.8 (executor 0) (12/50)
[2024-11-03T11:32:14.291+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 14.0 in stage 34.0 (TID 191) (172.18.0.8, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.292+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 12.0 in stage 34.0 (TID 189) in 16 ms on 172.18.0.8 (executor 0) (13/50)
[2024-11-03T11:32:14.295+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 15.0 in stage 34.0 (TID 192) (172.18.0.8, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.295+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 13.0 in stage 34.0 (TID 190) in 18 ms on 172.18.0.8 (executor 0) (14/50)
[2024-11-03T11:32:14.306+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 16.0 in stage 34.0 (TID 193) (172.18.0.8, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.307+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 14.0 in stage 34.0 (TID 191) in 17 ms on 172.18.0.8 (executor 0) (15/50)
[2024-11-03T11:32:14.311+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 17.0 in stage 34.0 (TID 194) (172.18.0.8, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.312+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 15.0 in stage 34.0 (TID 192) in 18 ms on 172.18.0.8 (executor 0) (16/50)
[2024-11-03T11:32:14.322+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 18.0 in stage 34.0 (TID 195) (172.18.0.8, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.323+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 16.0 in stage 34.0 (TID 193) in 17 ms on 172.18.0.8 (executor 0) (17/50)
[2024-11-03T11:32:14.327+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 19.0 in stage 34.0 (TID 196) (172.18.0.8, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.327+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 17.0 in stage 34.0 (TID 194) in 16 ms on 172.18.0.8 (executor 0) (18/50)
[2024-11-03T11:32:14.343+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 20.0 in stage 34.0 (TID 197) (172.18.0.8, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.344+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 18.0 in stage 34.0 (TID 195) in 21 ms on 172.18.0.8 (executor 0) (19/50)
[2024-11-03T11:32:14.351+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 21.0 in stage 34.0 (TID 198) (172.18.0.8, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.352+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 19.0 in stage 34.0 (TID 196) in 25 ms on 172.18.0.8 (executor 0) (20/50)
[2024-11-03T11:32:14.390+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 22.0 in stage 34.0 (TID 199) (172.18.0.8, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.391+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 20.0 in stage 34.0 (TID 197) in 49 ms on 172.18.0.8 (executor 0) (21/50)
[2024-11-03T11:32:14.397+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 23.0 in stage 34.0 (TID 200) (172.18.0.8, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.397+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 21.0 in stage 34.0 (TID 198) in 47 ms on 172.18.0.8 (executor 0) (22/50)
[2024-11-03T11:32:14.409+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 24.0 in stage 34.0 (TID 201) (172.18.0.8, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.410+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 22.0 in stage 34.0 (TID 199) in 21 ms on 172.18.0.8 (executor 0) (23/50)
[2024-11-03T11:32:14.416+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 25.0 in stage 34.0 (TID 202) (172.18.0.8, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.417+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 23.0 in stage 34.0 (TID 200) in 20 ms on 172.18.0.8 (executor 0) (24/50)
[2024-11-03T11:32:14.431+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 26.0 in stage 34.0 (TID 203) (172.18.0.8, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.432+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 24.0 in stage 34.0 (TID 201) in 22 ms on 172.18.0.8 (executor 0) (25/50)
[2024-11-03T11:32:14.435+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 27.0 in stage 34.0 (TID 204) (172.18.0.8, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.436+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 25.0 in stage 34.0 (TID 202) in 20 ms on 172.18.0.8 (executor 0) (26/50)
[2024-11-03T11:32:14.448+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 28.0 in stage 34.0 (TID 205) (172.18.0.8, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.449+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 26.0 in stage 34.0 (TID 203) in 19 ms on 172.18.0.8 (executor 0) (27/50)
[2024-11-03T11:32:14.455+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 29.0 in stage 34.0 (TID 206) (172.18.0.8, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.456+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 27.0 in stage 34.0 (TID 204) in 21 ms on 172.18.0.8 (executor 0) (28/50)
[2024-11-03T11:32:14.467+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 30.0 in stage 34.0 (TID 207) (172.18.0.8, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.468+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 28.0 in stage 34.0 (TID 205) in 21 ms on 172.18.0.8 (executor 0) (29/50)
[2024-11-03T11:32:14.472+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 31.0 in stage 34.0 (TID 208) (172.18.0.8, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.473+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 29.0 in stage 34.0 (TID 206) in 19 ms on 172.18.0.8 (executor 0) (30/50)
[2024-11-03T11:32:14.493+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 32.0 in stage 34.0 (TID 209) (172.18.0.8, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.494+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 30.0 in stage 34.0 (TID 207) in 27 ms on 172.18.0.8 (executor 0) (31/50)
[2024-11-03T11:32:14.498+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 33.0 in stage 34.0 (TID 210) (172.18.0.8, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.499+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 31.0 in stage 34.0 (TID 208) in 26 ms on 172.18.0.8 (executor 0) (32/50)
[2024-11-03T11:32:14.509+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 34.0 in stage 34.0 (TID 211) (172.18.0.8, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.510+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 32.0 in stage 34.0 (TID 209) in 17 ms on 172.18.0.8 (executor 0) (33/50)
[2024-11-03T11:32:14.514+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 35.0 in stage 34.0 (TID 212) (172.18.0.8, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.515+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 33.0 in stage 34.0 (TID 210) in 16 ms on 172.18.0.8 (executor 0) (34/50)
[2024-11-03T11:32:14.539+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 36.0 in stage 34.0 (TID 213) (172.18.0.8, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.540+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 34.0 in stage 34.0 (TID 211) in 31 ms on 172.18.0.8 (executor 0) (35/50)
[2024-11-03T11:32:14.542+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 37.0 in stage 34.0 (TID 214) (172.18.0.8, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.543+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 35.0 in stage 34.0 (TID 212) in 30 ms on 172.18.0.8 (executor 0) (36/50)
[2024-11-03T11:32:14.556+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 38.0 in stage 34.0 (TID 215) (172.18.0.8, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.557+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 36.0 in stage 34.0 (TID 213) in 18 ms on 172.18.0.8 (executor 0) (37/50)
[2024-11-03T11:32:14.559+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 39.0 in stage 34.0 (TID 216) (172.18.0.8, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.560+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 37.0 in stage 34.0 (TID 214) in 17 ms on 172.18.0.8 (executor 0) (38/50)
[2024-11-03T11:32:14.572+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 40.0 in stage 34.0 (TID 217) (172.18.0.8, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.573+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 38.0 in stage 34.0 (TID 215) in 18 ms on 172.18.0.8 (executor 0) (39/50)
[2024-11-03T11:32:14.574+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 41.0 in stage 34.0 (TID 218) (172.18.0.8, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.575+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 39.0 in stage 34.0 (TID 216) in 16 ms on 172.18.0.8 (executor 0) (40/50)
[2024-11-03T11:32:14.589+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 42.0 in stage 34.0 (TID 219) (172.18.0.8, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.590+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 41.0 in stage 34.0 (TID 218) in 16 ms on 172.18.0.8 (executor 0) (41/50)
[2024-11-03T11:32:14.591+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 43.0 in stage 34.0 (TID 220) (172.18.0.8, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.592+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 40.0 in stage 34.0 (TID 217) in 19 ms on 172.18.0.8 (executor 0) (42/50)
[2024-11-03T11:32:14.604+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 44.0 in stage 34.0 (TID 221) (172.18.0.8, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.605+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 42.0 in stage 34.0 (TID 219) in 17 ms on 172.18.0.8 (executor 0) (43/50)
[2024-11-03T11:32:14.609+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 45.0 in stage 34.0 (TID 222) (172.18.0.8, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.610+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 43.0 in stage 34.0 (TID 220) in 19 ms on 172.18.0.8 (executor 0) (44/50)
[2024-11-03T11:32:14.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 46.0 in stage 34.0 (TID 223) (172.18.0.8, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 44.0 in stage 34.0 (TID 221) in 15 ms on 172.18.0.8 (executor 0) (45/50)
[2024-11-03T11:32:14.626+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 47.0 in stage 34.0 (TID 224) (172.18.0.8, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.627+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 45.0 in stage 34.0 (TID 222) in 17 ms on 172.18.0.8 (executor 0) (46/50)
[2024-11-03T11:32:14.638+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 48.0 in stage 34.0 (TID 225) (172.18.0.8, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.639+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 46.0 in stage 34.0 (TID 223) in 20 ms on 172.18.0.8 (executor 0) (47/50)
[2024-11-03T11:32:14.644+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 49.0 in stage 34.0 (TID 226) (172.18.0.8, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-03T11:32:14.645+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 47.0 in stage 34.0 (TID 224) in 20 ms on 172.18.0.8 (executor 0) (48/50)
[2024-11-03T11:32:14.653+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 48.0 in stage 34.0 (TID 225) in 16 ms on 172.18.0.8 (executor 0) (49/50)
[2024-11-03T11:32:14.662+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 49.0 in stage 34.0 (TID 226) in 18 ms on 172.18.0.8 (executor 0) (50/50)
[2024-11-03T11:32:14.663+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-11-03T11:32:14.664+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: ResultStage 34 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.519 s
[2024-11-03T11:32:14.665+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:14.665+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-11-03T11:32:14.666+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Job 20 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.525794 s
[2024-11-03T11:32:14.685+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO CodeGenerator: Code generated in 14.480519 ms
[2024-11-03T11:32:14.695+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO OptimisticTransaction: [tableId=34f0700a,txnId=e3e22d5b] Attempting to commit version 4 with 4 actions with Serializable isolation level
[2024-11-03T11:32:14.782+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DeltaLog: Creating a new snapshot v4 for commit version 4
[2024-11-03T11:32:14.782+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DeltaLog: Loading version 4.
[2024-11-03T11:32:14.786+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 5, totalFileSize: 11475)
[2024-11-03T11:32:14.818+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:32:14.818+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:32:14.835+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 205.0 KiB, free 431.3 MiB)
[2024-11-03T11:32:14.842+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 431.2 MiB)
[2024-11-03T11:32:14.843+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on eb88dbfa1959:43169 (size: 36.1 KiB, free: 433.0 MiB)
[2024-11-03T11:32:14.843+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Created broadcast 33 from toString at String.java:2951
[2024-11-03T11:32:14.844+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10491497 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:14.856+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-03T11:32:14.857+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Got job 21 (toString at String.java:2951) with 2 output partitions
[2024-11-03T11:32:14.858+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Final stage: ResultStage 35 (toString at String.java:2951)
[2024-11-03T11:32:14.858+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:14.859+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:14.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[98] at toString at String.java:2951), which has no missing parents
[2024-11-03T11:32:14.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 50.4 KiB, free 431.2 MiB)
[2024-11-03T11:32:14.861+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 431.2 MiB)
[2024-11-03T11:32:14.862+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on eb88dbfa1959:43169 (size: 15.5 KiB, free: 433.0 MiB)
[2024-11-03T11:32:14.862+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:14.863+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 35 (MapPartitionsRDD[98] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:14.864+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Adding task set 35.0 with 2 tasks resource profile 0
[2024-11-03T11:32:14.864+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 227) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8201 bytes)
[2024-11-03T11:32:14.865+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 228) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 8078 bytes)
[2024-11-03T11:32:14.875+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.8:37445 (size: 15.5 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:14.886+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.8:37445 (size: 36.1 KiB, free: 1047.4 MiB)
[2024-11-03T11:32:14.922+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 228) in 58 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:14.931+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 227) in 68 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:14.932+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-11-03T11:32:14.933+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: ResultStage 35 (toString at String.java:2951) finished in 0.074 s
[2024-11-03T11:32:14.933+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:14.934+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2024-11-03T11:32:14.935+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DAGScheduler: Job 21 finished: toString at String.java:2951, took 0.076734 s
[2024-11-03T11:32:14.941+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] Created snapshot Snapshot(path=s3a://lakehouse/silver/merged_data/_delta_log, version=4, metadata=Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292)), logSegment=LogSegment(s3a://lakehouse/silver/merged_data/_delta_log,4,WrappedArray(S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1939; replication=1; blocksize=33554432; modification_time=1730633270778; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ca3327c2494957c46b6efdff87392535 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=1572; replication=1; blocksize=33554432; modification_time=1730633353899; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=499135ae9e973681b348d18a25ba2f7a versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633363129; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=797865593c6dd74174b6a29b3e403bc4 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=1898; replication=1; blocksize=33554432; modification_time=1730633525569; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=32ac357a7d7374f428afc596c592d94e versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633534742; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=0409b4b48160af88a712ae87a04df1c8 versionId=null),None,1730633534742), checksumOpt=None)
[2024-11-03T11:32:14.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/silver/merged_data/_delta_log, version=4, metadata=Metadata(34f0700a-bf72-40fe-be52-ebc588a7cc70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730633259292)), logSegment=LogSegment(s3a://lakehouse/silver/merged_data/_delta_log,4,WrappedArray(S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1939; replication=1; blocksize=33554432; modification_time=1730633270778; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=ca3327c2494957c46b6efdff87392535 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=1572; replication=1; blocksize=33554432; modification_time=1730633353899; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=499135ae9e973681b348d18a25ba2f7a versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633363129; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=797865593c6dd74174b6a29b3e403bc4 versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=1898; replication=1; blocksize=33554432; modification_time=1730633525569; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=32ac357a7d7374f428afc596c592d94e versionId=null, S3AFileStatus{path=s3a://lakehouse/silver/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=3033; replication=1; blocksize=33554432; modification_time=1730633534742; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=0409b4b48160af88a712ae87a04df1c8 versionId=null),None,1730633534742), checksumOpt=None)
[2024-11-03T11:32:14.944+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MapPartitionsRDD: Removing RDD 56 from persistence list
[2024-11-03T11:32:14.945+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Compute snapshot for version: 4
[2024-11-03T11:32:14.946+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManager: Removing RDD 56
[2024-11-03T11:32:14.947+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 204.7 KiB, free 431.0 MiB)
[2024-11-03T11:32:14.953+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 430.9 MiB)
[2024-11-03T11:32:14.953+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on eb88dbfa1959:43169 (size: 36.0 KiB, free: 433.0 MiB)
[2024-11-03T11:32:14.954+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:14 INFO SparkContext: Created broadcast 35 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:15.029+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_32_piece0 on eb88dbfa1959:43169 in memory (size: 138.6 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.030+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.8:37445 in memory (size: 138.6 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:15.036+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_33_piece0 on eb88dbfa1959:43169 in memory (size: 36.1 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.036+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.8:37445 in memory (size: 36.1 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:15.040+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_34_piece0 on eb88dbfa1959:43169 in memory (size: 15.5 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.041+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.8:37445 in memory (size: 15.5 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:15.112+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO FileSourceStrategy: Pushed Filters:
[2024-11-03T11:32:15.112+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-03T11:32:15.155+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 205.0 KiB, free 431.7 MiB)
[2024-11-03T11:32:15.169+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 431.7 MiB)
[2024-11-03T11:32:15.169+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on eb88dbfa1959:43169 (size: 36.1 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.170+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO SparkContext: Created broadcast 36 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:15.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10491497 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-03T11:32:15.186+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Registering RDD 102 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 9
[2024-11-03T11:32:15.187+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Got map stage job 22 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-03T11:32:15.188+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Final stage: ShuffleMapStage 36 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:15.188+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Parents of final stage: List()
[2024-11-03T11:32:15.189+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:15.189+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[102] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:15.193+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 138.3 KiB, free 431.6 MiB)
[2024-11-03T11:32:15.195+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 431.5 MiB)
[2024-11-03T11:32:15.196+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on eb88dbfa1959:43169 (size: 39.0 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.196+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:15.197+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[102] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-03T11:32:15.198+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSchedulerImpl: Adding task set 36.0 with 2 tasks resource profile 0
[2024-11-03T11:32:15.199+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 229) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 8190 bytes)
[2024-11-03T11:32:15.200+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 230) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 8067 bytes)
[2024-11-03T11:32:15.226+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.8:37445 (size: 39.0 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:15.249+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.8:37445 (size: 36.1 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:15.293+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 230) in 94 ms on 172.18.0.8 (executor 0) (1/2)
[2024-11-03T11:32:15.306+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 229) in 108 ms on 172.18.0.8 (executor 0) (2/2)
[2024-11-03T11:32:15.307+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2024-11-03T11:32:15.308+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: ShuffleMapStage 36 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.119 s
[2024-11-03T11:32:15.309+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:15.309+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:15.310+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:15.311+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:15.369+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_37_piece0 on eb88dbfa1959:43169 in memory (size: 39.0 KiB, free: 433.1 MiB)
[2024-11-03T11:32:15.374+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.8:37445 in memory (size: 39.0 KiB, free: 1047.6 MiB)
[2024-11-03T11:32:15.474+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Registering RDD 112 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 10
[2024-11-03T11:32:15.475+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Got map stage job 23 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-03T11:32:15.476+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Final stage: ShuffleMapStage 38 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:15.477+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
[2024-11-03T11:32:15.477+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:15.478+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[112] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:15.525+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 517.1 KiB, free 431.2 MiB)
[2024-11-03T11:32:15.527+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 431.1 MiB)
[2024-11-03T11:32:15.528+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on eb88dbfa1959:43169 (size: 123.3 KiB, free: 433.0 MiB)
[2024-11-03T11:32:15.529+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:15.530+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[112] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-03T11:32:15.530+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSchedulerImpl: Adding task set 38.0 with 50 tasks resource profile 0
[2024-11-03T11:32:15.531+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 8.0 in stage 38.0 (TID 231) (172.18.0.8, executor 0, partition 8, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.532+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 26.0 in stage 38.0 (TID 232) (172.18.0.8, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.541+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.8:37445 (size: 123.3 KiB, free: 1047.5 MiB)
[2024-11-03T11:32:15.555+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.18.0.8:47292
[2024-11-03T11:32:15.602+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_8 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.603+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_26 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 32.0 in stage 38.0 (TID 233) (172.18.0.8, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.621+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 8.0 in stage 38.0 (TID 231) in 90 ms on 172.18.0.8 (executor 0) (1/50)
[2024-11-03T11:32:15.622+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 42.0 in stage 38.0 (TID 234) (172.18.0.8, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.623+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 26.0 in stage 38.0 (TID 232) in 91 ms on 172.18.0.8 (executor 0) (2/50)
[2024-11-03T11:32:15.651+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_32 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.654+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_42 in memory on 172.18.0.8:37445 (size: 947.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.668+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 44.0 in stage 38.0 (TID 235) (172.18.0.8, executor 0, partition 44, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.669+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 32.0 in stage 38.0 (TID 233) in 48 ms on 172.18.0.8 (executor 0) (3/50)
[2024-11-03T11:32:15.670+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 48.0 in stage 38.0 (TID 236) (172.18.0.8, executor 0, partition 48, NODE_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.670+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 42.0 in stage 38.0 (TID 234) in 49 ms on 172.18.0.8 (executor 0) (4/50)
[2024-11-03T11:32:15.700+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_44 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.701+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_48 in memory on 172.18.0.8:37445 (size: 301.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.718+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 237) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.719+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 48.0 in stage 38.0 (TID 236) in 49 ms on 172.18.0.8 (executor 0) (5/50)
[2024-11-03T11:32:15.720+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 238) (172.18.0.8, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.721+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 44.0 in stage 38.0 (TID 235) in 53 ms on 172.18.0.8 (executor 0) (6/50)
[2024-11-03T11:32:15.759+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_0 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.760+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_1 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.778+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 2.0 in stage 38.0 (TID 239) (172.18.0.8, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.778+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 237) in 60 ms on 172.18.0.8 (executor 0) (7/50)
[2024-11-03T11:32:15.783+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 3.0 in stage 38.0 (TID 240) (172.18.0.8, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.784+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 238) in 65 ms on 172.18.0.8 (executor 0) (8/50)
[2024-11-03T11:32:15.820+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_2 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.827+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_3 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.844+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 4.0 in stage 38.0 (TID 241) (172.18.0.8, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.845+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 2.0 in stage 38.0 (TID 239) in 68 ms on 172.18.0.8 (executor 0) (9/50)
[2024-11-03T11:32:15.857+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 5.0 in stage 38.0 (TID 242) (172.18.0.8, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 3.0 in stage 38.0 (TID 240) in 77 ms on 172.18.0.8 (executor 0) (10/50)
[2024-11-03T11:32:15.899+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_4 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.901+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_5 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.924+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 6.0 in stage 38.0 (TID 243) (172.18.0.8, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.925+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 5.0 in stage 38.0 (TID 242) in 68 ms on 172.18.0.8 (executor 0) (11/50)
[2024-11-03T11:32:15.928+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 7.0 in stage 38.0 (TID 244) (172.18.0.8, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.929+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 4.0 in stage 38.0 (TID 241) in 82 ms on 172.18.0.8 (executor 0) (12/50)
[2024-11-03T11:32:15.968+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_7 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.969+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO BlockManagerInfo: Added rdd_109_6 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:15.993+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 9.0 in stage 38.0 (TID 245) (172.18.0.8, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.994+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 7.0 in stage 38.0 (TID 244) in 68 ms on 172.18.0.8 (executor 0) (13/50)
[2024-11-03T11:32:15.996+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Starting task 10.0 in stage 38.0 (TID 246) (172.18.0.8, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:15.997+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:15 INFO TaskSetManager: Finished task 6.0 in stage 38.0 (TID 243) in 73 ms on 172.18.0.8 (executor 0) (14/50)
[2024-11-03T11:32:16.041+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_10 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.042+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_9 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.062+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 11.0 in stage 38.0 (TID 247) (172.18.0.8, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.063+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 10.0 in stage 38.0 (TID 246) in 68 ms on 172.18.0.8 (executor 0) (15/50)
[2024-11-03T11:32:16.064+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 12.0 in stage 38.0 (TID 248) (172.18.0.8, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.065+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 9.0 in stage 38.0 (TID 245) in 73 ms on 172.18.0.8 (executor 0) (16/50)
[2024-11-03T11:32:16.101+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_11 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.106+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_12 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.123+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 13.0 in stage 38.0 (TID 249) (172.18.0.8, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.124+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 11.0 in stage 38.0 (TID 247) in 63 ms on 172.18.0.8 (executor 0) (17/50)
[2024-11-03T11:32:16.131+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 14.0 in stage 38.0 (TID 250) (172.18.0.8, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.132+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 12.0 in stage 38.0 (TID 248) in 69 ms on 172.18.0.8 (executor 0) (18/50)
[2024-11-03T11:32:16.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_13 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.176+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_14 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.191+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 15.0 in stage 38.0 (TID 251) (172.18.0.8, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.191+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 13.0 in stage 38.0 (TID 249) in 68 ms on 172.18.0.8 (executor 0) (19/50)
[2024-11-03T11:32:16.200+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 16.0 in stage 38.0 (TID 252) (172.18.0.8, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.201+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 14.0 in stage 38.0 (TID 250) in 71 ms on 172.18.0.8 (executor 0) (20/50)
[2024-11-03T11:32:16.229+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_15 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.239+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_16 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.253+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 17.0 in stage 38.0 (TID 253) (172.18.0.8, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.254+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 15.0 in stage 38.0 (TID 251) in 64 ms on 172.18.0.8 (executor 0) (21/50)
[2024-11-03T11:32:16.260+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 18.0 in stage 38.0 (TID 254) (172.18.0.8, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.261+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 16.0 in stage 38.0 (TID 252) in 61 ms on 172.18.0.8 (executor 0) (22/50)
[2024-11-03T11:32:16.311+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_17 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.314+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_18 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.333+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 19.0 in stage 38.0 (TID 255) (172.18.0.8, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.334+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 17.0 in stage 38.0 (TID 253) in 81 ms on 172.18.0.8 (executor 0) (23/50)
[2024-11-03T11:32:16.338+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 20.0 in stage 38.0 (TID 256) (172.18.0.8, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.343+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 18.0 in stage 38.0 (TID 254) in 79 ms on 172.18.0.8 (executor 0) (24/50)
[2024-11-03T11:32:16.392+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_20 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.396+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_19 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.423+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 21.0 in stage 38.0 (TID 257) (172.18.0.8, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.424+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 20.0 in stage 38.0 (TID 256) in 86 ms on 172.18.0.8 (executor 0) (25/50)
[2024-11-03T11:32:16.438+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 22.0 in stage 38.0 (TID 258) (172.18.0.8, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.441+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 19.0 in stage 38.0 (TID 255) in 106 ms on 172.18.0.8 (executor 0) (26/50)
[2024-11-03T11:32:16.474+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_21 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.484+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_22 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.496+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 23.0 in stage 38.0 (TID 259) (172.18.0.8, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.497+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 21.0 in stage 38.0 (TID 257) in 74 ms on 172.18.0.8 (executor 0) (27/50)
[2024-11-03T11:32:16.508+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 24.0 in stage 38.0 (TID 260) (172.18.0.8, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.509+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 22.0 in stage 38.0 (TID 258) in 71 ms on 172.18.0.8 (executor 0) (28/50)
[2024-11-03T11:32:16.537+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_23 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.542+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_24 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.560+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 25.0 in stage 38.0 (TID 261) (172.18.0.8, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.561+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 23.0 in stage 38.0 (TID 259) in 65 ms on 172.18.0.8 (executor 0) (29/50)
[2024-11-03T11:32:16.562+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 27.0 in stage 38.0 (TID 262) (172.18.0.8, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.562+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 24.0 in stage 38.0 (TID 260) in 54 ms on 172.18.0.8 (executor 0) (30/50)
[2024-11-03T11:32:16.598+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_27 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.599+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_25 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.619+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 28.0 in stage 38.0 (TID 263) (172.18.0.8, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 25.0 in stage 38.0 (TID 261) in 60 ms on 172.18.0.8 (executor 0) (31/50)
[2024-11-03T11:32:16.620+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 29.0 in stage 38.0 (TID 264) (172.18.0.8, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.621+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 27.0 in stage 38.0 (TID 262) in 60 ms on 172.18.0.8 (executor 0) (32/50)
[2024-11-03T11:32:16.651+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_28 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.658+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_29 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.673+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 30.0 in stage 38.0 (TID 265) (172.18.0.8, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.674+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 28.0 in stage 38.0 (TID 263) in 56 ms on 172.18.0.8 (executor 0) (33/50)
[2024-11-03T11:32:16.679+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 31.0 in stage 38.0 (TID 266) (172.18.0.8, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.680+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 29.0 in stage 38.0 (TID 264) in 60 ms on 172.18.0.8 (executor 0) (34/50)
[2024-11-03T11:32:16.710+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_30 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.718+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_31 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.732+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 33.0 in stage 38.0 (TID 267) (172.18.0.8, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.733+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 30.0 in stage 38.0 (TID 265) in 59 ms on 172.18.0.8 (executor 0) (35/50)
[2024-11-03T11:32:16.741+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 34.0 in stage 38.0 (TID 268) (172.18.0.8, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.742+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 31.0 in stage 38.0 (TID 266) in 63 ms on 172.18.0.8 (executor 0) (36/50)
[2024-11-03T11:32:16.778+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_33 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.788+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_34 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.803+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 35.0 in stage 38.0 (TID 269) (172.18.0.8, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.804+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 33.0 in stage 38.0 (TID 267) in 72 ms on 172.18.0.8 (executor 0) (37/50)
[2024-11-03T11:32:16.809+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 36.0 in stage 38.0 (TID 270) (172.18.0.8, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.810+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 34.0 in stage 38.0 (TID 268) in 68 ms on 172.18.0.8 (executor 0) (38/50)
[2024-11-03T11:32:16.841+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_35 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.851+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_36 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.860+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 37.0 in stage 38.0 (TID 271) (172.18.0.8, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.861+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 35.0 in stage 38.0 (TID 269) in 57 ms on 172.18.0.8 (executor 0) (39/50)
[2024-11-03T11:32:16.875+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 38.0 in stage 38.0 (TID 272) (172.18.0.8, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.876+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 36.0 in stage 38.0 (TID 270) in 67 ms on 172.18.0.8 (executor 0) (40/50)
[2024-11-03T11:32:16.899+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_37 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.922+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 39.0 in stage 38.0 (TID 273) (172.18.0.8, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.923+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 37.0 in stage 38.0 (TID 271) in 63 ms on 172.18.0.8 (executor 0) (41/50)
[2024-11-03T11:32:16.924+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_38 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 40.0 in stage 38.0 (TID 274) (172.18.0.8, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.943+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 38.0 in stage 38.0 (TID 272) in 68 ms on 172.18.0.8 (executor 0) (42/50)
[2024-11-03T11:32:16.968+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_39 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.990+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Starting task 41.0 in stage 38.0 (TID 275) (172.18.0.8, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:16.991+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO BlockManagerInfo: Added rdd_109_40 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:16.992+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:16 INFO TaskSetManager: Finished task 39.0 in stage 38.0 (TID 273) in 68 ms on 172.18.0.8 (executor 0) (43/50)
[2024-11-03T11:32:17.017+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 43.0 in stage 38.0 (TID 276) (172.18.0.8, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:17.018+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 40.0 in stage 38.0 (TID 274) in 76 ms on 172.18.0.8 (executor 0) (44/50)
[2024-11-03T11:32:17.037+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_41 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.050+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_43 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.053+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 45.0 in stage 38.0 (TID 277) (172.18.0.8, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:17.054+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 41.0 in stage 38.0 (TID 275) in 64 ms on 172.18.0.8 (executor 0) (45/50)
[2024-11-03T11:32:17.068+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 46.0 in stage 38.0 (TID 278) (172.18.0.8, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:17.068+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 43.0 in stage 38.0 (TID 276) in 51 ms on 172.18.0.8 (executor 0) (46/50)
[2024-11-03T11:32:17.083+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_45 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.100+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_46 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.103+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 47.0 in stage 38.0 (TID 279) (172.18.0.8, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:17.104+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 45.0 in stage 38.0 (TID 277) in 51 ms on 172.18.0.8 (executor 0) (47/50)
[2024-11-03T11:32:17.121+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 49.0 in stage 38.0 (TID 280) (172.18.0.8, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-03T11:32:17.122+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 46.0 in stage 38.0 (TID 278) in 55 ms on 172.18.0.8 (executor 0) (48/50)
[2024-11-03T11:32:17.136+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_47 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.152+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 47.0 in stage 38.0 (TID 279) in 48 ms on 172.18.0.8 (executor 0) (49/50)
[2024-11-03T11:32:17.155+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added rdd_109_49 in memory on 172.18.0.8:37445 (size: 46.0 B, free: 1047.5 MiB)
[2024-11-03T11:32:17.169+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 49.0 in stage 38.0 (TID 280) in 48 ms on 172.18.0.8 (executor 0) (50/50)
[2024-11-03T11:32:17.170+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
[2024-11-03T11:32:17.170+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: ShuffleMapStage 38 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.691 s
[2024-11-03T11:32:17.171+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: looking for newly runnable stages
[2024-11-03T11:32:17.171+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: running: Set()
[2024-11-03T11:32:17.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: waiting: Set()
[2024-11-03T11:32:17.172+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: failed: Set()
[2024-11-03T11:32:17.195+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-03T11:32:17.196+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Got job 24 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-03T11:32:17.197+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Final stage: ResultStage 41 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-03T11:32:17.198+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
[2024-11-03T11:32:17.199+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Missing parents: List()
[2024-11-03T11:32:17.200+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[115] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-03T11:32:17.205+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 452.3 KiB, free 430.6 MiB)
[2024-11-03T11:32:17.207+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 430.5 MiB)
[2024-11-03T11:32:17.208+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on eb88dbfa1959:43169 (size: 108.1 KiB, free: 432.9 MiB)
[2024-11-03T11:32:17.208+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1540
[2024-11-03T11:32:17.209+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[115] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-03T11:32:17.210+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2024-11-03T11:32:17.211+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 281) (172.18.0.8, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-03T11:32:17.221+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.8:37445 (size: 108.1 KiB, free: 1047.4 MiB)
[2024-11-03T11:32:17.232+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.18.0.8:47292
[2024-11-03T11:32:17.259+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 281) in 49 ms on 172.18.0.8 (executor 0) (1/1)
[2024-11-03T11:32:17.260+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2024-11-03T11:32:17.261+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: ResultStage 41 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.061 s
[2024-11-03T11:32:17.261+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-03T11:32:17.262+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
[2024-11-03T11:32:17.263+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO DAGScheduler: Job 24 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.065617 s
[2024-11-03T11:32:17.274+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO Snapshot: [tableId=34f0700a-bf72-40fe-be52-ebc588a7cc70] DELTA: Done
[2024-11-03T11:32:17.275+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO OptimisticTransaction: [tableId=34f0700a,txnId=e3e22d5b] Committed delta #4 to s3a://lakehouse/silver/merged_data/_delta_log
[2024-11-03T11:32:17.278+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-11-03T11:32:17.294+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO SparkUI: Stopped Spark web UI at http://eb88dbfa1959:4040
[2024-11-03T11:32:17.303+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO StandaloneSchedulerBackend: Shutting down all executors
[2024-11-03T11:32:17.303+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2024-11-03T11:32:17.331+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-11-03T11:32:17.359+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO MemoryStore: MemoryStore cleared
[2024-11-03T11:32:17.360+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManager: BlockManager stopped
[2024-11-03T11:32:17.364+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-11-03T11:32:17.374+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-11-03T11:32:17.443+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:17 INFO SparkContext: Successfully stopped SparkContext
[2024-11-03T11:32:18.016+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO ShutdownHookManager: Shutdown hook called
[2024-11-03T11:32:18.017+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-70211f4d-0338-4421-bad2-c3c6c83f862c
[2024-11-03T11:32:18.021+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3af8ea18-8bc9-4a0b-ac5c-336c18568c93
[2024-11-03T11:32:18.026+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-3af8ea18-8bc9-4a0b-ac5c-336c18568c93/pyspark-2e7728a8-7727-4530-a506-cc7f6768a770
[2024-11-03T11:32:18.034+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2024-11-03T11:32:18.035+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2024-11-03T11:32:18.035+0000] {spark_submit.py:579} INFO - 24/11/03 11:32:18 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2024-11-03T11:32:18.127+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=mergeeeee, task_id=merge_id, execution_date=20241102T000000, start_date=20241103T113119, end_date=20241103T113218
[2024-11-03T11:32:18.178+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-11-03T11:32:18.193+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
