[2024-11-04T11:27:15.237+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: mergeeeee.merge_id manual__2024-11-04T11:27:13.643739+00:00 [queued]>
[2024-11-04T11:27:15.247+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: mergeeeee.merge_id manual__2024-11-04T11:27:13.643739+00:00 [queued]>
[2024-11-04T11:27:15.247+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2024-11-04T11:27:15.274+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): merge_id> on 2024-11-04 11:27:13.643739+00:00
[2024-11-04T11:27:15.281+0000] {standard_task_runner.py:57} INFO - Started process 9505 to run task
[2024-11-04T11:27:15.283+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'mergeeeee', 'merge_id', 'manual__2024-11-04T11:27:13.643739+00:00', '--job-id', '200', '--raw', '--subdir', 'DAGS_FOLDER/merge_hihi.py', '--cfg-path', '/tmp/tmp_rdjaab2']
[2024-11-04T11:27:15.286+0000] {standard_task_runner.py:85} INFO - Job 200: Subtask merge_id
[2024-11-04T11:27:15.333+0000] {task_command.py:415} INFO - Running <TaskInstance: mergeeeee.merge_id manual__2024-11-04T11:27:13.643739+00:00 [running]> on host eb88dbfa1959
[2024-11-04T11:27:15.431+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='mergeeeee' AIRFLOW_CTX_TASK_ID='merge_id' AIRFLOW_CTX_EXECUTION_DATE='2024-11-04T11:27:13.643739+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-04T11:27:13.643739+00:00'
[2024-11-04T11:27:15.441+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2024-11-04T11:27:15.443+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --jars /opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/s3-2.18.41.jar,/opt/***/jars/aws-java-sdk-1.12.367.jar,/opt/***/jars/delta-core_2.12-2.4.0.jar,/opt/***/jars/delta-storage-2.2.0.jar, --packages org.apache.hadoop:hadoop-aws:3.3.4 --num-executors 2 --total-executor-cores 2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name arrow-spark --deploy-mode client /opt/***/jobs/python/test_merge.py s3a://lakehouse/bronze/keywords.parquet s3a://lakehouse/bronze/movies.parquet s3a://lakehouse/bronze/credits.parquet s3a://lakehouse/merge_data-movies/merged_data s3a://lakehouse/gold/gold_data
[2024-11-04T11:27:15.542+0000] {spark_submit.py:579} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-11-04T11:27:18.126+0000] {spark_submit.py:579} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-04T11:27:18.348+0000] {spark_submit.py:579} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-04T11:27:18.349+0000] {spark_submit.py:579} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-04T11:27:18.355+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-11-04T11:27:18.357+0000] {spark_submit.py:579} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-44d787ae-1334-4be1-9fa6-072401bcea7a;1.0
[2024-11-04T11:27:18.357+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-04T11:27:18.586+0000] {spark_submit.py:579} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in spark-list
[2024-11-04T11:27:18.629+0000] {spark_submit.py:579} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2024-11-04T11:27:18.674+0000] {spark_submit.py:579} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2024-11-04T11:27:18.712+0000] {spark_submit.py:579} INFO - :: resolution report :: resolve 342ms :: artifacts dl 14ms
[2024-11-04T11:27:18.713+0000] {spark_submit.py:579} INFO - :: modules in use:
[2024-11-04T11:27:18.714+0000] {spark_submit.py:579} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2024-11-04T11:27:18.715+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from spark-list in [default]
[2024-11-04T11:27:18.716+0000] {spark_submit.py:579} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2024-11-04T11:27:18.717+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-04T11:27:18.718+0000] {spark_submit.py:579} INFO - |                  |            modules            ||   artifacts   |
[2024-11-04T11:27:18.719+0000] {spark_submit.py:579} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-04T11:27:18.720+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-04T11:27:18.720+0000] {spark_submit.py:579} INFO - |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2024-11-04T11:27:18.721+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-04T11:27:18.723+0000] {spark_submit.py:579} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-44d787ae-1334-4be1-9fa6-072401bcea7a
[2024-11-04T11:27:18.724+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-04T11:27:18.734+0000] {spark_submit.py:579} INFO - 0 artifacts copied, 3 already retrieved (0kB/12ms)
[2024-11-04T11:27:19.170+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-04T11:27:21.223+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SparkContext: Running Spark version 3.4.3
[2024-11-04T11:27:21.273+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceUtils: ==============================================================
[2024-11-04T11:27:21.274+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-04T11:27:21.274+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceUtils: ==============================================================
[2024-11-04T11:27:21.275+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SparkContext: Submitted application: MergeData
[2024-11-04T11:27:21.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-04T11:27:21.331+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2024-11-04T11:27:21.336+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-04T11:27:21.439+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SecurityManager: Changing view acls to: ***
[2024-11-04T11:27:21.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SecurityManager: Changing modify acls to: ***
[2024-11-04T11:27:21.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SecurityManager: Changing view acls groups to:
[2024-11-04T11:27:21.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SecurityManager: Changing modify acls groups to:
[2024-11-04T11:27:21.443+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-04T11:27:21.936+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO Utils: Successfully started service 'sparkDriver' on port 38305.
[2024-11-04T11:27:21.977+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:21 INFO SparkEnv: Registering MapOutputTracker
[2024-11-04T11:27:22.043+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-04T11:27:22.064+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-04T11:27:22.065+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-04T11:27:22.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-04T11:27:22.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e76e094c-a713-4f03-99e3-e434c792021a
[2024-11-04T11:27:22.109+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-04T11:27:22.129+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-04T11:27:22.299+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-04T11:27:22.369+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-04T11:27:22.420+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///opt/***/jars/hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:38305/jars/hadoop-aws-3.3.4.jar with timestamp 1730719641213
[2024-11-04T11:27:22.422+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///opt/***/jars/s3-2.18.41.jar at spark://eb88dbfa1959:38305/jars/s3-2.18.41.jar with timestamp 1730719641213
[2024-11-04T11:27:22.424+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///opt/***/jars/aws-java-sdk-1.12.367.jar at spark://eb88dbfa1959:38305/jars/aws-java-sdk-1.12.367.jar with timestamp 1730719641213
[2024-11-04T11:27:22.426+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///opt/***/jars/delta-core_2.12-2.4.0.jar at spark://eb88dbfa1959:38305/jars/delta-core_2.12-2.4.0.jar with timestamp 1730719641213
[2024-11-04T11:27:22.428+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///opt/***/jars/delta-storage-2.2.0.jar at spark://eb88dbfa1959:38305/jars/delta-storage-2.2.0.jar with timestamp 1730719641213
[2024-11-04T11:27:22.428+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:38305/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1730719641213
[2024-11-04T11:27:22.429+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://eb88dbfa1959:38305/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1730719641213
[2024-11-04T11:27:22.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://eb88dbfa1959:38305/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1730719641213
[2024-11-04T11:27:22.434+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://eb88dbfa1959:38305/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1730719641213
[2024-11-04T11:27:22.437+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-50c2dd55-31ff-4064-add0-ee5fce999fcf/userFiles-b0925ce6-f934-4fa1-a669-075a128b99f4/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-11-04T11:27:22.453+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://eb88dbfa1959:38305/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1730719641213
[2024-11-04T11:27:22.454+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:22 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-50c2dd55-31ff-4064-add0-ee5fce999fcf/userFiles-b0925ce6-f934-4fa1-a669-075a128b99f4/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2024-11-04T11:27:23.145+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://eb88dbfa1959:38305/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1730719641213
[2024-11-04T11:27:23.146+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-50c2dd55-31ff-4064-add0-ee5fce999fcf/userFiles-b0925ce6-f934-4fa1-a669-075a128b99f4/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-11-04T11:27:23.574+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-04T11:27:23.625+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 30 ms (0 ms spent in bootstraps)
[2024-11-04T11:27:23.830+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241104112723-0014
[2024-11-04T11:27:23.846+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36409.
[2024-11-04T11:27:23.847+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO NettyBlockTransferService: Server created on eb88dbfa1959:36409
[2024-11-04T11:27:23.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-04T11:27:23.864+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, eb88dbfa1959, 36409, None)
[2024-11-04T11:27:23.872+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO BlockManagerMasterEndpoint: Registering block manager eb88dbfa1959:36409 with 434.4 MiB RAM, BlockManagerId(driver, eb88dbfa1959, 36409, None)
[2024-11-04T11:27:23.877+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, eb88dbfa1959, 36409, None)
[2024-11-04T11:27:23.879+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, eb88dbfa1959, 36409, None)
[2024-11-04T11:27:24.171+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:24 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-04T11:27:24.615+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-04T11:27:24.619+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:24 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2024-11-04T11:27:25.438+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241104112723-0014/0 on worker-20241104104415-172.18.0.5-39523 (172.18.0.5:39523) with 2 core(s)
[2024-11-04T11:27:25.443+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20241104112723-0014/0 on hostPort 172.18.0.5:39523 with 2 core(s), 2.0 GiB RAM
[2024-11-04T11:27:25.523+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241104112723-0014/0 is now RUNNING
[2024-11-04T11:27:26.084+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:26 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2024-11-04T11:27:26.107+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:26 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2024-11-04T11:27:26.107+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:26 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2024-11-04T11:27:27.936+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:27 INFO InMemoryFileIndex: It took 130 ms to list leaf files for 1 paths.
[2024-11-04T11:27:28.393+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:39048) with ID 0,  ResourceProfileId 0
[2024-11-04T11:27:28.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:28.495+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:35227 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.5, 35227, None)
[2024-11-04T11:27:28.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:27:28.507+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:28.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:28.511+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:28.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:28.603+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 106.8 KiB, free 434.3 MiB)
[2024-11-04T11:27:28.671+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.8 KiB, free 434.3 MiB)
[2024-11-04T11:27:28.677+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on eb88dbfa1959:36409 (size: 38.8 KiB, free: 434.4 MiB)
[2024-11-04T11:27:28.682+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:28.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:28.716+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-04T11:27:31.202+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7501 bytes)
[2024-11-04T11:27:31.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:35227 (size: 38.8 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:33.055+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1865 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:33.057+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-04T11:27:33.063+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 4.523 s
[2024-11-04T11:27:33.066+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:33.067+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-04T11:27:33.069+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 4.585800 s
[2024-11-04T11:27:33.260+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on eb88dbfa1959:36409 in memory (size: 38.8 KiB, free: 434.4 MiB)
[2024-11-04T11:27:33.275+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:35227 in memory (size: 38.8 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:35.972+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:35 INFO DeltaLog: Loading version 5.
[2024-11-04T11:27:36.672+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:36 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 6, totalFileSize: 23335)
[2024-11-04T11:27:37.003+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:37.004+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:27:37.540+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO CodeGenerator: Code generated in 358.034221 ms
[2024-11-04T11:27:37.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 204.8 KiB, free 434.2 MiB)
[2024-11-04T11:27:37.572+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)
[2024-11-04T11:27:37.573+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.4 MiB)
[2024-11-04T11:27:37.576+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO SparkContext: Created broadcast 1 from toString at String.java:2951
[2024-11-04T11:27:37.606+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12594579 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:37.687+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-04T11:27:37.688+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Got job 1 (toString at String.java:2951) with 2 output partitions
[2024-11-04T11:27:37.689+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Final stage: ResultStage 1 (toString at String.java:2951)
[2024-11-04T11:27:37.690+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:37.690+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:37.691+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at toString at String.java:2951), which has no missing parents
[2024-11-04T11:27:37.700+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 50.4 KiB, free 434.1 MiB)
[2024-11-04T11:27:37.709+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.1 MiB)
[2024-11-04T11:27:37.711+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on eb88dbfa1959:36409 (size: 15.5 KiB, free: 434.3 MiB)
[2024-11-04T11:27:37.712+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:37.713+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:37.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2024-11-04T11:27:37.720+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8234 bytes)
[2024-11-04T11:27:37.721+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8234 bytes)
[2024-11-04T11:27:37.776+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.5:35227 (size: 15.5 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:38.553+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:38.826+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1109 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:27:38.827+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1106 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:27:38.827+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-04T11:27:38.829+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO DAGScheduler: ResultStage 1 (toString at String.java:2951) finished in 1.133 s
[2024-11-04T11:27:38.830+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:38.831+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-04T11:27:38.831+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO DAGScheduler: Job 1 finished: toString at String.java:2951, took 1.142995 s
[2024-11-04T11:27:38.923+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO CodeGenerator: Code generated in 53.281429 ms
[2024-11-04T11:27:38.932+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:38 INFO Snapshot: [tableId=bc2ee861-fe71-49a2-848e-6180c7f9f04b] Created snapshot Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=5, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,5,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null),None,1730719581904), checksumOpt=None)
[2024-11-04T11:27:39.055+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on eb88dbfa1959:36409 in memory (size: 15.5 KiB, free: 434.4 MiB)
[2024-11-04T11:27:39.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.18.0.5:35227 in memory (size: 15.5 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:39.195+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO OptimisticTransaction: [tableId=86213540,txnId=37026920] Updated metadata from - to Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780))
[2024-11-04T11:27:39.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(keywords)
[2024-11-04T11:27:39.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(keywords#1)
[2024-11-04T11:27:39.524+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-04T11:27:39.566+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:27:39.617+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO CodeGenerator: Code generated in 33.702635 ms
[2024-11-04T11:27:39.626+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 206.4 KiB, free 434.0 MiB)
[2024-11-04T11:27:39.643+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.9 MiB)
[2024-11-04T11:27:39.644+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on eb88dbfa1959:36409 (size: 36.6 KiB, free: 434.3 MiB)
[2024-11-04T11:27:39.646+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO SparkContext: Created broadcast 3 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:39.653+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:39.672+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.4 MiB)
[2024-11-04T11:27:39.677+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:39.749+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Registering RDD 9 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-11-04T11:27:39.754+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Got map stage job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:27:39.755+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:39.756+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:39.758+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:39.761+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:39.774+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 31.5 KiB, free 434.1 MiB)
[2024-11-04T11:27:39.783+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 434.1 MiB)
[2024-11-04T11:27:39.785+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on eb88dbfa1959:36409 (size: 14.0 KiB, free: 434.4 MiB)
[2024-11-04T11:27:39.786+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:39.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:39.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2024-11-04T11:27:39.791+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7912 bytes)
[2024-11-04T11:27:39.825+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.5:35227 (size: 14.0 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:40.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.5:35227 (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:40.770+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 980 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:40.771+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-04T11:27:40.774+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: ShuffleMapStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.011 s
[2024-11-04T11:27:40.776+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:27:40.777+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: running: Set()
[2024-11-04T11:27:40.778+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:27:40.779+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: failed: Set()
[2024-11-04T11:27:40.802+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1480508, minimum partition size: 1048576
[2024-11-04T11:27:40.926+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:40.929+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:27:40.929+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:40.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2024-11-04T11:27:40.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:40.931+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:40.943+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on eb88dbfa1959:36409 in memory (size: 14.0 KiB, free: 434.4 MiB)
[2024-11-04T11:27:40.946+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.5:35227 in memory (size: 14.0 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:40.976+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 352.4 KiB, free 433.8 MiB)
[2024-11-04T11:27:40.980+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 128.3 KiB, free 433.7 MiB)
[2024-11-04T11:27:40.982+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on eb88dbfa1959:36409 (size: 128.3 KiB, free: 434.2 MiB)
[2024-11-04T11:27:40.983+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:40.984+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:40.984+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2024-11-04T11:27:40.992+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:27:40.993+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:40 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 5) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:27:41.031+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.18.0.5:35227 (size: 128.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:41.280+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.5:39048
[2024-11-04T11:27:44.957+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 5) in 3965 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:27:44.958+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 3968 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:27:44.958+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-11-04T11:27:44.960+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 4.006 s
[2024-11-04T11:27:44.961+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:44.962+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2024-11-04T11:27:44.962+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 4.035759 s
[2024-11-04T11:27:44.963+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO FileFormatWriter: Start to commit write Job 86d48f75-f910-496a-91bc-50ea6ffb332f.
[2024-11-04T11:27:44.966+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO FileFormatWriter: Write Job 86d48f75-f910-496a-91bc-50ea6ffb332f committed. Elapsed time: 1 ms.
[2024-11-04T11:27:44.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO FileFormatWriter: Finished processing stats for write job 86d48f75-f910-496a-91bc-50ea6ffb332f.
[2024-11-04T11:27:44.987+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:44 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Compute snapshot for version: 5
[2024-11-04T11:27:45.000+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 204.5 KiB, free 433.5 MiB)
[2024-11-04T11:27:45.012+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on eb88dbfa1959:36409 in memory (size: 128.3 KiB, free: 434.4 MiB)
[2024-11-04T11:27:45.014+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)
[2024-11-04T11:27:45.015+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.18.0.5:35227 in memory (size: 128.3 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:45.016+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:45.017+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO SparkContext: Created broadcast 6 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:45.607+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:45.607+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:27:45.624+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-11-04T11:27:45.846+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO CodeGenerator: Code generated in 136.807128 ms
[2024-11-04T11:27:45.850+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 204.8 KiB, free 433.7 MiB)
[2024-11-04T11:27:45.863+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.7 MiB)
[2024-11-04T11:27:45.864+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:45.865+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO SparkContext: Created broadcast 7 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:45.867+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12594579 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:45.898+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Registering RDD 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2024-11-04T11:27:45.899+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Got map stage job 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-04T11:27:45.899+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:45.900+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:45.902+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:45.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[15] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:45.919+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 138.3 KiB, free 433.6 MiB)
[2024-11-04T11:27:45.927+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.5 MiB)
[2024-11-04T11:27:45.928+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:45.929+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:45.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[15] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:45.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2024-11-04T11:27:45.932+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8223 bytes)
[2024-11-04T11:27:45.933+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8223 bytes)
[2024-11-04T11:27:45.950+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:45 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:46.263+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:46.361+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 428 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:27:46.362+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 431 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:27:46.363+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-11-04T11:27:46.364+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.459 s
[2024-11-04T11:27:46.365+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:27:46.365+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO DAGScheduler: running: Set()
[2024-11-04T11:27:46.366+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:27:46.367+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO DAGScheduler: failed: Set()
[2024-11-04T11:27:46.419+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:46.422+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:46.741+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 19938 bytes
[2024-11-04T11:27:46.742+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO CodeGenerator: Code generated in 206.630561 ms
[2024-11-04T11:27:46.830+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:46 INFO CodeGenerator: Code generated in 60.902734 ms
[2024-11-04T11:27:47.265+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO CodeGenerator: Code generated in 106.654659 ms
[2024-11-04T11:27:47.276+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Registering RDD 25 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2024-11-04T11:27:47.276+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Got map stage job 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:27:47.277+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Final stage: ShuffleMapStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:47.277+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2024-11-04T11:27:47.286+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:47.288+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[25] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:47.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 517.1 KiB, free 433.2 MiB)
[2024-11-04T11:27:47.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 433.1 MiB)
[2024-11-04T11:27:47.415+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on eb88dbfa1959:36409 (size: 123.3 KiB, free: 434.2 MiB)
[2024-11-04T11:27:47.416+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:47.417+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[25] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:27:47.417+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO TaskSchedulerImpl: Adding task set 7.0 with 50 tasks resource profile 0
[2024-11-04T11:27:47.419+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 8) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:47.420+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 9) (172.18.0.5, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:47.436+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.5:35227 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:47.565+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.5:39048
[2024-11-04T11:27:48.017+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_1 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.018+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_3 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.195+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 10) (172.18.0.5, executor 0, partition 5, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.196+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 9) in 777 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:27:48.198+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 11.0 in stage 7.0 (TID 11) (172.18.0.5, executor 0, partition 11, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.199+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 8) in 780 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:27:48.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_5 in memory on 172.18.0.5:35227 (size: 1350.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.295+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_11 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.332+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 15.0 in stage 7.0 (TID 12) (172.18.0.5, executor 0, partition 15, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.333+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 10) in 139 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:27:48.337+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 18.0 in stage 7.0 (TID 13) (172.18.0.5, executor 0, partition 18, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.338+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 11.0 in stage 7.0 (TID 11) in 141 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:27:48.410+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_18 in memory on 172.18.0.5:35227 (size: 361.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_15 in memory on 172.18.0.5:35227 (size: 1367.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.448+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 22.0 in stage 7.0 (TID 14) (172.18.0.5, executor 0, partition 22, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.449+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 18.0 in stage 7.0 (TID 13) in 111 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:27:48.450+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 29.0 in stage 7.0 (TID 15) (172.18.0.5, executor 0, partition 29, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.451+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 15.0 in stage 7.0 (TID 12) in 119 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:27:48.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_29 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_22 in memory on 172.18.0.5:35227 (size: 371.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.544+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 32.0 in stage 7.0 (TID 16) (172.18.0.5, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.545+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 29.0 in stage 7.0 (TID 15) in 96 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:27:48.547+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 34.0 in stage 7.0 (TID 17) (172.18.0.5, executor 0, partition 34, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.548+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 22.0 in stage 7.0 (TID 14) in 100 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:27:48.633+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_34 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_32 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.694+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 42.0 in stage 7.0 (TID 18) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.695+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 34.0 in stage 7.0 (TID 17) in 148 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:27:48.697+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 19) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.698+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 32.0 in stage 7.0 (TID 16) in 153 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:27:48.775+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_0 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.819+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 20) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.820+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 19) in 124 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:27:48.917+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO BlockManagerInfo: Added rdd_22_2 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:48.969+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 21) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:48.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:48 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 20) in 150 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:27:49.040+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_42 in memory on 172.18.0.5:35227 (size: 835.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.047+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_4 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.075+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 22) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.076+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 42.0 in stage 7.0 (TID 18) in 383 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:27:49.085+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 23) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.086+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 21) in 118 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:27:49.147+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_6 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.158+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_7 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.180+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 8.0 in stage 7.0 (TID 24) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.181+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 22) in 106 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:27:49.193+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 9.0 in stage 7.0 (TID 25) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.194+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 23) in 109 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:27:49.242+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_8 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.261+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_9 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.276+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 10.0 in stage 7.0 (TID 26) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.277+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 8.0 in stage 7.0 (TID 24) in 97 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:27:49.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 12.0 in stage 7.0 (TID 27) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.297+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 9.0 in stage 7.0 (TID 25) in 105 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:27:49.338+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_10 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.362+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_12 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.376+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 13.0 in stage 7.0 (TID 28) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.377+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 10.0 in stage 7.0 (TID 26) in 102 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:27:49.393+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 14.0 in stage 7.0 (TID 29) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.394+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 12.0 in stage 7.0 (TID 27) in 98 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:27:49.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_13 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.447+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_14 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.461+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 16.0 in stage 7.0 (TID 30) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.462+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 13.0 in stage 7.0 (TID 28) in 85 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:27:49.476+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 17.0 in stage 7.0 (TID 31) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.476+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 14.0 in stage 7.0 (TID 29) in 83 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:27:49.522+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_16 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.532+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_17 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.560+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 19.0 in stage 7.0 (TID 32) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 16.0 in stage 7.0 (TID 30) in 101 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:27:49.574+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 20.0 in stage 7.0 (TID 33) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.575+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 17.0 in stage 7.0 (TID 31) in 99 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:27:49.623+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_19 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.633+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_20 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.650+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 21.0 in stage 7.0 (TID 34) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 19.0 in stage 7.0 (TID 32) in 90 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:27:49.660+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 23.0 in stage 7.0 (TID 35) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.660+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 20.0 in stage 7.0 (TID 33) in 87 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:27:49.703+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_21 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_23 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.732+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 24.0 in stage 7.0 (TID 36) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.733+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 21.0 in stage 7.0 (TID 34) in 84 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:27:49.747+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 25.0 in stage 7.0 (TID 37) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.748+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 23.0 in stage 7.0 (TID 35) in 89 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:27:49.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_24 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_25 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.821+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 26.0 in stage 7.0 (TID 38) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.823+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 24.0 in stage 7.0 (TID 36) in 90 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:27:49.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 27.0 in stage 7.0 (TID 39) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.852+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 25.0 in stage 7.0 (TID 37) in 106 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:27:49.894+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_26 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.912+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_27 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.924+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 28.0 in stage 7.0 (TID 40) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.925+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 26.0 in stage 7.0 (TID 38) in 104 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:27:49.943+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Starting task 30.0 in stage 7.0 (TID 41) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:49.944+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO TaskSetManager: Finished task 27.0 in stage 7.0 (TID 39) in 93 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:27:49.979+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_28 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:49.997+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:49 INFO BlockManagerInfo: Added rdd_22_30 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.006+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 31.0 in stage 7.0 (TID 42) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.007+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 28.0 in stage 7.0 (TID 40) in 84 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:27:50.023+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 33.0 in stage 7.0 (TID 43) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.023+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 30.0 in stage 7.0 (TID 41) in 81 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:27:50.053+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_31 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.077+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_33 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.084+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 35.0 in stage 7.0 (TID 44) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.085+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 31.0 in stage 7.0 (TID 42) in 78 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:27:50.106+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 36.0 in stage 7.0 (TID 45) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.106+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 33.0 in stage 7.0 (TID 43) in 84 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:27:50.142+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_35 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_36 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.173+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 37.0 in stage 7.0 (TID 46) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.174+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 35.0 in stage 7.0 (TID 44) in 91 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:27:50.194+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 38.0 in stage 7.0 (TID 47) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.195+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 36.0 in stage 7.0 (TID 45) in 90 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:27:50.223+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_37 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_38 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.251+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 39.0 in stage 7.0 (TID 48) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.252+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 37.0 in stage 7.0 (TID 46) in 79 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:27:50.278+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 40.0 in stage 7.0 (TID 49) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 38.0 in stage 7.0 (TID 47) in 86 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:27:50.321+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_39 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.336+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_40 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.356+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 41.0 in stage 7.0 (TID 50) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.357+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 39.0 in stage 7.0 (TID 48) in 107 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:27:50.367+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 43.0 in stage 7.0 (TID 51) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.368+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 40.0 in stage 7.0 (TID 49) in 91 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:27:50.410+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_41 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_43 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.436+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 44.0 in stage 7.0 (TID 52) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.437+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 41.0 in stage 7.0 (TID 50) in 81 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:27:50.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 45.0 in stage 7.0 (TID 53) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 43.0 in stage 7.0 (TID 51) in 74 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:27:50.486+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_44 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.489+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_45 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 46.0 in stage 7.0 (TID 54) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.511+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 44.0 in stage 7.0 (TID 52) in 76 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:27:50.514+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 47.0 in stage 7.0 (TID 55) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 45.0 in stage 7.0 (TID 53) in 74 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:27:50.569+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_46 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_47 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 48.0 in stage 7.0 (TID 56) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 47.0 in stage 7.0 (TID 55) in 85 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:27:50.603+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 49.0 in stage 7.0 (TID 57) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:50.604+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 46.0 in stage 7.0 (TID 54) in 94 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:27:50.661+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_48 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.669+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added rdd_22_49 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:50.687+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 48.0 in stage 7.0 (TID 56) in 90 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:27:50.696+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 49.0 in stage 7.0 (TID 57) in 93 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:27:50.697+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2024-11-04T11:27:50.698+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: ShuffleMapStage 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 3.402 s
[2024-11-04T11:27:50.698+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:27:50.699+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: running: Set()
[2024-11-04T11:27:50.700+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:27:50.700+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: failed: Set()
[2024-11-04T11:27:50.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on eb88dbfa1959:36409 in memory (size: 123.3 KiB, free: 434.3 MiB)
[2024-11-04T11:27:50.718+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.5:35227 in memory (size: 123.3 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:50.731+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:50.732+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Got job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:27:50.733+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:50.733+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
[2024-11-04T11:27:50.734+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:50.734+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:50.739+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 452.3 KiB, free 433.3 MiB)
[2024-11-04T11:27:50.746+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 433.1 MiB)
[2024-11-04T11:27:50.748+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on eb88dbfa1959:36409 (size: 108.1 KiB, free: 434.2 MiB)
[2024-11-04T11:27:50.749+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:50.749+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:50.750+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2024-11-04T11:27:50.751+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 58) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:27:50.763+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.5:35227 (size: 108.1 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:50.776+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.5:39048
[2024-11-04T11:27:50.859+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 58) in 107 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:50.859+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2024-11-04T11:27:50.860+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: ResultStage 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.124 s
[2024-11-04T11:27:50.861+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:50.862+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2024-11-04T11:27:50.862+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO DAGScheduler: Job 6 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.129783 s
[2024-11-04T11:27:50.921+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO CodeGenerator: Code generated in 38.984004 ms
[2024-11-04T11:27:50.924+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:50 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Done
[2024-11-04T11:27:51.108+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on eb88dbfa1959:36409 in memory (size: 108.1 KiB, free: 434.3 MiB)
[2024-11-04T11:27:51.110+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.5:35227 in memory (size: 108.1 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:51.260+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO CodeGenerator: Code generated in 127.988317 ms
[2024-11-04T11:27:51.276+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:51.278+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Got job 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:27:51.278+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:51.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
[2024-11-04T11:27:51.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:51.280+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:51.286+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 592.1 KiB, free 433.1 MiB)
[2024-11-04T11:27:51.290+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 138.6 KiB, free 433.0 MiB)
[2024-11-04T11:27:51.291+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on eb88dbfa1959:36409 (size: 138.6 KiB, free: 434.2 MiB)
[2024-11-04T11:27:51.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:51.293+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 12 (MapPartitionsRDD[30] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:27:51.294+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSchedulerImpl: Adding task set 12.0 with 50 tasks resource profile 0
[2024-11-04T11:27:51.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 59) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.297+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 60) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.310+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.5:35227 (size: 138.6 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:51.465+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 61) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.466+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 59) in 170 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:27:51.467+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 62) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.468+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 60) in 172 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:27:51.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 63) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.484+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 61) in 18 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:27:51.485+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 64) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.486+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 62) in 19 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:27:51.504+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 65) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.505+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 64) in 20 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:27:51.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 66) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.507+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 63) in 25 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:27:51.523+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 8.0 in stage 12.0 (TID 67) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.524+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 66) in 18 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:27:51.527+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 9.0 in stage 12.0 (TID 68) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.528+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 65) in 23 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:27:51.541+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 10.0 in stage 12.0 (TID 69) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.542+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 8.0 in stage 12.0 (TID 67) in 20 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:27:51.543+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 11.0 in stage 12.0 (TID 70) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.545+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 9.0 in stage 12.0 (TID 68) in 18 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:27:51.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 12.0 in stage 12.0 (TID 71) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 10.0 in stage 12.0 (TID 69) in 20 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:27:51.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 13.0 in stage 12.0 (TID 72) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 11.0 in stage 12.0 (TID 70) in 20 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:27:51.578+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 14.0 in stage 12.0 (TID 73) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.580+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 15.0 in stage 12.0 (TID 74) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.581+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 12.0 in stage 12.0 (TID 71) in 20 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:27:51.581+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 13.0 in stage 12.0 (TID 72) in 18 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:27:51.594+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 16.0 in stage 12.0 (TID 75) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.595+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 14.0 in stage 12.0 (TID 73) in 16 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:27:51.599+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 17.0 in stage 12.0 (TID 76) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.600+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 15.0 in stage 12.0 (TID 74) in 20 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:27:51.610+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 18.0 in stage 12.0 (TID 77) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.611+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 16.0 in stage 12.0 (TID 75) in 18 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:27:51.618+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 19.0 in stage 12.0 (TID 78) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.619+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 17.0 in stage 12.0 (TID 76) in 21 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:27:51.630+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 20.0 in stage 12.0 (TID 79) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.631+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 18.0 in stage 12.0 (TID 77) in 20 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:27:51.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 21.0 in stage 12.0 (TID 80) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.635+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 19.0 in stage 12.0 (TID 78) in 17 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:27:51.647+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 22.0 in stage 12.0 (TID 81) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.648+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 20.0 in stage 12.0 (TID 79) in 19 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:27:51.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 23.0 in stage 12.0 (TID 82) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 21.0 in stage 12.0 (TID 80) in 17 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:27:51.670+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 24.0 in stage 12.0 (TID 83) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.671+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 22.0 in stage 12.0 (TID 81) in 23 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:27:51.674+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 25.0 in stage 12.0 (TID 84) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.675+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 23.0 in stage 12.0 (TID 82) in 24 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:27:51.701+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 26.0 in stage 12.0 (TID 85) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.702+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 24.0 in stage 12.0 (TID 83) in 33 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:27:51.704+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 27.0 in stage 12.0 (TID 86) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.705+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 25.0 in stage 12.0 (TID 84) in 31 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:27:51.720+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 28.0 in stage 12.0 (TID 87) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.721+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 27.0 in stage 12.0 (TID 86) in 18 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:27:51.722+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 29.0 in stage 12.0 (TID 88) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.723+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 26.0 in stage 12.0 (TID 85) in 22 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:27:51.736+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 30.0 in stage 12.0 (TID 89) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.737+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 28.0 in stage 12.0 (TID 87) in 17 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:27:51.740+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 31.0 in stage 12.0 (TID 90) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.741+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 29.0 in stage 12.0 (TID 88) in 19 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:27:51.752+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 32.0 in stage 12.0 (TID 91) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.753+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 30.0 in stage 12.0 (TID 89) in 18 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:27:51.756+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 33.0 in stage 12.0 (TID 92) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.757+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 31.0 in stage 12.0 (TID 90) in 17 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:27:51.767+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 34.0 in stage 12.0 (TID 93) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.768+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 32.0 in stage 12.0 (TID 91) in 17 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:27:51.770+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 35.0 in stage 12.0 (TID 94) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.771+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 33.0 in stage 12.0 (TID 92) in 15 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:27:51.783+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 36.0 in stage 12.0 (TID 95) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.784+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 34.0 in stage 12.0 (TID 93) in 17 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:27:51.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 37.0 in stage 12.0 (TID 96) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.789+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 35.0 in stage 12.0 (TID 94) in 18 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:27:51.802+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 38.0 in stage 12.0 (TID 97) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.803+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 36.0 in stage 12.0 (TID 95) in 20 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:27:51.804+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 39.0 in stage 12.0 (TID 98) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.805+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 37.0 in stage 12.0 (TID 96) in 17 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:27:51.817+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 40.0 in stage 12.0 (TID 99) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.818+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 38.0 in stage 12.0 (TID 97) in 17 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:27:51.819+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 41.0 in stage 12.0 (TID 100) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.820+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 39.0 in stage 12.0 (TID 98) in 17 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:27:51.833+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 42.0 in stage 12.0 (TID 101) (172.18.0.5, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.834+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 40.0 in stage 12.0 (TID 99) in 17 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:27:51.835+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 43.0 in stage 12.0 (TID 102) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.836+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 41.0 in stage 12.0 (TID 100) in 16 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:27:51.849+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 44.0 in stage 12.0 (TID 103) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.849+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 42.0 in stage 12.0 (TID 101) in 16 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:27:51.850+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 45.0 in stage 12.0 (TID 104) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 43.0 in stage 12.0 (TID 102) in 17 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:27:51.864+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 46.0 in stage 12.0 (TID 105) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.865+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 47.0 in stage 12.0 (TID 106) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.866+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 44.0 in stage 12.0 (TID 103) in 18 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:27:51.867+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 45.0 in stage 12.0 (TID 104) in 16 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:27:51.879+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 48.0 in stage 12.0 (TID 107) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.880+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 47.0 in stage 12.0 (TID 106) in 15 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:27:51.883+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Starting task 49.0 in stage 12.0 (TID 108) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:27:51.884+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 46.0 in stage 12.0 (TID 105) in 19 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:27:51.895+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 48.0 in stage 12.0 (TID 107) in 15 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:27:51.898+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSetManager: Finished task 49.0 in stage 12.0 (TID 108) in 16 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:27:51.899+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-11-04T11:27:51.899+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: ResultStage 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.617 s
[2024-11-04T11:27:51.900+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:51.901+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-11-04T11:27:51.901+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO DAGScheduler: Job 7 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.623400 s
[2024-11-04T11:27:51.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO CodeGenerator: Code generated in 21.088512 ms
[2024-11-04T11:27:51.993+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:51 INFO OptimisticTransaction: [tableId=86213540,txnId=37026920] Attempting to commit version 6 with 6 actions with Serializable isolation level
[2024-11-04T11:27:52.139+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on eb88dbfa1959:36409 in memory (size: 138.6 KiB, free: 434.3 MiB)
[2024-11-04T11:27:52.141+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.5:35227 in memory (size: 138.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:52.423+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DeltaLog: Creating a new snapshot v6 for commit version 6
[2024-11-04T11:27:52.424+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DeltaLog: Loading version 6.
[2024-11-04T11:27:52.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 7, totalFileSize: 25553)
[2024-11-04T11:27:52.480+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:52.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:27:52.505+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 205.0 KiB, free 433.5 MiB)
[2024-11-04T11:27:52.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.5 MiB)
[2024-11-04T11:27:52.520+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:52.522+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO SparkContext: Created broadcast 12 from toString at String.java:2951
[2024-11-04T11:27:52.523+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 14692840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:52.542+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-04T11:27:52.556+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Got job 8 (toString at String.java:2951) with 2 output partitions
[2024-11-04T11:27:52.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Final stage: ResultStage 13 (toString at String.java:2951)
[2024-11-04T11:27:52.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:52.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:52.560+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[34] at toString at String.java:2951), which has no missing parents
[2024-11-04T11:27:52.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 50.4 KiB, free 433.4 MiB)
[2024-11-04T11:27:52.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 433.4 MiB)
[2024-11-04T11:27:52.569+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on eb88dbfa1959:36409 (size: 15.5 KiB, free: 434.2 MiB)
[2024-11-04T11:27:52.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:52.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[34] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:52.571+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSchedulerImpl: Adding task set 13.0 with 2 tasks resource profile 0
[2024-11-04T11:27:52.572+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 109) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8368 bytes)
[2024-11-04T11:27:52.573+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 110) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8234 bytes)
[2024-11-04T11:27:52.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.18.0.5:35227 (size: 15.5 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:52.601+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:52.655+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 110) in 81 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:27:52.668+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 109) in 96 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:27:52.668+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-11-04T11:27:52.669+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: ResultStage 13 (toString at String.java:2951) finished in 0.107 s
[2024-11-04T11:27:52.670+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:52.670+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-11-04T11:27:52.671+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DAGScheduler: Job 8 finished: toString at String.java:2951, took 0.127281 s
[2024-11-04T11:27:52.679+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] Created snapshot Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=6, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,6,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null),None,1730719672364), checksumOpt=None)
[2024-11-04T11:27:52.680+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=6, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,6,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null),None,1730719672364), checksumOpt=None)
[2024-11-04T11:27:52.682+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MapPartitionsRDD: Removing RDD 22 from persistence list
[2024-11-04T11:27:52.688+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManager: Removing RDD 22
[2024-11-04T11:27:52.697+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Compute snapshot for version: 6
[2024-11-04T11:27:52.701+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 204.7 KiB, free 433.2 MiB)
[2024-11-04T11:27:52.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on eb88dbfa1959:36409 in memory (size: 15.5 KiB, free: 434.3 MiB)
[2024-11-04T11:27:52.715+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.18.0.5:35227 in memory (size: 15.5 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:52.717+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.2 MiB)
[2024-11-04T11:27:52.718+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:52.719+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO SparkContext: Created broadcast 14 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:52.724+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:52.725+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:52.980+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:52.981+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:52 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:27:53.038+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 205.0 KiB, free 433.3 MiB)
[2024-11-04T11:27:53.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.2 MiB)
[2024-11-04T11:27:53.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:53.053+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO SparkContext: Created broadcast 15 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:53.054+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 14692840 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:53.069+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Registering RDD 38 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 3
[2024-11-04T11:27:53.069+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Got map stage job 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-04T11:27:53.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Final stage: ShuffleMapStage 14 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:53.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:53.072+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:53.073+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[38] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:53.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 138.3 KiB, free 433.1 MiB)
[2024-11-04T11:27:53.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.0 MiB)
[2024-11-04T11:27:53.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:53.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:53.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[38] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:53.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2024-11-04T11:27:53.098+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 111) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8357 bytes)
[2024-11-04T11:27:53.098+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 112) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8223 bytes)
[2024-11-04T11:27:53.110+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:53.133+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:53.200+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 112) in 102 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:27:53.214+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 111) in 117 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:27:53.215+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-11-04T11:27:53.216+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: ShuffleMapStage 14 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.142 s
[2024-11-04T11:27:53.217+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:27:53.217+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: running: Set()
[2024-11-04T11:27:53.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:27:53.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: failed: Set()
[2024-11-04T11:27:53.270+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Removed broadcast_16_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:53.272+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:53.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Registering RDD 48 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 4
[2024-11-04T11:27:53.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Got map stage job 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:27:53.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Final stage: ShuffleMapStage 16 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:53.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[2024-11-04T11:27:53.445+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:53.446+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[48] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:53.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 517.1 KiB, free 432.7 MiB)
[2024-11-04T11:27:53.512+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 432.6 MiB)
[2024-11-04T11:27:53.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on eb88dbfa1959:36409 (size: 123.3 KiB, free: 434.1 MiB)
[2024-11-04T11:27:53.514+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:53.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[48] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:27:53.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSchedulerImpl: Adding task set 16.0 with 50 tasks resource profile 0
[2024-11-04T11:27:53.517+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 113) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 114) (172.18.0.5, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.529+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.5:35227 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:53.542+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.5:39048
[2024-11-04T11:27:53.584+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_3 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.585+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_1 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.609+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 5.0 in stage 16.0 (TID 115) (172.18.0.5, executor 0, partition 5, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.609+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 113) in 93 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:27:53.612+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 11.0 in stage 16.0 (TID 116) (172.18.0.5, executor 0, partition 11, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.613+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 114) in 96 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:27:53.645+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_11 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.646+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_5 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.663+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 15.0 in stage 16.0 (TID 117) (172.18.0.5, executor 0, partition 15, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.664+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 5.0 in stage 16.0 (TID 115) in 55 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:27:53.664+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 18.0 in stage 16.0 (TID 118) (172.18.0.5, executor 0, partition 18, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.665+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 11.0 in stage 16.0 (TID 116) in 54 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:27:53.699+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_15 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.700+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_18 in memory on 172.18.0.5:35227 (size: 361.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.719+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 22.0 in stage 16.0 (TID 119) (172.18.0.5, executor 0, partition 22, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.720+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 15.0 in stage 16.0 (TID 117) in 56 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:27:53.720+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 24.0 in stage 16.0 (TID 120) (172.18.0.5, executor 0, partition 24, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.721+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 18.0 in stage 16.0 (TID 118) in 57 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:27:53.753+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_22 in memory on 172.18.0.5:35227 (size: 371.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.754+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_24 in memory on 172.18.0.5:35227 (size: 482.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.778+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 29.0 in stage 16.0 (TID 121) (172.18.0.5, executor 0, partition 29, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.779+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 24.0 in stage 16.0 (TID 120) in 58 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:27:53.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 32.0 in stage 16.0 (TID 122) (172.18.0.5, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 22.0 in stage 16.0 (TID 119) in 61 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:27:53.830+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_32 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.833+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_29 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.850+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 34.0 in stage 16.0 (TID 123) (172.18.0.5, executor 0, partition 34, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 32.0 in stage 16.0 (TID 122) in 72 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:27:53.856+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 42.0 in stage 16.0 (TID 124) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.857+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 29.0 in stage 16.0 (TID 121) in 79 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:27:53.899+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_34 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.901+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_42 in memory on 172.18.0.5:35227 (size: 479.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.926+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 43.0 in stage 16.0 (TID 125) (172.18.0.5, executor 0, partition 43, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.927+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 34.0 in stage 16.0 (TID 123) in 77 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:27:53.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 126) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.931+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 42.0 in stage 16.0 (TID 124) in 75 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:27:53.966+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_43 in memory on 172.18.0.5:35227 (size: 492.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.968+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO BlockManagerInfo: Added rdd_45_0 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:53.988+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 127) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.989+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 43.0 in stage 16.0 (TID 125) in 63 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:27:53.990+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Starting task 4.0 in stage 16.0 (TID 128) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:53.991+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:53 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 126) in 61 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:27:54.033+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_2 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.033+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_4 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.055+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 6.0 in stage 16.0 (TID 129) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.056+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 127) in 68 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:27:54.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 7.0 in stage 16.0 (TID 130) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.059+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 4.0 in stage 16.0 (TID 128) in 70 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:27:54.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_7 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_6 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.115+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 8.0 in stage 16.0 (TID 131) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.116+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 7.0 in stage 16.0 (TID 130) in 58 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:27:54.117+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 9.0 in stage 16.0 (TID 132) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.118+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 6.0 in stage 16.0 (TID 129) in 62 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:27:54.155+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_8 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.156+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_9 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.175+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 10.0 in stage 16.0 (TID 133) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.177+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 12.0 in stage 16.0 (TID 134) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.178+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 8.0 in stage 16.0 (TID 131) in 62 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:27:54.179+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 9.0 in stage 16.0 (TID 132) in 60 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:27:54.217+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_10 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.220+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_12 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.248+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 13.0 in stage 16.0 (TID 135) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.249+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 12.0 in stage 16.0 (TID 134) in 73 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:27:54.250+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 14.0 in stage 16.0 (TID 136) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.251+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 10.0 in stage 16.0 (TID 133) in 76 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:27:54.338+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_13 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.343+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_14 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.408+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 16.0 in stage 16.0 (TID 137) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.410+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 13.0 in stage 16.0 (TID 135) in 162 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:27:54.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 17.0 in stage 16.0 (TID 138) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.412+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 14.0 in stage 16.0 (TID 136) in 162 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:27:54.458+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_17 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_16 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.478+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 19.0 in stage 16.0 (TID 139) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.478+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 16.0 in stage 16.0 (TID 137) in 77 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:27:54.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 20.0 in stage 16.0 (TID 140) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 17.0 in stage 16.0 (TID 138) in 70 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:27:54.514+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_19 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_20 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.535+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 21.0 in stage 16.0 (TID 141) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.536+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 20.0 in stage 16.0 (TID 140) in 55 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:27:54.537+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 23.0 in stage 16.0 (TID 142) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.537+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 19.0 in stage 16.0 (TID 139) in 60 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:27:54.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_21 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.571+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_23 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.591+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 25.0 in stage 16.0 (TID 143) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.592+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 23.0 in stage 16.0 (TID 142) in 55 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:27:54.593+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 26.0 in stage 16.0 (TID 144) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.594+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 21.0 in stage 16.0 (TID 141) in 58 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:27:54.629+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_25 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_26 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.649+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 27.0 in stage 16.0 (TID 145) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.650+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 25.0 in stage 16.0 (TID 143) in 58 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:27:54.653+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 28.0 in stage 16.0 (TID 146) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.654+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 26.0 in stage 16.0 (TID 144) in 61 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:27:54.687+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_27 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.690+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_28 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.710+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 30.0 in stage 16.0 (TID 147) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.710+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 27.0 in stage 16.0 (TID 145) in 62 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:27:54.713+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 31.0 in stage 16.0 (TID 148) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 28.0 in stage 16.0 (TID 146) in 61 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:27:54.749+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_31 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.751+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_30 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.779+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 33.0 in stage 16.0 (TID 149) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 31.0 in stage 16.0 (TID 148) in 67 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:27:54.781+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 35.0 in stage 16.0 (TID 150) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.782+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 30.0 in stage 16.0 (TID 147) in 72 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:27:54.826+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_33 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.831+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_35 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.850+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 36.0 in stage 16.0 (TID 151) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 33.0 in stage 16.0 (TID 149) in 71 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:27:54.854+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 37.0 in stage 16.0 (TID 152) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.855+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 35.0 in stage 16.0 (TID 150) in 73 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:27:54.882+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_36 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.884+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_37 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.900+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 38.0 in stage 16.0 (TID 153) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.901+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 36.0 in stage 16.0 (TID 151) in 50 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:27:54.906+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 39.0 in stage 16.0 (TID 154) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.907+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 37.0 in stage 16.0 (TID 152) in 54 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:27:54.935+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_38 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.940+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_39 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.954+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 40.0 in stage 16.0 (TID 155) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.955+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 38.0 in stage 16.0 (TID 153) in 56 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:27:54.959+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Starting task 41.0 in stage 16.0 (TID 156) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:54.960+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO TaskSetManager: Finished task 39.0 in stage 16.0 (TID 154) in 53 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:27:54.992+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_41 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:54.993+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:54 INFO BlockManagerInfo: Added rdd_45_40 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.011+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 44.0 in stage 16.0 (TID 157) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.013+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 45.0 in stage 16.0 (TID 158) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.014+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 40.0 in stage 16.0 (TID 155) in 59 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:27:55.015+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 41.0 in stage 16.0 (TID 156) in 55 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:27:55.044+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_44 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_45 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.061+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 46.0 in stage 16.0 (TID 159) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.062+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 44.0 in stage 16.0 (TID 157) in 50 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:27:55.066+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 47.0 in stage 16.0 (TID 160) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.068+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 45.0 in stage 16.0 (TID 158) in 54 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:27:55.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_46 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.103+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_47 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.113+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 48.0 in stage 16.0 (TID 161) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.114+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 46.0 in stage 16.0 (TID 159) in 53 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:27:55.121+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 49.0 in stage 16.0 (TID 162) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:27:55.122+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 47.0 in stage 16.0 (TID 160) in 55 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:27:55.157+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_48 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.163+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added rdd_45_49 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:27:55.178+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 48.0 in stage 16.0 (TID 161) in 65 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:27:55.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 49.0 in stage 16.0 (TID 162) in 62 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:27:55.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2024-11-04T11:27:55.185+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: ShuffleMapStage 16 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.736 s
[2024-11-04T11:27:55.186+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:27:55.186+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: running: Set()
[2024-11-04T11:27:55.187+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:27:55.187+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: failed: Set()
[2024-11-04T11:27:55.214+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:27:55.215+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Got job 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:27:55.216+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Final stage: ResultStage 19 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:27:55.217+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
[2024-11-04T11:27:55.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:55.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[51] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:27:55.221+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 452.3 KiB, free 432.2 MiB)
[2024-11-04T11:27:55.224+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 432.0 MiB)
[2024-11-04T11:27:55.225+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on eb88dbfa1959:36409 (size: 108.1 KiB, free: 434.0 MiB)
[2024-11-04T11:27:55.226+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:55.226+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[51] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:55.227+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2024-11-04T11:27:55.228+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 163) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:27:55.238+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.18.0.5:35227 (size: 108.1 KiB, free: 1048.5 MiB)
[2024-11-04T11:27:55.249+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.5:39048
[2024-11-04T11:27:55.281+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 163) in 54 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:55.282+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2024-11-04T11:27:55.282+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: ResultStage 19 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.064 s
[2024-11-04T11:27:55.283+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:55.284+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2024-11-04T11:27:55.284+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Job 11 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.068237 s
[2024-11-04T11:27:55.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Done
[2024-11-04T11:27:55.351+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO OptimisticTransaction: [tableId=86213540,txnId=37026920] Committed delta #6 to s3a://lakehouse/merge_data-movies/merged_data/_delta_log
[2024-11-04T11:27:55.415+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO InMemoryFileIndex: It took 15 ms to list leaf files for 1 paths.
[2024-11-04T11:27:55.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:55.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Got job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:27:55.443+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:55.444+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:55.445+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:55.446+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[53] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:55.451+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 107.1 KiB, free 431.9 MiB)
[2024-11-04T11:27:55.454+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 431.9 MiB)
[2024-11-04T11:27:55.457+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 434.0 MiB)
[2024-11-04T11:27:55.458+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:55.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[53] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:55.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-11-04T11:27:55.461+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 164) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7499 bytes)
[2024-11-04T11:27:55.471+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1048.4 MiB)
[2024-11-04T11:27:55.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 164) in 50 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:55.511+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-11-04T11:27:55.512+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.066 s
[2024-11-04T11:27:55.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:55.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-11-04T11:27:55.514+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO DAGScheduler: Job 12 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.070581 s
[2024-11-04T11:27:55.597+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_18_piece0 on eb88dbfa1959:36409 in memory (size: 108.1 KiB, free: 434.1 MiB)
[2024-11-04T11:27:55.600+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.18.0.5:35227 in memory (size: 108.1 KiB, free: 1048.5 MiB)
[2024-11-04T11:27:55.608+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 434.1 MiB)
[2024-11-04T11:27:55.609+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:55.640+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManager: Removing RDD 22
[2024-11-04T11:27:55.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on eb88dbfa1959:36409 in memory (size: 36.6 KiB, free: 434.1 MiB)
[2024-11-04T11:27:55.653+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.5:35227 in memory (size: 36.6 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:55.666+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_6_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:55.675+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:27:55.678+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.6 MiB)
[2024-11-04T11:27:55.686+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on eb88dbfa1959:36409 in memory (size: 123.3 KiB, free: 434.3 MiB)
[2024-11-04T11:27:55.687+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:55 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.18.0.5:35227 in memory (size: 123.3 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:56.268+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.
[2024-11-04T11:27:56.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:56.293+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Got job 13 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:27:56.294+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Final stage: ResultStage 21 (load at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:56.295+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:56.295+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:56.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[55] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:56.301+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 107.1 KiB, free 433.8 MiB)
[2024-11-04T11:27:56.304+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.8 MiB)
[2024-11-04T11:27:56.306+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:56.307+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:56.307+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[55] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:56.308+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2024-11-04T11:27:56.309+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 165) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7500 bytes)
[2024-11-04T11:27:56.321+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:56.349+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 165) in 41 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:56.350+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2024-11-04T11:27:56.351+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: ResultStage 21 (load at NativeMethodAccessorImpl.java:0) finished in 0.055 s
[2024-11-04T11:27:56.351+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:56.352+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2024-11-04T11:27:56.353+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Job 13 finished: load at NativeMethodAccessorImpl.java:0, took 0.058598 s
[2024-11-04T11:27:56.453+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:56.453+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:27:56.491+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1048.8 MiB)
[2024-11-04T11:27:56.492+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Removed broadcast_20_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-04T11:27:56.501+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO CodeGenerator: Code generated in 21.23839 ms
[2024-11-04T11:27:56.511+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO CodeGenerator: Code generated in 8.276884 ms
[2024-11-04T11:27:56.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 206.5 KiB, free 433.7 MiB)
[2024-11-04T11:27:56.528+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.7 MiB)
[2024-11-04T11:27:56.529+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on eb88dbfa1959:36409 (size: 36.6 KiB, free: 434.3 MiB)
[2024-11-04T11:27:56.530+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO SparkContext: Created broadcast 21 from showString at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:56.532+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:56.549+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:56.551+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Got job 14 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:27:56.552+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Final stage: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:56.553+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:56.554+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:56.555+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[61] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:56.572+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 28.8 KiB, free 433.7 MiB)
[2024-11-04T11:27:56.589+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 433.7 MiB)
[2024-11-04T11:27:56.590+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on eb88dbfa1959:36409 (size: 12.6 KiB, free: 434.3 MiB)
[2024-11-04T11:27:56.591+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:56.592+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[61] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:27:56.593+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2024-11-04T11:27:56.594+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 166) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7922 bytes)
[2024-11-04T11:27:56.607+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.18.0.5:35227 (size: 12.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:56.671+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.18.0.5:35227 (size: 36.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:56.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 166) in 337 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:27:56.931+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2024-11-04T11:27:56.932+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0) finished in 0.377 s
[2024-11-04T11:27:56.932+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:27:56.933+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2024-11-04T11:27:56.933+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO DAGScheduler: Job 14 finished: showString at NativeMethodAccessorImpl.java:0, took 0.382011 s
[2024-11-04T11:27:56.944+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:56 INFO CodeGenerator: Code generated in 8.094554 ms
[2024-11-04T11:27:57.006+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-04T11:27:57.006+0000] {spark_submit.py:579} INFO - |id   |cast_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |director                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2024-11-04T11:27:57.007+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-04T11:27:57.008+0000] {spark_submit.py:579} INFO - |862  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.008+0000] {spark_submit.py:579} INFO - |8844 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.009+0000] {spark_submit.py:579} INFO - |15602|[Walter Matthau, Jack Lemmon, Ann-Margret, Sophia Loren, Daryl Hannah, Burgess Meredith, Kevin Pollak]                                                                                                                                                                                                                                                                                                                                                                                                            |[Walter Matthau, Jack Lemmon, Ann-Margret, Sophia Loren, Daryl Hannah, Burgess Meredith, Kevin Pollak]                                                                                                                                                                                                                                                                                                                                                                                                            |
[2024-11-04T11:27:57.010+0000] {spark_submit.py:579} INFO - |31357|[Whitney Houston, Angela Bassett, Loretta Devine, Lela Rochon, Gregory Hines, Dennis Haysbert, Michael Beach, Mykelti Williamson, Lamont Johnson, Wesley Snipes]                                                                                                                                                                                                                                                                                                                                                  |[Whitney Houston, Angela Bassett, Loretta Devine, Lela Rochon, Gregory Hines, Dennis Haysbert, Michael Beach, Mykelti Williamson, Lamont Johnson, Wesley Snipes]                                                                                                                                                                                                                                                                                                                                                  |
[2024-11-04T11:27:57.010+0000] {spark_submit.py:579} INFO - |11862|[Steve Martin, Diane Keaton, Martin Short, Kimberly Williams-Paisley, George Newbern, Kieran Culkin, BD Wong, Peter Michael Goetz, Kate McGregor-Stewart, Jane Adams, Eugene Levy, Lori Alan]                                                                                                                                                                                                                                                                                                                     |[Steve Martin, Diane Keaton, Martin Short, Kimberly Williams-Paisley, George Newbern, Kieran Culkin, BD Wong, Peter Michael Goetz, Kate McGregor-Stewart, Jane Adams, Eugene Levy, Lori Alan]                                                                                                                                                                                                                                                                                                                     |
[2024-11-04T11:27:57.011+0000] {spark_submit.py:579} INFO - |949  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.011+0000] {spark_submit.py:579} INFO - |11860|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.012+0000] {spark_submit.py:579} INFO - |45325|[Jonathan Taylor Thomas, Brad Renfro, Rachael Leigh Cook, Michael McShane, Amy Wright, Eric Schweig, Tamara Mello]                                                                                                                                                                                                                                                                                                                                                                                                |[Jonathan Taylor Thomas, Brad Renfro, Rachael Leigh Cook, Michael McShane, Amy Wright, Eric Schweig, Tamara Mello]                                                                                                                                                                                                                                                                                                                                                                                                |
[2024-11-04T11:27:57.013+0000] {spark_submit.py:579} INFO - |9091 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.013+0000] {spark_submit.py:579} INFO - |710  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.014+0000] {spark_submit.py:579} INFO - |9087 |[Michael Douglas, Annette Bening, Michael J. Fox, Martin Sheen, Anna Deavere Smith, Shawna Waldron, Samantha Mathis, David Paymer, Richard Dreyfuss, Nina Siemaszko, Wendie Malick, Beau Billingslea, Gail Strickland, Joshua Malina, Clement von Franckenstein, John Mahoney, John Mahon, Gabriel Jarret]                                                                                                                                                                                                        |[Michael Douglas, Annette Bening, Michael J. Fox, Martin Sheen, Anna Deavere Smith, Shawna Waldron, Samantha Mathis, David Paymer, Richard Dreyfuss, Nina Siemaszko, Wendie Malick, Beau Billingslea, Gail Strickland, Joshua Malina, Clement von Franckenstein, John Mahoney, John Mahon, Gabriel Jarret]                                                                                                                                                                                                        |
[2024-11-04T11:27:57.015+0000] {spark_submit.py:579} INFO - |12110|[Leslie Nielsen, Mel Brooks, Amy Yasbeck, Peter MacNicol, Lysette Anthony, Harvey Korman, Steven Weber, Mark Blankfield, Megan Cavanagh, Gregg Binkley, Anne Bancroft]                                                                                                                                                                                                                                                                                                                                            |[Leslie Nielsen, Mel Brooks, Amy Yasbeck, Peter MacNicol, Lysette Anthony, Harvey Korman, Steven Weber, Mark Blankfield, Megan Cavanagh, Gregg Binkley, Anne Bancroft]                                                                                                                                                                                                                                                                                                                                            |
[2024-11-04T11:27:57.015+0000] {spark_submit.py:579} INFO - |21032|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.016+0000] {spark_submit.py:579} INFO - |10858|[Anthony Hopkins, Joan Allen, Powers Boothe, Ed Harris, Bob Hoskins, E.G. Marshall, David Paymer, David Hyde Pierce, Paul Sorvino, Mary Steenburgen, J.T. Walsh, James Woods, Brian Bedford, Kevin Dunn, Fyvush Finkel, Annabeth Gish, Larry Hagman, Madeline Kahn, Dan Hedaya, Bridgette Wilson, Tom Bower, Tony Goldwyn, Edward Herrmann, Tony Lo Bianco, Saul Rubinek, Robert Beltran, John Cunningham, John Diehl, John C. McGinley, Michael Chiklis, Ric Young, Boris Sichkin, Sam Waterston, Marley Shelton]|[Anthony Hopkins, Joan Allen, Powers Boothe, Ed Harris, Bob Hoskins, E.G. Marshall, David Paymer, David Hyde Pierce, Paul Sorvino, Mary Steenburgen, J.T. Walsh, James Woods, Brian Bedford, Kevin Dunn, Fyvush Finkel, Annabeth Gish, Larry Hagman, Madeline Kahn, Dan Hedaya, Bridgette Wilson, Tom Bower, Tony Goldwyn, Edward Herrmann, Tony Lo Bianco, Saul Rubinek, Robert Beltran, John Cunningham, John Diehl, John C. McGinley, Michael Chiklis, Ric Young, Boris Sichkin, Sam Waterston, Marley Shelton]|
[2024-11-04T11:27:57.017+0000] {spark_submit.py:579} INFO - |1408 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.017+0000] {spark_submit.py:579} INFO - |524  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.018+0000] {spark_submit.py:579} INFO - |4584 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.019+0000] {spark_submit.py:579} INFO - |5    |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.019+0000] {spark_submit.py:579} INFO - |9273 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.020+0000] {spark_submit.py:579} INFO - |11517|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-04T11:27:57.021+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-04T11:27:57.022+0000] {spark_submit.py:579} INFO - only showing top 20 rows
[2024-11-04T11:27:57.022+0000] {spark_submit.py:579} INFO - 
[2024-11-04T11:27:57.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO OptimisticTransaction: [tableId=86213540,txnId=386d9b55] Updated metadata from - to Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780))
[2024-11-04T11:27:57.130+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:27:57.131+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#1566, Some(Etc/UTC)), lambdafunction(lambda x#1578.name, lambda x#1578, false)))
[2024-11-04T11:27:57.147+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-04T11:27:57.161+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:27:57.192+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO CodeGenerator: Code generated in 24.508236 ms
[2024-11-04T11:27:57.205+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO CodeGenerator: Code generated in 9.364606 ms
[2024-11-04T11:27:57.210+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 206.7 KiB, free 433.4 MiB)
[2024-11-04T11:27:57.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 433.4 MiB)
[2024-11-04T11:27:57.219+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on eb88dbfa1959:36409 (size: 36.7 KiB, free: 434.2 MiB)
[2024-11-04T11:27:57.220+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO SparkContext: Created broadcast 23 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:27:57.221+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:27:57.241+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Registering RDD 69 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 5
[2024-11-04T11:27:57.241+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Got map stage job 15 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:27:57.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:27:57.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:27:57.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:27:57.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[69] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:27:57.248+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 49.4 KiB, free 433.4 MiB)
[2024-11-04T11:27:57.251+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 433.3 MiB)
[2024-11-04T11:27:57.252+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on eb88dbfa1959:36409 (size: 19.6 KiB, free: 434.2 MiB)
[2024-11-04T11:27:57.253+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:27:57.254+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[69] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:27:57.255+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO TaskSchedulerImpl: Adding task set 23.0 with 2 tasks resource profile 0
[2024-11-04T11:27:57.256+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 167) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:27:57.257+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 168) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:27:57.269+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.18.0.5:35227 (size: 19.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:57.377+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.18.0.5:35227 (size: 36.7 KiB, free: 1048.7 MiB)
[2024-11-04T11:27:57.428+0000] {spark_submit.py:579} INFO - 24/11/04 11:27:57 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 168) in 171 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:00.323+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 167) in 3068 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:00.324+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2024-11-04T11:28:00.325+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: ShuffleMapStage 23 (save at NativeMethodAccessorImpl.java:0) finished in 3.079 s
[2024-11-04T11:28:00.325+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:00.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:00.327+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:00.327+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:00.328+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO ShufflePartitionsUtil: For shuffle(5), advisory target size: 67108864, actual target size 5348362, minimum partition size: 1048576
[2024-11-04T11:28:00.334+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:00.357+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO CodeGenerator: Code generated in 19.037911 ms
[2024-11-04T11:28:00.386+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:00.387+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Got job 16 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:00.388+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Final stage: ResultStage 25 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:00.389+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)
[2024-11-04T11:28:00.390+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:00.390+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[71] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:00.406+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_22_piece0 on eb88dbfa1959:36409 in memory (size: 12.6 KiB, free: 434.2 MiB)
[2024-11-04T11:28:00.407+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.18.0.5:35227 in memory (size: 12.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:00.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_24_piece0 on eb88dbfa1959:36409 in memory (size: 19.6 KiB, free: 434.3 MiB)
[2024-11-04T11:28:00.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.18.0.5:35227 in memory (size: 19.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:00.419+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 373.1 KiB, free 433.1 MiB)
[2024-11-04T11:28:00.422+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 135.1 KiB, free 433.0 MiB)
[2024-11-04T11:28:00.423+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on eb88dbfa1959:36409 (size: 135.1 KiB, free: 434.1 MiB)
[2024-11-04T11:28:00.424+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:00.424+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 25 (MapPartitionsRDD[71] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:00.425+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks resource profile 0
[2024-11-04T11:28:00.426+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 169) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:00.427+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 170) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:00.435+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.18.0.5:35227 (size: 135.1 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:00.453+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.18.0.5:39048
[2024-11-04T11:28:00.924+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 170) in 497 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:00.938+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 169) in 512 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:00.939+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2024-11-04T11:28:00.939+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: ResultStage 25 (save at NativeMethodAccessorImpl.java:0) finished in 0.545 s
[2024-11-04T11:28:00.940+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:00.940+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2024-11-04T11:28:00.941+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO DAGScheduler: Job 16 finished: save at NativeMethodAccessorImpl.java:0, took 0.553043 s
[2024-11-04T11:28:00.942+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO FileFormatWriter: Start to commit write Job f91387ca-940e-4a28-8a0f-58bc3fa69b29.
[2024-11-04T11:28:00.942+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO FileFormatWriter: Write Job f91387ca-940e-4a28-8a0f-58bc3fa69b29 committed. Elapsed time: 0 ms.
[2024-11-04T11:28:00.943+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO FileFormatWriter: Finished processing stats for write job f91387ca-940e-4a28-8a0f-58bc3fa69b29.
[2024-11-04T11:28:00.951+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_25_piece0 on eb88dbfa1959:36409 in memory (size: 135.1 KiB, free: 434.3 MiB)
[2024-11-04T11:28:00.953+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:00 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.18.0.5:35227 in memory (size: 135.1 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:01.073+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:01.075+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Got job 17 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:28:01.076+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Final stage: ResultStage 27 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:01.076+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
[2024-11-04T11:28:01.079+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:01.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[73] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:01.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 592.2 KiB, free 432.9 MiB)
[2024-11-04T11:28:01.092+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 138.5 KiB, free 432.7 MiB)
[2024-11-04T11:28:01.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on eb88dbfa1959:36409 (size: 138.5 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:01.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 27 (MapPartitionsRDD[73] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:28:01.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Adding task set 27.0 with 50 tasks resource profile 0
[2024-11-04T11:28:01.097+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 171) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.098+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 172) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.109+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.18.0.5:35227 (size: 138.5 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:01.137+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 2.0 in stage 27.0 (TID 173) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.138+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_21_piece0 on eb88dbfa1959:36409 in memory (size: 36.6 KiB, free: 434.2 MiB)
[2024-11-04T11:28:01.139+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 172) in 41 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:28:01.140+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 3.0 in stage 27.0 (TID 174) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.141+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 171) in 43 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:28:01.141+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.18.0.5:35227 in memory (size: 36.6 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:01.153+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 4.0 in stage 27.0 (TID 175) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.153+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 3.0 in stage 27.0 (TID 174) in 15 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:28:01.155+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 5.0 in stage 27.0 (TID 176) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.155+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 2.0 in stage 27.0 (TID 173) in 18 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:28:01.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 6.0 in stage 27.0 (TID 177) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 5.0 in stage 27.0 (TID 176) in 15 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:28:01.170+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 7.0 in stage 27.0 (TID 178) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.170+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 4.0 in stage 27.0 (TID 175) in 18 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:28:01.181+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 8.0 in stage 27.0 (TID 179) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.182+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 6.0 in stage 27.0 (TID 177) in 14 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:28:01.183+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 9.0 in stage 27.0 (TID 180) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 7.0 in stage 27.0 (TID 178) in 15 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:28:01.199+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 10.0 in stage 27.0 (TID 181) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.199+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 8.0 in stage 27.0 (TID 179) in 18 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:28:01.200+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 11.0 in stage 27.0 (TID 182) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.201+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 9.0 in stage 27.0 (TID 180) in 18 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:28:01.213+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 12.0 in stage 27.0 (TID 183) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.214+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 10.0 in stage 27.0 (TID 181) in 15 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:28:01.215+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 13.0 in stage 27.0 (TID 184) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.215+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 11.0 in stage 27.0 (TID 182) in 15 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:28:01.227+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 14.0 in stage 27.0 (TID 185) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.228+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 15.0 in stage 27.0 (TID 186) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.229+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 12.0 in stage 27.0 (TID 183) in 17 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:28:01.230+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 13.0 in stage 27.0 (TID 184) in 15 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:28:01.242+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 16.0 in stage 27.0 (TID 187) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 17.0 in stage 27.0 (TID 188) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 14.0 in stage 27.0 (TID 185) in 16 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:28:01.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 15.0 in stage 27.0 (TID 186) in 15 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:28:01.255+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 18.0 in stage 27.0 (TID 189) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.256+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 17.0 in stage 27.0 (TID 188) in 14 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:28:01.257+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 19.0 in stage 27.0 (TID 190) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.258+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 16.0 in stage 27.0 (TID 187) in 16 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:28:01.277+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 20.0 in stage 27.0 (TID 191) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.278+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 21.0 in stage 27.0 (TID 192) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 18.0 in stage 27.0 (TID 189) in 24 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:28:01.280+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 19.0 in stage 27.0 (TID 190) in 23 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:28:01.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 22.0 in stage 27.0 (TID 193) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 20.0 in stage 27.0 (TID 191) in 15 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:28:01.293+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 23.0 in stage 27.0 (TID 194) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.294+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 21.0 in stage 27.0 (TID 192) in 16 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:28:01.310+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 24.0 in stage 27.0 (TID 195) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.311+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 25.0 in stage 27.0 (TID 196) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 22.0 in stage 27.0 (TID 193) in 20 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:28:01.313+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 23.0 in stage 27.0 (TID 194) in 19 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:28:01.324+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 26.0 in stage 27.0 (TID 197) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.324+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 24.0 in stage 27.0 (TID 195) in 15 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:28:01.325+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 27.0 in stage 27.0 (TID 198) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 25.0 in stage 27.0 (TID 196) in 14 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:28:01.335+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 28.0 in stage 27.0 (TID 199) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.336+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 26.0 in stage 27.0 (TID 197) in 12 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:28:01.337+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 29.0 in stage 27.0 (TID 200) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.337+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 27.0 in stage 27.0 (TID 198) in 13 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:28:01.348+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 30.0 in stage 27.0 (TID 201) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.349+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 28.0 in stage 27.0 (TID 199) in 13 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:28:01.349+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 31.0 in stage 27.0 (TID 202) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.350+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 29.0 in stage 27.0 (TID 200) in 14 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:28:01.362+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 32.0 in stage 27.0 (TID 203) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.363+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 33.0 in stage 27.0 (TID 204) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.364+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 30.0 in stage 27.0 (TID 201) in 15 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:28:01.364+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 31.0 in stage 27.0 (TID 202) in 14 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:28:01.375+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 34.0 in stage 27.0 (TID 205) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.376+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 32.0 in stage 27.0 (TID 203) in 15 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:28:01.379+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 35.0 in stage 27.0 (TID 206) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.380+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 33.0 in stage 27.0 (TID 204) in 18 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:28:01.389+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 36.0 in stage 27.0 (TID 207) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.390+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 34.0 in stage 27.0 (TID 205) in 15 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:28:01.396+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 37.0 in stage 27.0 (TID 208) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.396+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 35.0 in stage 27.0 (TID 206) in 17 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:28:01.403+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 38.0 in stage 27.0 (TID 209) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.404+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 36.0 in stage 27.0 (TID 207) in 15 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:28:01.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 39.0 in stage 27.0 (TID 210) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 37.0 in stage 27.0 (TID 208) in 14 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:28:01.416+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 40.0 in stage 27.0 (TID 211) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.417+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 38.0 in stage 27.0 (TID 209) in 13 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:28:01.421+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 41.0 in stage 27.0 (TID 212) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.422+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 39.0 in stage 27.0 (TID 210) in 14 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:28:01.429+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 42.0 in stage 27.0 (TID 213) (172.18.0.5, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 40.0 in stage 27.0 (TID 211) in 13 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:28:01.435+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 43.0 in stage 27.0 (TID 214) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.435+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 41.0 in stage 27.0 (TID 212) in 14 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:28:01.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 44.0 in stage 27.0 (TID 215) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.451+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 42.0 in stage 27.0 (TID 213) in 14 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:28:01.452+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 45.0 in stage 27.0 (TID 216) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.452+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 43.0 in stage 27.0 (TID 214) in 18 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:28:01.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 46.0 in stage 27.0 (TID 217) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.460+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 44.0 in stage 27.0 (TID 215) in 17 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:28:01.468+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 47.0 in stage 27.0 (TID 218) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.469+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 45.0 in stage 27.0 (TID 216) in 17 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:28:01.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 48.0 in stage 27.0 (TID 219) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 46.0 in stage 27.0 (TID 217) in 16 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:28:01.484+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 49.0 in stage 27.0 (TID 220) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:01.485+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 47.0 in stage 27.0 (TID 218) in 18 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:28:01.488+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 48.0 in stage 27.0 (TID 219) in 15 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:28:01.496+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 49.0 in stage 27.0 (TID 220) in 11 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:28:01.496+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2024-11-04T11:28:01.497+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: ResultStage 27 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.415 s
[2024-11-04T11:28:01.498+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:01.498+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2024-11-04T11:28:01.499+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Job 17 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.423173 s
[2024-11-04T11:28:01.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO OptimisticTransaction: [tableId=86213540,txnId=386d9b55] Attempting to commit version 7 with 6 actions with Serializable isolation level
[2024-11-04T11:28:01.566+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DeltaLog: Creating a new snapshot v7 for commit version 7
[2024-11-04T11:28:01.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DeltaLog: Loading version 7.
[2024-11-04T11:28:01.571+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 8, totalFileSize: 28030)
[2024-11-04T11:28:01.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:01.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:01.611+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 205.0 KiB, free 432.8 MiB)
[2024-11-04T11:28:01.617+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 432.7 MiB)
[2024-11-04T11:28:01.618+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.619+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Created broadcast 27 from toString at String.java:2951
[2024-11-04T11:28:01.620+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 16791231 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:01.631+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-04T11:28:01.632+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Got job 18 (toString at String.java:2951) with 2 output partitions
[2024-11-04T11:28:01.632+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Final stage: ResultStage 28 (toString at String.java:2951)
[2024-11-04T11:28:01.633+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:01.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:01.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[77] at toString at String.java:2951), which has no missing parents
[2024-11-04T11:28:01.635+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 50.4 KiB, free 432.7 MiB)
[2024-11-04T11:28:01.636+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 432.7 MiB)
[2024-11-04T11:28:01.636+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on eb88dbfa1959:36409 (size: 15.5 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.637+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:01.637+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 28 (MapPartitionsRDD[77] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:01.638+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0
[2024-11-04T11:28:01.639+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 221) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8368 bytes)
[2024-11-04T11:28:01.639+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 222) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8368 bytes)
[2024-11-04T11:28:01.647+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.18.0.5:35227 (size: 15.5 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:01.658+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.5 MiB)
[2024-11-04T11:28:01.713+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 221) in 74 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:01.713+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 222) in 74 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:01.714+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2024-11-04T11:28:01.715+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: ResultStage 28 (toString at String.java:2951) finished in 0.080 s
[2024-11-04T11:28:01.715+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:01.716+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2024-11-04T11:28:01.716+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DAGScheduler: Job 18 finished: toString at String.java:2951, took 0.083007 s
[2024-11-04T11:28:01.721+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] Created snapshot Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=7, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,7,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000007.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719681533; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=937d0e786f0d2d97aa4f4d7a57de16e3 versionId=null),None,1730719681533), checksumOpt=None)
[2024-11-04T11:28:01.722+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=7, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,7,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000007.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719681533; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=937d0e786f0d2d97aa4f4d7a57de16e3 versionId=null),None,1730719681533), checksumOpt=None)
[2024-11-04T11:28:01.723+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MapPartitionsRDD: Removing RDD 45 from persistence list
[2024-11-04T11:28:01.724+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManager: Removing RDD 45
[2024-11-04T11:28:01.725+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Compute snapshot for version: 7
[2024-11-04T11:28:01.728+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 204.7 KiB, free 432.5 MiB)
[2024-11-04T11:28:01.734+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 432.4 MiB)
[2024-11-04T11:28:01.735+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.735+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO SparkContext: Created broadcast 29 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:01.784+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.785+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:01.791+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on eb88dbfa1959:36409 in memory (size: 15.5 KiB, free: 434.1 MiB)
[2024-11-04T11:28:01.793+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.18.0.5:35227 in memory (size: 15.5 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:01.803+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_26_piece0 on eb88dbfa1959:36409 in memory (size: 138.5 KiB, free: 434.3 MiB)
[2024-11-04T11:28:01.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.18.0.5:35227 in memory (size: 138.5 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:01.962+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:01.963+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:01 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:02.015+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 205.0 KiB, free 433.3 MiB)
[2024-11-04T11:28:02.024+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.2 MiB)
[2024-11-04T11:28:02.026+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:02.027+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO SparkContext: Created broadcast 30 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:02.028+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 16791231 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:02.037+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Registering RDD 81 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 6
[2024-11-04T11:28:02.038+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Got map stage job 19 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-04T11:28:02.039+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Final stage: ShuffleMapStage 29 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:02.039+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:02.040+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:02.040+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[81] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:02.043+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 138.3 KiB, free 433.1 MiB)
[2024-11-04T11:28:02.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.0 MiB)
[2024-11-04T11:28:02.052+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:02.052+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:02.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[81] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:02.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks resource profile 0
[2024-11-04T11:28:02.059+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 223) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8357 bytes)
[2024-11-04T11:28:02.060+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 224) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8357 bytes)
[2024-11-04T11:28:02.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:02.105+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:02.166+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 224) in 107 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:02.166+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 223) in 107 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:02.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2024-11-04T11:28:02.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: ShuffleMapStage 29 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.127 s
[2024-11-04T11:28:02.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:02.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:02.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:02.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:02.221+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:02.222+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Removed broadcast_31_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:02.323+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Registering RDD 91 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 7
[2024-11-04T11:28:02.324+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Got map stage job 20 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:28:02.325+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Final stage: ShuffleMapStage 31 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:02.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
[2024-11-04T11:28:02.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:02.327+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[91] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:02.377+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 517.1 KiB, free 432.7 MiB)
[2024-11-04T11:28:02.379+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 432.6 MiB)
[2024-11-04T11:28:02.380+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on eb88dbfa1959:36409 (size: 123.3 KiB, free: 434.1 MiB)
[2024-11-04T11:28:02.380+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:02.381+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[91] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:28:02.382+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSchedulerImpl: Adding task set 31.0 with 50 tasks resource profile 0
[2024-11-04T11:28:02.383+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 225) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.384+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 3.0 in stage 31.0 (TID 226) (172.18.0.5, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.393+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.18.0.5:35227 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:02.414+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.18.0.5:39048
[2024-11-04T11:28:02.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_3 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.442+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_1 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.456+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 5.0 in stage 31.0 (TID 227) (172.18.0.5, executor 0, partition 5, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.457+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 3.0 in stage 31.0 (TID 226) in 73 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:28:02.458+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 11.0 in stage 31.0 (TID 228) (172.18.0.5, executor 0, partition 11, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 225) in 75 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:28:02.490+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_5 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.495+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_11 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 15.0 in stage 31.0 (TID 229) (172.18.0.5, executor 0, partition 15, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 5.0 in stage 31.0 (TID 227) in 50 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:28:02.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 18.0 in stage 31.0 (TID 230) (172.18.0.5, executor 0, partition 18, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.511+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 11.0 in stage 31.0 (TID 228) in 53 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:28:02.545+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_15 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.548+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_18 in memory on 172.18.0.5:35227 (size: 361.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 22.0 in stage 31.0 (TID 231) (172.18.0.5, executor 0, partition 22, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.565+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 15.0 in stage 31.0 (TID 229) in 59 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:28:02.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 24.0 in stage 31.0 (TID 232) (172.18.0.5, executor 0, partition 24, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 18.0 in stage 31.0 (TID 230) in 57 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:28:02.600+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_22 in memory on 172.18.0.5:35227 (size: 371.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.601+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_24 in memory on 172.18.0.5:35227 (size: 562.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.614+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 26.0 in stage 31.0 (TID 233) (172.18.0.5, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.615+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 24.0 in stage 31.0 (TID 232) in 48 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:28:02.616+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 29.0 in stage 31.0 (TID 234) (172.18.0.5, executor 0, partition 29, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.616+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 22.0 in stage 31.0 (TID 231) in 51 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:28:02.641+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_29 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.644+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_26 in memory on 172.18.0.5:35227 (size: 470.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.657+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 32.0 in stage 31.0 (TID 235) (172.18.0.5, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.657+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 29.0 in stage 31.0 (TID 234) in 42 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:28:02.658+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 34.0 in stage 31.0 (TID 236) (172.18.0.5, executor 0, partition 34, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.659+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 26.0 in stage 31.0 (TID 233) in 45 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:28:02.689+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_32 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.691+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_34 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.708+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 42.0 in stage 31.0 (TID 237) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.709+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 32.0 in stage 31.0 (TID 235) in 52 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:28:02.709+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 43.0 in stage 31.0 (TID 238) (172.18.0.5, executor 0, partition 43, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.710+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 34.0 in stage 31.0 (TID 236) in 52 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:28:02.741+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_43 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.742+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_42 in memory on 172.18.0.5:35227 (size: 567.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.758+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 239) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.759+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 43.0 in stage 31.0 (TID 238) in 50 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:28:02.760+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 2.0 in stage 31.0 (TID 240) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.760+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 42.0 in stage 31.0 (TID 237) in 52 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:28:02.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_0 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.789+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_2 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.802+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 4.0 in stage 31.0 (TID 241) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.803+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 2.0 in stage 31.0 (TID 240) in 44 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:28:02.804+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 6.0 in stage 31.0 (TID 242) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.805+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 239) in 46 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:28:02.831+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_4 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.837+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_6 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.846+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 7.0 in stage 31.0 (TID 243) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.847+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 4.0 in stage 31.0 (TID 241) in 45 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:28:02.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 8.0 in stage 31.0 (TID 244) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.852+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 6.0 in stage 31.0 (TID 242) in 49 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:28:02.874+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_7 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.878+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_8 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.887+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 9.0 in stage 31.0 (TID 245) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.888+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 7.0 in stage 31.0 (TID 243) in 42 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:28:02.893+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 10.0 in stage 31.0 (TID 246) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.894+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 8.0 in stage 31.0 (TID 244) in 43 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:28:02.921+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_9 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.926+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_10 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.947+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 12.0 in stage 31.0 (TID 247) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.948+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 9.0 in stage 31.0 (TID 245) in 60 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:28:02.953+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Starting task 13.0 in stage 31.0 (TID 248) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:02.953+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO TaskSetManager: Finished task 10.0 in stage 31.0 (TID 246) in 60 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:28:02.985+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_12 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:02.989+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:02 INFO BlockManagerInfo: Added rdd_88_13 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.000+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 14.0 in stage 31.0 (TID 249) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.001+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 12.0 in stage 31.0 (TID 247) in 54 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:28:03.005+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 16.0 in stage 31.0 (TID 250) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.006+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 13.0 in stage 31.0 (TID 248) in 53 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:28:03.031+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_14 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.033+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_16 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.047+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 17.0 in stage 31.0 (TID 251) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.048+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 14.0 in stage 31.0 (TID 249) in 47 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:28:03.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 19.0 in stage 31.0 (TID 252) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 16.0 in stage 31.0 (TID 250) in 45 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:28:03.076+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_17 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.079+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_19 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.092+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 20.0 in stage 31.0 (TID 253) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.092+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 17.0 in stage 31.0 (TID 251) in 46 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:28:03.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 21.0 in stage 31.0 (TID 254) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.097+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 19.0 in stage 31.0 (TID 252) in 47 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:28:03.121+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_20 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.125+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_21 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.137+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 23.0 in stage 31.0 (TID 255) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.138+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 20.0 in stage 31.0 (TID 253) in 46 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:28:03.141+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 25.0 in stage 31.0 (TID 256) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.142+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 21.0 in stage 31.0 (TID 254) in 45 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:28:03.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_23 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.177+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_25 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.183+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 27.0 in stage 31.0 (TID 257) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 23.0 in stage 31.0 (TID 255) in 46 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:28:03.196+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 28.0 in stage 31.0 (TID 258) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.197+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 25.0 in stage 31.0 (TID 256) in 56 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:28:03.214+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_27 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.228+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_28 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.230+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 30.0 in stage 31.0 (TID 259) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.231+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 27.0 in stage 31.0 (TID 257) in 48 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:28:03.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 31.0 in stage 31.0 (TID 260) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 28.0 in stage 31.0 (TID 258) in 48 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:28:03.257+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_30 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.270+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_31 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.271+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 33.0 in stage 31.0 (TID 261) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.272+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 30.0 in stage 31.0 (TID 259) in 43 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:28:03.285+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 35.0 in stage 31.0 (TID 262) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.286+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 31.0 in stage 31.0 (TID 260) in 43 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:28:03.299+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_33 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_35 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.313+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 36.0 in stage 31.0 (TID 263) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.314+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 33.0 in stage 31.0 (TID 261) in 42 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:28:03.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 37.0 in stage 31.0 (TID 264) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 35.0 in stage 31.0 (TID 262) in 41 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:28:03.344+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_36 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.356+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_37 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.358+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 38.0 in stage 31.0 (TID 265) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.359+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 36.0 in stage 31.0 (TID 263) in 46 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:28:03.371+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 39.0 in stage 31.0 (TID 266) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.372+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 37.0 in stage 31.0 (TID 264) in 46 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:28:03.408+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_38 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.423+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_39 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 40.0 in stage 31.0 (TID 267) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 38.0 in stage 31.0 (TID 265) in 72 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:28:03.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 41.0 in stage 31.0 (TID 268) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 39.0 in stage 31.0 (TID 266) in 69 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:28:03.460+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_40 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.468+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_41 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 44.0 in stage 31.0 (TID 269) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 40.0 in stage 31.0 (TID 267) in 45 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:28:03.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 45.0 in stage 31.0 (TID 270) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 41.0 in stage 31.0 (TID 268) in 43 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:28:03.501+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_44 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_45 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 46.0 in stage 31.0 (TID 271) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.516+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 44.0 in stage 31.0 (TID 269) in 43 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:28:03.522+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 47.0 in stage 31.0 (TID 272) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.523+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 45.0 in stage 31.0 (TID 270) in 41 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:28:03.544+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_46 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.549+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_47 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 48.0 in stage 31.0 (TID 273) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 46.0 in stage 31.0 (TID 271) in 43 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:28:03.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 49.0 in stage 31.0 (TID 274) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:03.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 47.0 in stage 31.0 (TID 272) in 40 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:28:03.585+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_48 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.588+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added rdd_88_49 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-04T11:28:03.600+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 48.0 in stage 31.0 (TID 273) in 42 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:28:03.601+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 49.0 in stage 31.0 (TID 274) in 40 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:28:03.602+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2024-11-04T11:28:03.602+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: ShuffleMapStage 31 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.275 s
[2024-11-04T11:28:03.603+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:03.603+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:03.604+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:03.604+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:03.621+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:03.622+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Got job 21 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:28:03.623+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Final stage: ResultStage 34 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:03.623+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
[2024-11-04T11:28:03.624+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:03.625+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[94] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:03.629+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 452.3 KiB, free 432.2 MiB)
[2024-11-04T11:28:03.631+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 108.2 KiB, free 432.0 MiB)
[2024-11-04T11:28:03.632+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on eb88dbfa1959:36409 (size: 108.2 KiB, free: 434.0 MiB)
[2024-11-04T11:28:03.633+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:03.633+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[94] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:03.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2024-11-04T11:28:03.635+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 275) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:03.643+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.18.0.5:35227 (size: 108.2 KiB, free: 1048.5 MiB)
[2024-11-04T11:28:03.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.18.0.5:39048
[2024-11-04T11:28:03.672+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 275) in 38 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:03.673+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2024-11-04T11:28:03.674+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: ResultStage 34 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.048 s
[2024-11-04T11:28:03.674+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:03.675+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2024-11-04T11:28:03.676+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO DAGScheduler: Job 21 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.051602 s
[2024-11-04T11:28:03.682+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Done
[2024-11-04T11:28:03.683+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO OptimisticTransaction: [tableId=86213540,txnId=386d9b55] Committed delta #7 to s3a://lakehouse/merge_data-movies/merged_data/_delta_log
[2024-11-04T11:28:03.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO OptimisticTransaction: [tableId=86213540,txnId=eb657e5c] Updated metadata from - to Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780))
[2024-11-04T11:28:03.861+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Removed broadcast_32_piece0 on eb88dbfa1959:36409 in memory (size: 123.3 KiB, free: 434.1 MiB)
[2024-11-04T11:28:03.863+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.18.0.5:35227 in memory (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:03.866+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Removed broadcast_33_piece0 on eb88dbfa1959:36409 in memory (size: 108.2 KiB, free: 434.2 MiB)
[2024-11-04T11:28:03.867+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:03 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.18.0.5:35227 in memory (size: 108.2 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.003+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:04.004+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:04.005+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(keywords),IsNotNull(id)
[2024-11-04T11:28:04.006+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(keywords#1),isnotnull(id#0L)
[2024-11-04T11:28:04.008+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:04.008+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#1566, Some(Etc/UTC)), lambdafunction(lambda x#1578.name, lambda x#1578, false))),isnotnull(id#1568L)
[2024-11-04T11:28:04.049+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-04T11:28:04.068+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:04.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO CodeGenerator: Code generated in 10.772499 ms
[2024-11-04T11:28:04.084+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 206.5 KiB, free 433.0 MiB)
[2024-11-04T11:28:04.097+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 433.0 MiB)
[2024-11-04T11:28:04.097+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on eb88dbfa1959:36409 (size: 36.6 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.098+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Created broadcast 34 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:04.099+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:04.108+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Registering RDD 98 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 8
[2024-11-04T11:28:04.109+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Got map stage job 22 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:28:04.110+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:04.110+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:04.111+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:04.112+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:04.113+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[98] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:04.114+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 31.8 KiB, free 433.0 MiB)
[2024-11-04T11:28:04.114+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 14.1 KiB, free 432.9 MiB)
[2024-11-04T11:28:04.115+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on eb88dbfa1959:36409 (size: 14.1 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.116+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:04.117+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[98] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:04.118+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2024-11-04T11:28:04.118+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 276) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7912 bytes)
[2024-11-04T11:28:04.137+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManager: Removing RDD 45
[2024-11-04T11:28:04.139+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.18.0.5:35227 (size: 14.1 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.142+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.143+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.146+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO CodeGenerator: Code generated in 31.255293 ms
[2024-11-04T11:28:04.147+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.150+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on eb88dbfa1959:36409 in memory (size: 36.7 KiB, free: 434.3 MiB)
[2024-11-04T11:28:04.151+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.18.0.5:35227 in memory (size: 36.7 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.156+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO CodeGenerator: Code generated in 8.473401 ms
[2024-11-04T11:28:04.160+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 206.7 KiB, free 433.4 MiB)
[2024-11-04T11:28:04.166+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 433.4 MiB)
[2024-11-04T11:28:04.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on eb88dbfa1959:36409 (size: 36.7 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Created broadcast 36 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:04.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:04.170+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.18.0.5:35227 (size: 36.6 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.180+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Registering RDD 106 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 9
[2024-11-04T11:28:04.181+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Got map stage job 23 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:04.182+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:04.182+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:04.183+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:04.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[106] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:04.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 49.8 KiB, free 433.4 MiB)
[2024-11-04T11:28:04.185+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 433.3 MiB)
[2024-11-04T11:28:04.186+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on eb88dbfa1959:36409 (size: 19.7 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.187+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:04.187+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[106] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:04.188+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSchedulerImpl: Adding task set 36.0 with 2 tasks resource profile 0
[2024-11-04T11:28:04.189+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 277) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:28:04.196+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.18.0.5:35227 (size: 19.7 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.248+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.18.0.5:35227 (size: 36.7 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:04.407+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Starting task 1.0 in stage 36.0 (TID 278) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:28:04.408+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 276) in 291 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:04.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2024-11-04T11:28:04.410+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: ShuffleMapStage 35 (save at NativeMethodAccessorImpl.java:0) finished in 0.300 s
[2024-11-04T11:28:04.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:04.412+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: running: Set(ShuffleMapStage 36)
[2024-11-04T11:28:04.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:04.414+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:04.445+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:04.446+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:04.458+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1480508, minimum partition size: 1048576
[2024-11-04T11:28:04.487+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Finished task 1.0 in stage 36.0 (TID 278) in 80 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:04.504+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:04.505+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Got job 24 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2024-11-04T11:28:04.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Final stage: ResultStage 38 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-11-04T11:28:04.507+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
[2024-11-04T11:28:04.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:04.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[109] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-11-04T11:28:04.510+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 31.0 KiB, free 433.3 MiB)
[2024-11-04T11:28:04.512+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 14.5 KiB, free 433.3 MiB)
[2024-11-04T11:28:04.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on eb88dbfa1959:36409 (size: 14.5 KiB, free: 434.2 MiB)
[2024-11-04T11:28:04.514+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:04.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 38 (MapPartitionsRDD[109] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:04.516+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks resource profile 0
[2024-11-04T11:28:04.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 279) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:04.532+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.18.0.5:35227 (size: 14.5 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:04.538+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 172.18.0.5:39048
[2024-11-04T11:28:04.880+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Added taskresult_279 in memory on 172.18.0.5:35227 (size: 2.5 MiB, free: 1046.1 MiB)
[2024-11-04T11:28:04.884+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 280) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:04.911+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TransportClientFactory: Successfully created connection to /172.18.0.5:35227 after 3 ms (0 ms spent in bootstraps)
[2024-11-04T11:28:04.949+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 279) in 432 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:04.958+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:04 INFO BlockManagerInfo: Removed taskresult_279 on 172.18.0.5:35227 in memory (size: 2.5 MiB, free: 1048.6 MiB)
[2024-11-04T11:28:05.048+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added taskresult_280 in memory on 172.18.0.5:35227 (size: 2.5 MiB, free: 1046.1 MiB)
[2024-11-04T11:28:05.065+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 280) in 181 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:05.066+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
[2024-11-04T11:28:05.067+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: ResultStage 38 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.558 s
[2024-11-04T11:28:05.068+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:05.068+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
[2024-11-04T11:28:05.069+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Removed taskresult_280 on 172.18.0.5:35227 in memory (size: 2.5 MiB, free: 1048.6 MiB)
[2024-11-04T11:28:05.069+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Job 24 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.563171 s
[2024-11-04T11:28:05.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO CodeGenerator: Code generated in 5.549286 ms
[2024-11-04T11:28:05.111+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Removed broadcast_38_piece0 on eb88dbfa1959:36409 in memory (size: 14.5 KiB, free: 434.2 MiB)
[2024-11-04T11:28:05.112+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.18.0.5:35227 in memory (size: 14.5 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:05.116+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Removed broadcast_35_piece0 on eb88dbfa1959:36409 in memory (size: 14.1 KiB, free: 434.2 MiB)
[2024-11-04T11:28:05.117+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.18.0.5:35227 in memory (size: 14.1 KiB, free: 1048.7 MiB)
[2024-11-04T11:28:05.236+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 429.4 MiB)
[2024-11-04T11:28:05.237+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on eb88dbfa1959:36409 (size: 4.0 MiB, free: 430.2 MiB)
[2024-11-04T11:28:05.238+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_39_piece1 stored as bytes in memory (estimated size 1563.5 KiB, free 427.9 MiB)
[2024-11-04T11:28:05.238+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_39_piece1 in memory on eb88dbfa1959:36409 (size: 1563.5 KiB, free: 428.7 MiB)
[2024-11-04T11:28:05.239+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:05.250+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:05.250+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:05.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO CodeGenerator: Code generated in 14.229222 ms
[2024-11-04T11:28:05.382+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO CodeGenerator: Code generated in 52.5589 ms
[2024-11-04T11:28:05.387+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 208.5 KiB, free 427.7 MiB)
[2024-11-04T11:28:05.395+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 427.6 MiB)
[2024-11-04T11:28:05.396+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on eb88dbfa1959:36409 (size: 37.2 KiB, free: 428.7 MiB)
[2024-11-04T11:28:05.397+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO SparkContext: Created broadcast 40 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:05.398+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10691623 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:05.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Registering RDD 115 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 10
[2024-11-04T11:28:05.412+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Got map stage job 25 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:05.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Final stage: ShuffleMapStage 39 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:05.414+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:05.414+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:05.415+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[115] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:05.471+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 60.3 KiB, free 427.6 MiB)
[2024-11-04T11:28:05.473+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 22.9 KiB, free 427.5 MiB)
[2024-11-04T11:28:05.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on eb88dbfa1959:36409 (size: 22.9 KiB, free: 428.7 MiB)
[2024-11-04T11:28:05.474+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:05.475+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[115] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:05.476+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO TaskSchedulerImpl: Adding task set 39.0 with 2 tasks resource profile 0
[2024-11-04T11:28:05.477+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 281) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7910 bytes)
[2024-11-04T11:28:05.487+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.18.0.5:35227 (size: 22.9 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:05.615+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.18.0.5:35227 (size: 4.0 MiB, free: 1044.6 MiB)
[2024-11-04T11:28:05.623+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_39_piece1 in memory on 172.18.0.5:35227 (size: 1563.5 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:05.655+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:05 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.18.0.5:35227 (size: 37.2 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:06.821+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSetManager: Starting task 1.0 in stage 39.0 (TID 282) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7910 bytes)
[2024-11-04T11:28:06.822+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 277) in 2633 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:06.823+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2024-11-04T11:28:06.823+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: ShuffleMapStage 36 (save at NativeMethodAccessorImpl.java:0) finished in 2.640 s
[2024-11-04T11:28:06.824+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:06.825+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: running: Set(ShuffleMapStage 39)
[2024-11-04T11:28:06.826+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:06.827+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:06.833+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO ShufflePartitionsUtil: For shuffle(9), advisory target size: 67108864, actual target size 5348362, minimum partition size: 1048576
[2024-11-04T11:28:06.850+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:06.878+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO CodeGenerator: Code generated in 24.665918 ms
[2024-11-04T11:28:06.889+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSetManager: Finished task 1.0 in stage 39.0 (TID 282) in 68 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:06.892+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Registering RDD 118 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 11
[2024-11-04T11:28:06.893+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Got map stage job 26 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:06.893+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:06.894+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)
[2024-11-04T11:28:06.894+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:06.895+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[118] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:06.902+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 51.5 KiB, free 427.5 MiB)
[2024-11-04T11:28:06.904+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 20.8 KiB, free 427.5 MiB)
[2024-11-04T11:28:06.904+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on eb88dbfa1959:36409 (size: 20.8 KiB, free: 428.6 MiB)
[2024-11-04T11:28:06.905+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:06.905+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[118] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:06.906+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSchedulerImpl: Adding task set 41.0 with 2 tasks resource profile 0
[2024-11-04T11:28:06.907+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 283) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:06.917+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.18.0.5:35227 (size: 20.8 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:06.924+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:06 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.18.0.5:39048
[2024-11-04T11:28:07.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 284) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:07.091+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 283) in 184 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:07.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 284) in 154 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:07.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2024-11-04T11:28:07.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: ShuffleMapStage 41 (save at NativeMethodAccessorImpl.java:0) finished in 0.348 s
[2024-11-04T11:28:07.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:07.246+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: running: Set(ShuffleMapStage 39)
[2024-11-04T11:28:07.247+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:07.248+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:07.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 281) in 1850 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:07.327+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2024-11-04T11:28:07.328+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: ShuffleMapStage 39 (save at NativeMethodAccessorImpl.java:0) finished in 1.914 s
[2024-11-04T11:28:07.329+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:07.329+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:07.330+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:07.331+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:07.347+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO ShufflePartitionsUtil: For shuffle(10, 11), advisory target size: 67108864, actual target size 19291977, minimum partition size: 1048576
[2024-11-04T11:28:07.398+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_42_piece0 on eb88dbfa1959:36409 in memory (size: 20.8 KiB, free: 428.7 MiB)
[2024-11-04T11:28:07.401+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.18.0.5:35227 in memory (size: 20.8 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:07.425+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO CodeGenerator: Code generated in 43.92326 ms
[2024-11-04T11:28:07.436+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO CodeGenerator: Code generated in 8.174118 ms
[2024-11-04T11:28:07.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO CodeGenerator: Code generated in 6.693123 ms
[2024-11-04T11:28:07.523+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:07.524+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Got job 27 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:07.525+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Final stage: ResultStage 45 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:07.525+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42, ShuffleMapStage 44)
[2024-11-04T11:28:07.526+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:07.526+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[124] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:07.551+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 447.2 KiB, free 427.1 MiB)
[2024-11-04T11:28:07.554+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 158.9 KiB, free 426.9 MiB)
[2024-11-04T11:28:07.555+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on eb88dbfa1959:36409 (size: 158.9 KiB, free: 428.5 MiB)
[2024-11-04T11:28:07.555+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:07.556+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 45 (MapPartitionsRDD[124] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:07.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSchedulerImpl: Adding task set 45.0 with 2 tasks resource profile 0
[2024-11-04T11:28:07.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 285) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7649 bytes)
[2024-11-04T11:28:07.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO TaskSetManager: Starting task 1.0 in stage 45.0 (TID 286) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7649 bytes)
[2024-11-04T11:28:07.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.18.0.5:35227 (size: 158.9 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:07.592+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_41_piece0 on eb88dbfa1959:36409 in memory (size: 22.9 KiB, free: 428.5 MiB)
[2024-11-04T11:28:07.593+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.18.0.5:35227 in memory (size: 22.9 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:07.597+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_37_piece0 on eb88dbfa1959:36409 in memory (size: 19.7 KiB, free: 428.5 MiB)
[2024-11-04T11:28:07.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.18.0.5:35227 in memory (size: 19.7 KiB, free: 1043.0 MiB)
[2024-11-04T11:28:07.625+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.18.0.5:39048
[2024-11-04T11:28:07.649+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.18.0.5:39048
[2024-11-04T11:28:08.802+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 285) in 1244 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:08.802+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSetManager: Finished task 1.0 in stage 45.0 (TID 286) in 1244 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:08.803+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2024-11-04T11:28:08.803+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: ResultStage 45 (save at NativeMethodAccessorImpl.java:0) finished in 1.275 s
[2024-11-04T11:28:08.804+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:08.804+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
[2024-11-04T11:28:08.805+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Job 27 finished: save at NativeMethodAccessorImpl.java:0, took 1.280128 s
[2024-11-04T11:28:08.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO FileFormatWriter: Start to commit write Job cd209c69-e0d6-4771-9f53-91ca04ca877e.
[2024-11-04T11:28:08.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO FileFormatWriter: Write Job cd209c69-e0d6-4771-9f53-91ca04ca877e committed. Elapsed time: 0 ms.
[2024-11-04T11:28:08.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO FileFormatWriter: Finished processing stats for write job cd209c69-e0d6-4771-9f53-91ca04ca877e.
[2024-11-04T11:28:08.932+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO BlockManagerInfo: Removed broadcast_43_piece0 on eb88dbfa1959:36409 in memory (size: 158.9 KiB, free: 428.7 MiB)
[2024-11-04T11:28:08.934+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.18.0.5:35227 in memory (size: 158.9 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:08.962+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:08.963+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Got job 28 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:28:08.964+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Final stage: ResultStage 47 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:08.965+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 46)
[2024-11-04T11:28:08.965+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:08.966+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[126] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:08.971+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 592.2 KiB, free 427.1 MiB)
[2024-11-04T11:28:08.974+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 138.6 KiB, free 427.0 MiB)
[2024-11-04T11:28:08.975+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on eb88dbfa1959:36409 (size: 138.6 KiB, free: 428.6 MiB)
[2024-11-04T11:28:08.976+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:08.976+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 47 (MapPartitionsRDD[126] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:28:08.977+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSchedulerImpl: Adding task set 47.0 with 50 tasks resource profile 0
[2024-11-04T11:28:08.978+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 287) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:08.979+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 288) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:08.990+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:08 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.18.0.5:35227 (size: 138.6 KiB, free: 1043.0 MiB)
[2024-11-04T11:28:09.006+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 289) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.008+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 287) in 29 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:28:09.009+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 290) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.010+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 288) in 31 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:28:09.021+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 4.0 in stage 47.0 (TID 291) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.022+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 289) in 16 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:28:09.023+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 5.0 in stage 47.0 (TID 292) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.024+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 290) in 15 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:28:09.033+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 6.0 in stage 47.0 (TID 293) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.034+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 4.0 in stage 47.0 (TID 291) in 13 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:28:09.036+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 7.0 in stage 47.0 (TID 294) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.036+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 5.0 in stage 47.0 (TID 292) in 13 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:28:09.046+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 8.0 in stage 47.0 (TID 295) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.047+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 6.0 in stage 47.0 (TID 293) in 13 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:28:09.047+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 9.0 in stage 47.0 (TID 296) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.048+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 7.0 in stage 47.0 (TID 294) in 13 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:28:09.058+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 10.0 in stage 47.0 (TID 297) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.059+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 11.0 in stage 47.0 (TID 298) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.060+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 9.0 in stage 47.0 (TID 296) in 12 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:28:09.061+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 8.0 in stage 47.0 (TID 295) in 14 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:28:09.070+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 12.0 in stage 47.0 (TID 299) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.071+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 10.0 in stage 47.0 (TID 297) in 12 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:28:09.072+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 13.0 in stage 47.0 (TID 300) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.072+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 11.0 in stage 47.0 (TID 298) in 13 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:28:09.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 14.0 in stage 47.0 (TID 301) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.082+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 12.0 in stage 47.0 (TID 299) in 12 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:28:09.084+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 15.0 in stage 47.0 (TID 302) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.085+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 13.0 in stage 47.0 (TID 300) in 13 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:28:09.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 16.0 in stage 47.0 (TID 303) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 14.0 in stage 47.0 (TID 301) in 12 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:28:09.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 17.0 in stage 47.0 (TID 304) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 15.0 in stage 47.0 (TID 302) in 11 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:28:09.105+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 18.0 in stage 47.0 (TID 305) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.106+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 16.0 in stage 47.0 (TID 303) in 13 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:28:09.107+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 19.0 in stage 47.0 (TID 306) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.108+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 17.0 in stage 47.0 (TID 304) in 13 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:28:09.118+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 20.0 in stage 47.0 (TID 307) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.118+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 18.0 in stage 47.0 (TID 305) in 13 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:28:09.119+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 21.0 in stage 47.0 (TID 308) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.120+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 19.0 in stage 47.0 (TID 306) in 13 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:28:09.130+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 22.0 in stage 47.0 (TID 309) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.131+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 23.0 in stage 47.0 (TID 310) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.132+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 21.0 in stage 47.0 (TID 308) in 12 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:28:09.132+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 20.0 in stage 47.0 (TID 307) in 14 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:28:09.141+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 24.0 in stage 47.0 (TID 311) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.142+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 25.0 in stage 47.0 (TID 312) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.143+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 23.0 in stage 47.0 (TID 310) in 12 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:28:09.143+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 22.0 in stage 47.0 (TID 309) in 14 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:28:09.152+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 26.0 in stage 47.0 (TID 313) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.153+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 27.0 in stage 47.0 (TID 314) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.154+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 24.0 in stage 47.0 (TID 311) in 12 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:28:09.155+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 25.0 in stage 47.0 (TID 312) in 12 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:28:09.165+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 28.0 in stage 47.0 (TID 315) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.166+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 29.0 in stage 47.0 (TID 316) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.166+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 26.0 in stage 47.0 (TID 313) in 14 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:28:09.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 27.0 in stage 47.0 (TID 314) in 13 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:28:09.178+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 30.0 in stage 47.0 (TID 317) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.179+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 31.0 in stage 47.0 (TID 318) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.180+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 28.0 in stage 47.0 (TID 315) in 15 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:28:09.180+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 29.0 in stage 47.0 (TID 316) in 14 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:28:09.193+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 32.0 in stage 47.0 (TID 319) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.194+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 31.0 in stage 47.0 (TID 318) in 15 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:28:09.194+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 33.0 in stage 47.0 (TID 320) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.195+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 30.0 in stage 47.0 (TID 317) in 18 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:28:09.206+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 34.0 in stage 47.0 (TID 321) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.207+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 35.0 in stage 47.0 (TID 322) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.208+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 32.0 in stage 47.0 (TID 319) in 15 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:28:09.209+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 33.0 in stage 47.0 (TID 320) in 14 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:28:09.219+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 36.0 in stage 47.0 (TID 323) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.220+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 35.0 in stage 47.0 (TID 322) in 12 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:28:09.220+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 37.0 in stage 47.0 (TID 324) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.221+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 34.0 in stage 47.0 (TID 321) in 14 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:28:09.231+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 38.0 in stage 47.0 (TID 325) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.232+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 36.0 in stage 47.0 (TID 323) in 12 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:28:09.233+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 39.0 in stage 47.0 (TID 326) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.233+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 37.0 in stage 47.0 (TID 324) in 13 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:28:09.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 40.0 in stage 47.0 (TID 327) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 38.0 in stage 47.0 (TID 325) in 12 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:28:09.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 41.0 in stage 47.0 (TID 328) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.246+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 39.0 in stage 47.0 (TID 326) in 13 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:28:09.256+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 42.0 in stage 47.0 (TID 329) (172.18.0.5, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.256+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 41.0 in stage 47.0 (TID 328) in 12 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:28:09.257+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 43.0 in stage 47.0 (TID 330) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.258+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 40.0 in stage 47.0 (TID 327) in 14 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:28:09.267+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 44.0 in stage 47.0 (TID 331) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.268+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 43.0 in stage 47.0 (TID 330) in 11 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:28:09.269+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 45.0 in stage 47.0 (TID 332) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.269+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 42.0 in stage 47.0 (TID 329) in 14 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:28:09.280+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 46.0 in stage 47.0 (TID 333) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.281+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 45.0 in stage 47.0 (TID 332) in 13 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:28:09.282+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 47.0 in stage 47.0 (TID 334) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.283+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 44.0 in stage 47.0 (TID 331) in 15 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:28:09.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 48.0 in stage 47.0 (TID 335) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.297+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 46.0 in stage 47.0 (TID 333) in 16 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:28:09.297+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 49.0 in stage 47.0 (TID 336) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-04T11:28:09.298+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 47.0 in stage 47.0 (TID 334) in 17 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:28:09.308+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 48.0 in stage 47.0 (TID 335) in 13 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:28:09.310+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 49.0 in stage 47.0 (TID 336) in 13 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:28:09.311+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2024-11-04T11:28:09.311+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: ResultStage 47 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.344 s
[2024-11-04T11:28:09.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:09.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2024-11-04T11:28:09.313+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Job 28 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.348992 s
[2024-11-04T11:28:09.319+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO OptimisticTransaction: [tableId=86213540,txnId=eb657e5c] Attempting to commit version 8 with 6 actions with Serializable isolation level
[2024-11-04T11:28:09.383+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DeltaLog: Creating a new snapshot v8 for commit version 8
[2024-11-04T11:28:09.384+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DeltaLog: Loading version 8.
[2024-11-04T11:28:09.387+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 9, totalFileSize: 35188)
[2024-11-04T11:28:09.418+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:09.419+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:09.432+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 205.0 KiB, free 426.8 MiB)
[2024-11-04T11:28:09.438+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 426.7 MiB)
[2024-11-04T11:28:09.439+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 428.5 MiB)
[2024-11-04T11:28:09.440+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Created broadcast 45 from toString at String.java:2951
[2024-11-04T11:28:09.441+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18891962 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:09.452+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-04T11:28:09.453+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Got job 29 (toString at String.java:2951) with 2 output partitions
[2024-11-04T11:28:09.454+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Final stage: ResultStage 48 (toString at String.java:2951)
[2024-11-04T11:28:09.454+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:09.455+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:09.456+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[130] at toString at String.java:2951), which has no missing parents
[2024-11-04T11:28:09.457+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 50.4 KiB, free 426.7 MiB)
[2024-11-04T11:28:09.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 426.7 MiB)
[2024-11-04T11:28:09.459+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on eb88dbfa1959:36409 (size: 15.5 KiB, free: 428.5 MiB)
[2024-11-04T11:28:09.460+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:09.461+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 48 (MapPartitionsRDD[130] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:09.461+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Adding task set 48.0 with 2 tasks resource profile 0
[2024-11-04T11:28:09.462+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 337) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8502 bytes)
[2024-11-04T11:28:09.463+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 338) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8368 bytes)
[2024-11-04T11:28:09.471+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.18.0.5:35227 (size: 15.5 KiB, free: 1043.0 MiB)
[2024-11-04T11:28:09.484+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:09.546+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 338) in 84 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:09.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 337) in 96 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:09.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2024-11-04T11:28:09.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: ResultStage 48 (toString at String.java:2951) finished in 0.102 s
[2024-11-04T11:28:09.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:09.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2024-11-04T11:28:09.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Job 29 finished: toString at String.java:2951, took 0.105728 s
[2024-11-04T11:28:09.565+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] Created snapshot Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=8, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,8,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000007.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719681533; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=937d0e786f0d2d97aa4f4d7a57de16e3 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000008.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719689351; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=197a021cd8569378b3f91b86ae56b5d7 versionId=null),None,1730719689351), checksumOpt=None)
[2024-11-04T11:28:09.566+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log, version=8, metadata=Metadata(86213540-d86c-4bf5-a754-381e7ce4af70,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres","type":"string","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"original_title","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"popularity","type":"double","nullable":true,"metadata":{}},{"name":"poster_path","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"status","type":"string","nullable":true,"metadata":{}},{"name":"tagline","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"keywords","type":"string","nullable":true,"metadata":{}},{"name":"keyword_convert","type":"string","nullable":true,"metadata":{}},{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719197780)), logSegment=LogSegment(s3a://lakehouse/merge_data-movies/merged_data/_delta_log,8,WrappedArray(S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000000.json; isDirectory=false; length=1847; replication=1; blocksize=33554432; modification_time=1730719206719; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=b63bbdc02a9de9a3c3d7e0f1c3290374 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000001.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719219385; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=406d268e6336acf8311ab9691a922a04 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000002.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719227552; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=d4a4cf33fb3e113d6a1e652a1f7d8eee versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000003.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719565951; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=40db4b247d2f28a93ba11f75c9bbe580 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000004.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719574180; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=5aa96bbe223143cf6f8916daff0daae6 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000005.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719581904; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=530cec8425d2026333b068118b4f1c27 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000006.json; isDirectory=false; length=2218; replication=1; blocksize=33554432; modification_time=1730719672364; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=94a5df9b4a06b108161b9be08bd63627 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000007.json; isDirectory=false; length=2477; replication=1; blocksize=33554432; modification_time=1730719681533; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=937d0e786f0d2d97aa4f4d7a57de16e3 versionId=null, S3AFileStatus{path=s3a://lakehouse/merge_data-movies/merged_data/_delta_log/00000000000000000008.json; isDirectory=false; length=7158; replication=1; blocksize=33554432; modification_time=1730719689351; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=197a021cd8569378b3f91b86ae56b5d7 versionId=null),None,1730719689351), checksumOpt=None)
[2024-11-04T11:28:09.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MapPartitionsRDD: Removing RDD 88 from persistence list
[2024-11-04T11:28:09.568+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManager: Removing RDD 88
[2024-11-04T11:28:09.568+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Compute snapshot for version: 8
[2024-11-04T11:28:09.570+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 204.7 KiB, free 426.5 MiB)
[2024-11-04T11:28:09.580+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_44_piece0 on eb88dbfa1959:36409 in memory (size: 138.6 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.582+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.18.0.5:35227 in memory (size: 138.6 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:09.583+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 427.2 MiB)
[2024-11-04T11:28:09.584+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.586+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Created broadcast 47 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:09.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_45_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:09.591+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_46_piece0 on eb88dbfa1959:36409 in memory (size: 15.5 KiB, free: 428.7 MiB)
[2024-11-04T11:28:09.591+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.18.0.5:35227 in memory (size: 15.5 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:09.723+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:09.724+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:09.765+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 205.0 KiB, free 427.3 MiB)
[2024-11-04T11:28:09.771+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 427.2 MiB)
[2024-11-04T11:28:09.772+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.772+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Created broadcast 48 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:09.773+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 18891962 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:09.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Registering RDD 134 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 12
[2024-11-04T11:28:09.781+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Got map stage job 30 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-04T11:28:09.782+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Final stage: ShuffleMapStage 49 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:09.782+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:09.783+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:09.783+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[134] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:09.786+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 138.3 KiB, free 427.1 MiB)
[2024-11-04T11:28:09.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 427.0 MiB)
[2024-11-04T11:28:09.788+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.789+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:09.789+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[134] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:09.790+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Adding task set 49.0 with 2 tasks resource profile 0
[2024-11-04T11:28:09.791+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 339) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8491 bytes)
[2024-11-04T11:28:09.791+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 340) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 8357 bytes)
[2024-11-04T11:28:09.799+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:09.816+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:09.886+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 340) in 95 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:09.901+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 339) in 111 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:09.902+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2024-11-04T11:28:09.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: ShuffleMapStage 49 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.120 s
[2024-11-04T11:28:09.904+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:09.904+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:09.905+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:09.906+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:09.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_49_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 428.6 MiB)
[2024-11-04T11:28:09.973+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:09 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1043.1 MiB)
[2024-11-04T11:28:10.127+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Registering RDD 144 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 13
[2024-11-04T11:28:10.128+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Got map stage job 31 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:28:10.128+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Final stage: ShuffleMapStage 51 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:10.129+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
[2024-11-04T11:28:10.129+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:10.130+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Submitting ShuffleMapStage 51 (MapPartitionsRDD[144] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:10.164+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 517.1 KiB, free 426.7 MiB)
[2024-11-04T11:28:10.167+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 123.2 KiB, free 426.6 MiB)
[2024-11-04T11:28:10.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on eb88dbfa1959:36409 (size: 123.2 KiB, free: 428.5 MiB)
[2024-11-04T11:28:10.168+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:10.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 51 (MapPartitionsRDD[144] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:28:10.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSchedulerImpl: Adding task set 51.0 with 50 tasks resource profile 0
[2024-11-04T11:28:10.170+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 1.0 in stage 51.0 (TID 341) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.171+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 3.0 in stage 51.0 (TID 342) (172.18.0.5, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.180+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.18.0.5:35227 (size: 123.2 KiB, free: 1043.0 MiB)
[2024-11-04T11:28:10.194+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 172.18.0.5:39048
[2024-11-04T11:28:10.223+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_3 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.224+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_1 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.243+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 5.0 in stage 51.0 (TID 343) (172.18.0.5, executor 0, partition 5, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.244+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 3.0 in stage 51.0 (TID 342) in 74 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:28:10.245+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 11.0 in stage 51.0 (TID 344) (172.18.0.5, executor 0, partition 11, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.246+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 1.0 in stage 51.0 (TID 341) in 75 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:28:10.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_5 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.280+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_11 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.300+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 15.0 in stage 51.0 (TID 345) (172.18.0.5, executor 0, partition 15, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.301+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 11.0 in stage 51.0 (TID 344) in 56 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:28:10.302+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 18.0 in stage 51.0 (TID 346) (172.18.0.5, executor 0, partition 18, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.303+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 5.0 in stage 51.0 (TID 343) in 59 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:28:10.331+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_18 in memory on 172.18.0.5:35227 (size: 361.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.334+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_15 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.354+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 22.0 in stage 51.0 (TID 347) (172.18.0.5, executor 0, partition 22, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.355+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 18.0 in stage 51.0 (TID 346) in 53 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:28:10.358+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 24.0 in stage 51.0 (TID 348) (172.18.0.5, executor 0, partition 24, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.359+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 15.0 in stage 51.0 (TID 345) in 59 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:28:10.405+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_22 in memory on 172.18.0.5:35227 (size: 371.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_24 in memory on 172.18.0.5:35227 (size: 368.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.425+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 26.0 in stage 51.0 (TID 349) (172.18.0.5, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.426+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 22.0 in stage 51.0 (TID 347) in 72 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:28:10.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 29.0 in stage 51.0 (TID 350) (172.18.0.5, executor 0, partition 29, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 24.0 in stage 51.0 (TID 348) in 72 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:28:10.461+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_26 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.468+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_29 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.477+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 32.0 in stage 51.0 (TID 351) (172.18.0.5, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.478+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 26.0 in stage 51.0 (TID 349) in 52 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:28:10.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 34.0 in stage 51.0 (TID 352) (172.18.0.5, executor 0, partition 34, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.483+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 29.0 in stage 51.0 (TID 350) in 54 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:28:10.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_34 in memory on 172.18.0.5:35227 (size: 301.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_32 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.530+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 42.0 in stage 51.0 (TID 353) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.531+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 34.0 in stage 51.0 (TID 352) in 48 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:28:10.535+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 43.0 in stage 51.0 (TID 354) (172.18.0.5, executor 0, partition 43, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.536+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 32.0 in stage 51.0 (TID 351) in 58 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:28:10.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_42 in memory on 172.18.0.5:35227 (size: 835.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.568+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_43 in memory on 172.18.0.5:35227 (size: 302.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.578+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 45.0 in stage 51.0 (TID 355) (172.18.0.5, executor 0, partition 45, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.579+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 42.0 in stage 51.0 (TID 353) in 48 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:28:10.583+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 46.0 in stage 51.0 (TID 356) (172.18.0.5, executor 0, partition 46, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.584+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 43.0 in stage 51.0 (TID 354) in 49 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:28:10.606+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_45 in memory on 172.18.0.5:35227 (size: 1367.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.612+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_46 in memory on 172.18.0.5:35227 (size: 1350.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.620+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 357) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.621+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 45.0 in stage 51.0 (TID 355) in 43 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:28:10.625+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 2.0 in stage 51.0 (TID 358) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.625+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 46.0 in stage 51.0 (TID 356) in 42 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:28:10.646+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_0 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_2 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.659+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 4.0 in stage 51.0 (TID 359) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.660+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 357) in 40 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:28:10.664+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 6.0 in stage 51.0 (TID 360) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.665+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 2.0 in stage 51.0 (TID 358) in 40 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:28:10.687+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_4 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.690+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_6 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.701+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 7.0 in stage 51.0 (TID 361) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.702+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 4.0 in stage 51.0 (TID 359) in 44 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:28:10.704+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 8.0 in stage 51.0 (TID 362) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.705+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 6.0 in stage 51.0 (TID 360) in 41 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:28:10.729+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_7 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.730+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_8 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.742+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 9.0 in stage 51.0 (TID 363) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.743+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 8.0 in stage 51.0 (TID 362) in 38 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:28:10.743+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 10.0 in stage 51.0 (TID 364) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.744+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 7.0 in stage 51.0 (TID 361) in 42 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:28:10.768+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_10 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.768+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_9 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.781+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 12.0 in stage 51.0 (TID 365) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.782+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 9.0 in stage 51.0 (TID 363) in 39 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:28:10.782+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 13.0 in stage 51.0 (TID 366) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.783+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 10.0 in stage 51.0 (TID 364) in 39 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:28:10.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_12 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.808+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_13 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.818+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 14.0 in stage 51.0 (TID 367) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.819+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 12.0 in stage 51.0 (TID 365) in 38 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:28:10.821+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 16.0 in stage 51.0 (TID 368) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.821+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 13.0 in stage 51.0 (TID 366) in 39 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:28:10.845+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_14 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.847+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_16 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.862+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 17.0 in stage 51.0 (TID 369) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.862+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 14.0 in stage 51.0 (TID 367) in 44 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:28:10.863+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 19.0 in stage 51.0 (TID 370) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.864+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 16.0 in stage 51.0 (TID 368) in 43 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:28:10.886+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_19 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.887+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_17 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 20.0 in stage 51.0 (TID 371) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 19.0 in stage 51.0 (TID 370) in 41 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:28:10.904+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 21.0 in stage 51.0 (TID 372) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.905+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 17.0 in stage 51.0 (TID 369) in 43 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:28:10.928+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_20 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_21 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.950+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 23.0 in stage 51.0 (TID 373) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.951+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 20.0 in stage 51.0 (TID 371) in 48 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:28:10.951+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 25.0 in stage 51.0 (TID 374) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.952+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 21.0 in stage 51.0 (TID 372) in 48 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:28:10.982+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_25 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.984+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO BlockManagerInfo: Added rdd_141_23 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:10.998+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Starting task 27.0 in stage 51.0 (TID 375) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:10.999+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:10 INFO TaskSetManager: Finished task 25.0 in stage 51.0 (TID 374) in 47 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:28:11.000+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 28.0 in stage 51.0 (TID 376) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.001+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 23.0 in stage 51.0 (TID 373) in 51 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:28:11.026+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_27 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.028+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_28 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.041+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 30.0 in stage 51.0 (TID 377) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.041+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 27.0 in stage 51.0 (TID 375) in 43 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:28:11.042+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 31.0 in stage 51.0 (TID 378) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.043+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 28.0 in stage 51.0 (TID 376) in 42 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:28:11.072+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_30 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.073+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_31 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.089+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 33.0 in stage 51.0 (TID 379) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 30.0 in stage 51.0 (TID 377) in 49 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:28:11.090+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 35.0 in stage 51.0 (TID 380) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.091+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 31.0 in stage 51.0 (TID 378) in 49 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:28:11.121+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_33 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.122+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_35 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.137+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 36.0 in stage 51.0 (TID 381) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.138+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 35.0 in stage 51.0 (TID 380) in 48 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:28:11.139+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 37.0 in stage 51.0 (TID 382) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.140+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 33.0 in stage 51.0 (TID 379) in 50 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:28:11.169+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_36 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.171+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_37 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 38.0 in stage 51.0 (TID 383) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.185+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 36.0 in stage 51.0 (TID 381) in 47 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:28:11.188+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 39.0 in stage 51.0 (TID 384) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.189+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 37.0 in stage 51.0 (TID 382) in 51 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:28:11.242+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_38 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.254+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_39 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 40.0 in stage 51.0 (TID 385) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.300+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 38.0 in stage 51.0 (TID 383) in 112 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:28:11.301+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 41.0 in stage 51.0 (TID 386) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.303+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 39.0 in stage 51.0 (TID 384) in 111 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:28:11.366+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_40 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.369+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_41 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.392+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 44.0 in stage 51.0 (TID 387) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.393+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 40.0 in stage 51.0 (TID 385) in 98 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:28:11.394+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 47.0 in stage 51.0 (TID 388) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.395+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 41.0 in stage 51.0 (TID 386) in 97 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:28:11.423+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_47 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.424+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_44 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.437+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 48.0 in stage 51.0 (TID 389) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.437+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 44.0 in stage 51.0 (TID 387) in 45 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:28:11.438+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 49.0 in stage 51.0 (TID 390) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:11.439+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 47.0 in stage 51.0 (TID 388) in 45 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:28:11.463+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_48 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.464+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added rdd_141_49 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1043.0 MiB)
[2024-11-04T11:28:11.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 49.0 in stage 51.0 (TID 390) in 44 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:28:11.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 48.0 in stage 51.0 (TID 389) in 45 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:28:11.483+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2024-11-04T11:28:11.483+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: ShuffleMapStage 51 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.352 s
[2024-11-04T11:28:11.484+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:11.484+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:11.485+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:11.486+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:11.503+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:11.504+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Got job 32 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:28:11.504+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Final stage: ResultStage 54 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:11.505+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)
[2024-11-04T11:28:11.506+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:11.507+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[147] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:11.513+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 452.3 KiB, free 426.2 MiB)
[2024-11-04T11:28:11.515+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 426.0 MiB)
[2024-11-04T11:28:11.516+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on eb88dbfa1959:36409 (size: 108.1 KiB, free: 428.4 MiB)
[2024-11-04T11:28:11.516+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:11.517+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[147] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:11.517+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2024-11-04T11:28:11.518+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 391) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:11.527+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.18.0.5:35227 (size: 108.1 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:11.535+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 172.18.0.5:39048
[2024-11-04T11:28:11.556+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 391) in 39 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:11.557+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2024-11-04T11:28:11.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: ResultStage 54 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.052 s
[2024-11-04T11:28:11.558+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:11.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2024-11-04T11:28:11.560+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Job 32 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.055561 s
[2024-11-04T11:28:11.566+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO Snapshot: [tableId=86213540-d86c-4bf5-a754-381e7ce4af70] DELTA: Done
[2024-11-04T11:28:11.567+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO OptimisticTransaction: [tableId=86213540,txnId=eb657e5c] Committed delta #8 to s3a://lakehouse/merge_data-movies/merged_data/_delta_log
[2024-11-04T11:28:11.760+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
[2024-11-04T11:28:11.766+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO InitialSnapshot: [tableId=4d18a86d-2d13-4d28-b45b-0c871fb904dd] Created snapshot InitialSnapshot(path=s3a://lakehouse/gold/gold_data/_delta_log, version=-1, metadata=Metadata(276d1500-40a9-4600-95f9-b8cebe61454f,null,null,Format(parquet,Map()),null,List(),Map(),Some(1730719691764)), logSegment=LogSegment(s3a://lakehouse/gold/gold_data/_delta_log,-1,List(),None,-1), checksumOpt=None)
[2024-11-04T11:28:11.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DeltaLog: No delta log found for the Delta table at s3a://lakehouse/gold/gold_data/_delta_log
[2024-11-04T11:28:11.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO InitialSnapshot: [tableId=276d1500-40a9-4600-95f9-b8cebe61454f] Created snapshot InitialSnapshot(path=s3a://lakehouse/gold/gold_data/_delta_log, version=-1, metadata=Metadata(60c587c3-8881-4ab5-973d-7555b4f8d2fb,null,null,Format(parquet,Map()),null,List(),Map(),Some(1730719691780)), logSegment=LogSegment(s3a://lakehouse/gold/gold_data/_delta_log,-1,List(),None,-1), checksumOpt=None)
[2024-11-04T11:28:11.786+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO OptimisticTransaction: [tableId=60c587c3,txnId=2952a0f7] Updated metadata from - to Metadata(03dab920-7c80-4f99-a908-a0c39ac874f3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719691783))
[2024-11-04T11:28:11.913+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:11.914+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:11.914+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(keywords),IsNotNull(id)
[2024-11-04T11:28:11.915+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(keywords#1),isnotnull(id#0L)
[2024-11-04T11:28:11.917+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:11.917+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#1566, Some(Etc/UTC)), lambdafunction(lambda x#1578.name, lambda x#1578, false))),isnotnull(id#1568L)
[2024-11-04T11:28:11.930+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-04T11:28:11.943+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:11.948+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 206.5 KiB, free 425.8 MiB)
[2024-11-04T11:28:11.954+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 425.8 MiB)
[2024-11-04T11:28:11.954+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on eb88dbfa1959:36409 (size: 36.6 KiB, free: 428.4 MiB)
[2024-11-04T11:28:11.955+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Created broadcast 52 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:11.956+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:11.963+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Registering RDD 151 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 14
[2024-11-04T11:28:11.964+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Got map stage job 33 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-04T11:28:11.965+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Final stage: ShuffleMapStage 55 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:11.965+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:11.966+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:11.966+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:11.967+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[151] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:11.967+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 31.8 KiB, free 425.8 MiB)
[2024-11-04T11:28:11.968+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 14.1 KiB, free 425.8 MiB)
[2024-11-04T11:28:11.969+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on eb88dbfa1959:36409 (size: 14.1 KiB, free: 428.3 MiB)
[2024-11-04T11:28:11.969+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:11.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[151] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:11.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2024-11-04T11:28:11.971+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 392) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7912 bytes)
[2024-11-04T11:28:11.972+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 206.7 KiB, free 425.6 MiB)
[2024-11-04T11:28:11.977+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 36.7 KiB, free 425.5 MiB)
[2024-11-04T11:28:11.977+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on eb88dbfa1959:36409 (size: 36.7 KiB, free: 428.3 MiB)
[2024-11-04T11:28:11.978+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.18.0.5:35227 (size: 14.1 KiB, free: 1042.8 MiB)
[2024-11-04T11:28:11.978+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Created broadcast 54 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:11.979+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:11.993+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Registering RDD 159 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 15
[2024-11-04T11:28:11.994+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Got map stage job 34 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:11.994+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Final stage: ShuffleMapStage 56 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:11.995+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:11.995+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:11.996+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[159] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:11.996+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.18.0.5:35227 (size: 36.6 KiB, free: 1042.8 MiB)
[2024-11-04T11:28:11.997+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 49.8 KiB, free 425.5 MiB)
[2024-11-04T11:28:11.998+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 425.5 MiB)
[2024-11-04T11:28:11.999+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on eb88dbfa1959:36409 (size: 19.7 KiB, free: 428.3 MiB)
[2024-11-04T11:28:11.999+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:11 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:12.000+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[159] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:12.001+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Adding task set 56.0 with 2 tasks resource profile 0
[2024-11-04T11:28:12.002+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 393) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:28:12.010+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.18.0.5:35227 (size: 19.7 KiB, free: 1042.8 MiB)
[2024-11-04T11:28:12.026+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.18.0.5:35227 (size: 36.7 KiB, free: 1042.8 MiB)
[2024-11-04T11:28:12.198+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 394) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-04T11:28:12.199+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 392) in 228 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:12.200+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2024-11-04T11:28:12.200+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: ShuffleMapStage 55 (save at NativeMethodAccessorImpl.java:0) finished in 0.235 s
[2024-11-04T11:28:12.201+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:12.202+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: running: Set(ShuffleMapStage 56)
[2024-11-04T11:28:12.203+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:12.204+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:12.218+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:12.219+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:12.220+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_53_piece0 on eb88dbfa1959:36409 in memory (size: 14.1 KiB, free: 428.3 MiB)
[2024-11-04T11:28:12.221+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.18.0.5:35227 in memory (size: 14.1 KiB, free: 1042.8 MiB)
[2024-11-04T11:28:12.225+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_51_piece0 on eb88dbfa1959:36409 in memory (size: 108.1 KiB, free: 428.4 MiB)
[2024-11-04T11:28:12.226+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.18.0.5:35227 in memory (size: 108.1 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:12.232+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1480508, minimum partition size: 1048576
[2024-11-04T11:28:12.253+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:12.267+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 394) in 69 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:12.272+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO CodeGenerator: Code generated in 16.127252 ms
[2024-11-04T11:28:12.286+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:12.287+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Got job 35 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2024-11-04T11:28:12.288+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Final stage: ResultStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-11-04T11:28:12.289+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)
[2024-11-04T11:28:12.290+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:12.290+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[162] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-11-04T11:28:12.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 34.6 KiB, free 426.0 MiB)
[2024-11-04T11:28:12.300+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 426.0 MiB)
[2024-11-04T11:28:12.301+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on eb88dbfa1959:36409 (size: 16.1 KiB, free: 428.4 MiB)
[2024-11-04T11:28:12.302+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:12.302+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 58 (MapPartitionsRDD[162] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:12.303+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Adding task set 58.0 with 2 tasks resource profile 0
[2024-11-04T11:28:12.304+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 395) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:12.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.18.0.5:35227 (size: 16.1 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:12.317+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 172.18.0.5:39048
[2024-11-04T11:28:12.356+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_36_piece0 on eb88dbfa1959:36409 in memory (size: 36.7 KiB, free: 428.4 MiB)
[2024-11-04T11:28:12.357+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.18.0.5:35227 in memory (size: 36.7 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:12.360+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_34_piece0 on eb88dbfa1959:36409 in memory (size: 36.6 KiB, free: 428.5 MiB)
[2024-11-04T11:28:12.363+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.18.0.5:35227 in memory (size: 36.6 KiB, free: 1042.9 MiB)
[2024-11-04T11:28:12.368+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManager: Removing RDD 88
[2024-11-04T11:28:12.374+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_29_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 428.5 MiB)
[2024-11-04T11:28:12.380+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_40_piece0 on eb88dbfa1959:36409 in memory (size: 37.2 KiB, free: 428.5 MiB)
[2024-11-04T11:28:12.381+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Starting task 1.0 in stage 58.0 (TID 396) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:12.382+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 395) in 77 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:12.382+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.18.0.5:35227 in memory (size: 37.2 KiB, free: 1043.0 MiB)
[2024-11-04T11:28:12.392+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_39_piece0 on eb88dbfa1959:36409 in memory (size: 4.0 MiB, free: 432.5 MiB)
[2024-11-04T11:28:12.394+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_39_piece1 on eb88dbfa1959:36409 in memory (size: 1563.5 KiB, free: 434.1 MiB)
[2024-11-04T11:28:12.395+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.18.0.5:35227 in memory (size: 4.0 MiB, free: 1047.0 MiB)
[2024-11-04T11:28:12.397+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_39_piece1 on 172.18.0.5:35227 in memory (size: 1563.5 KiB, free: 1048.5 MiB)
[2024-11-04T11:28:12.400+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_50_piece0 on eb88dbfa1959:36409 in memory (size: 123.2 KiB, free: 434.2 MiB)
[2024-11-04T11:28:12.404+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.18.0.5:35227 in memory (size: 123.2 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:12.407+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_30_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2024-11-04T11:28:12.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:12.431+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Finished task 1.0 in stage 58.0 (TID 396) in 51 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:12.432+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2024-11-04T11:28:12.432+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: ResultStage 58 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.141 s
[2024-11-04T11:28:12.433+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:12.433+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
[2024-11-04T11:28:12.434+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Job 35 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.145459 s
[2024-11-04T11:28:12.462+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 814.1 KiB, free 432.5 MiB)
[2024-11-04T11:28:12.463+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on eb88dbfa1959:36409 (size: 814.1 KiB, free: 433.4 MiB)
[2024-11-04T11:28:12.464+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO SparkContext: Created broadcast 57 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:12.472+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)
[2024-11-04T11:28:12.473+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO FileSourceStrategy: Post-Scan Filters: NOT cast(cast(budget#776 as int) as string) IN (/zV8bHuSL6WXoD6FWogP9j4x80bL.jpg,/ff9qCepilowshEtG2GYWwzt2bs4.jpg,/zaSf5OG7V8X8gqFvly88zDdRm46.jpg),isnotnull(id#779)
[2024-11-04T11:28:12.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO CodeGenerator: Code generated in 9.634152 ms
[2024-11-04T11:28:12.537+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO CodeGenerator: Code generated in 21.761583 ms
[2024-11-04T11:28:12.540+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 207.9 KiB, free 432.3 MiB)
[2024-11-04T11:28:12.547+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 432.3 MiB)
[2024-11-04T11:28:12.548+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on eb88dbfa1959:36409 (size: 36.9 KiB, free: 433.4 MiB)
[2024-11-04T11:28:12.549+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO SparkContext: Created broadcast 58 from save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:12.550+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10691623 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:12.559+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Registering RDD 168 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 16
[2024-11-04T11:28:12.560+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Got map stage job 36 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:12.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Final stage: ShuffleMapStage 59 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:12.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:12.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:12.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Submitting ShuffleMapStage 59 (MapPartitionsRDD[168] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:12.586+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 53.1 KiB, free 432.3 MiB)
[2024-11-04T11:28:12.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 21.3 KiB, free 432.2 MiB)
[2024-11-04T11:28:12.588+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on eb88dbfa1959:36409 (size: 21.3 KiB, free: 433.4 MiB)
[2024-11-04T11:28:12.589+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:12.589+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 59 (MapPartitionsRDD[168] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:12.590+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSchedulerImpl: Adding task set 59.0 with 2 tasks resource profile 0
[2024-11-04T11:28:12.590+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 397) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7910 bytes)
[2024-11-04T11:28:12.598+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.18.0.5:35227 (size: 21.3 KiB, free: 1048.6 MiB)
[2024-11-04T11:28:12.656+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.18.0.5:35227 (size: 814.1 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:12.668+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:12 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.18.0.5:35227 (size: 36.9 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:13.500+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 398) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7910 bytes)
[2024-11-04T11:28:13.501+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 397) in 911 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:13.541+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 398) in 41 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:13.542+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool
[2024-11-04T11:28:13.543+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO DAGScheduler: ShuffleMapStage 59 (save at NativeMethodAccessorImpl.java:0) finished in 0.980 s
[2024-11-04T11:28:13.543+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:13.544+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO DAGScheduler: running: Set(ShuffleMapStage 56)
[2024-11-04T11:28:13.544+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:13.545+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:13 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:14.048+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 393) in 2047 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:14.049+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2024-11-04T11:28:14.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: ShuffleMapStage 56 (save at NativeMethodAccessorImpl.java:0) finished in 2.055 s
[2024-11-04T11:28:14.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:14.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:14.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:14.052+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:14.055+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO ShufflePartitionsUtil: For shuffle(15), advisory target size: 67108864, actual target size 5348362, minimum partition size: 1048576
[2024-11-04T11:28:14.063+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-04T11:28:14.076+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO CodeGenerator: Code generated in 10.594575 ms
[2024-11-04T11:28:14.087+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Registering RDD 171 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 17
[2024-11-04T11:28:14.087+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Got map stage job 37 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:14.088+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Final stage: ShuffleMapStage 61 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:14.088+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 60)
[2024-11-04T11:28:14.089+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:14.089+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting ShuffleMapStage 61 (MapPartitionsRDD[171] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:14.091+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 49.0 KiB, free 432.2 MiB)
[2024-11-04T11:28:14.092+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 20.3 KiB, free 432.2 MiB)
[2024-11-04T11:28:14.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on eb88dbfa1959:36409 (size: 20.3 KiB, free: 433.4 MiB)
[2024-11-04T11:28:14.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:14.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[171] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:14.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Adding task set 61.0 with 2 tasks resource profile 0
[2024-11-04T11:28:14.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 399) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:14.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 1.0 in stage 61.0 (TID 400) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:14.102+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.18.0.5:35227 (size: 20.3 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:14.108+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.18.0.5:39048
[2024-11-04T11:28:14.241+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 399) in 147 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:14.258+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 1.0 in stage 61.0 (TID 400) in 163 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:14.258+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2024-11-04T11:28:14.259+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: ShuffleMapStage 61 (save at NativeMethodAccessorImpl.java:0) finished in 0.169 s
[2024-11-04T11:28:14.260+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:14.261+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:14.261+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:14.262+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:14.272+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO ShufflePartitionsUtil: For shuffle(17), advisory target size: 67108864, actual target size 1239972, minimum partition size: 1048576
[2024-11-04T11:28:14.302+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:14.304+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Got job 38 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions
[2024-11-04T11:28:14.305+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Final stage: ResultStage 64 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)
[2024-11-04T11:28:14.306+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 63)
[2024-11-04T11:28:14.306+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:14.307+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[173] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents
[2024-11-04T11:28:14.309+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 8.3 KiB, free 432.2 MiB)
[2024-11-04T11:28:14.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 432.2 MiB)
[2024-11-04T11:28:14.312+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on eb88dbfa1959:36409 (size: 4.2 KiB, free: 433.3 MiB)
[2024-11-04T11:28:14.313+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:14.314+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 64 (MapPartitionsRDD[173] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:14.315+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Adding task set 64.0 with 2 tasks resource profile 0
[2024-11-04T11:28:14.316+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 401) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7386 bytes)
[2024-11-04T11:28:14.317+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 1.0 in stage 64.0 (TID 402) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7386 bytes)
[2024-11-04T11:28:14.329+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.18.0.5:35227 (size: 4.2 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:14.335+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 172.18.0.5:39048
[2024-11-04T11:28:14.379+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added taskresult_401 in memory on 172.18.0.5:35227 (size: 1122.9 KiB, free: 1046.7 MiB)
[2024-11-04T11:28:14.380+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added taskresult_402 in memory on 172.18.0.5:35227 (size: 1137.1 KiB, free: 1045.6 MiB)
[2024-11-04T11:28:14.405+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 1.0 in stage 64.0 (TID 402) in 88 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:14.407+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_59_piece0 on eb88dbfa1959:36409 in memory (size: 21.3 KiB, free: 433.4 MiB)
[2024-11-04T11:28:14.409+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 401) in 93 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:14.410+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2024-11-04T11:28:14.411+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed taskresult_402 on 172.18.0.5:35227 in memory (size: 1137.1 KiB, free: 1046.7 MiB)
[2024-11-04T11:28:14.412+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: ResultStage 64 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.101 s
[2024-11-04T11:28:14.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.18.0.5:35227 in memory (size: 21.3 KiB, free: 1046.7 MiB)
[2024-11-04T11:28:14.413+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:14.414+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2024-11-04T11:28:14.415+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Job 38 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.107716 s
[2024-11-04T11:28:14.427+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed taskresult_401 on 172.18.0.5:35227 in memory (size: 1122.9 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:14.429+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_60_piece0 on eb88dbfa1959:36409 in memory (size: 20.3 KiB, free: 433.4 MiB)
[2024-11-04T11:28:14.432+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.18.0.5:35227 in memory (size: 20.3 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:14.438+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_56_piece0 on eb88dbfa1959:36409 in memory (size: 16.1 KiB, free: 433.4 MiB)
[2024-11-04T11:28:14.443+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.18.0.5:35227 in memory (size: 16.1 KiB, free: 1047.8 MiB)
[2024-11-04T11:28:14.457+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.4 MiB, free 430.0 MiB)
[2024-11-04T11:28:14.457+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on eb88dbfa1959:36409 (size: 2.4 MiB, free: 431.0 MiB)
[2024-11-04T11:28:14.458+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Created broadcast 62 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264
[2024-11-04T11:28:14.462+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO ShufflePartitionsUtil: For shuffle(16), advisory target size: 67108864, actual target size 8080765, minimum partition size: 1048576
[2024-11-04T11:28:14.487+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TorrentBroadcast: Started reading broadcast variable 62 with 1 pieces (estimated total size 4.0 MiB)
[2024-11-04T11:28:14.490+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TorrentBroadcast: Reading broadcast variable 62 took 2 ms
[2024-11-04T11:28:14.524+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO CodeGenerator: Code generated in 13.259013 ms
[2024-11-04T11:28:14.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-04T11:28:14.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Got job 39 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-04T11:28:14.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Final stage: ResultStage 66 (save at NativeMethodAccessorImpl.java:0)
[2024-11-04T11:28:14.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 65)
[2024-11-04T11:28:14.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:14.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[175] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-04T11:28:14.579+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 364.4 KiB, free 429.6 MiB)
[2024-11-04T11:28:14.581+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 129.8 KiB, free 429.5 MiB)
[2024-11-04T11:28:14.582+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on eb88dbfa1959:36409 (size: 129.8 KiB, free: 430.9 MiB)
[2024-11-04T11:28:14.582+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:14.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 66 (MapPartitionsRDD[175] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-04T11:28:14.588+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSchedulerImpl: Adding task set 66.0 with 2 tasks resource profile 0
[2024-11-04T11:28:14.589+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 403) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7386 bytes)
[2024-11-04T11:28:14.590+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Starting task 1.0 in stage 66.0 (TID 404) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7386 bytes)
[2024-11-04T11:28:14.599+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.18.0.5:35227 (size: 129.8 KiB, free: 1047.7 MiB)
[2024-11-04T11:28:14.613+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 172.18.0.5:39048
[2024-11-04T11:28:14.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.18.0.5:35227 (size: 2.4 MiB, free: 1045.3 MiB)
[2024-11-04T11:28:14.720+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:14 INFO TaskSetManager: Finished task 1.0 in stage 66.0 (TID 404) in 131 ms on 172.18.0.5 (executor 0) (1/2)
[2024-11-04T11:28:15.261+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 403) in 673 ms on 172.18.0.5 (executor 0) (2/2)
[2024-11-04T11:28:15.262+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool
[2024-11-04T11:28:15.263+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: ResultStage 66 (save at NativeMethodAccessorImpl.java:0) finished in 0.700 s
[2024-11-04T11:28:15.263+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:15.264+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
[2024-11-04T11:28:15.265+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Job 39 finished: save at NativeMethodAccessorImpl.java:0, took 0.701719 s
[2024-11-04T11:28:15.265+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileFormatWriter: Start to commit write Job 5b44d2af-25a3-471f-853f-19d9733b9838.
[2024-11-04T11:28:15.266+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileFormatWriter: Write Job 5b44d2af-25a3-471f-853f-19d9733b9838 committed. Elapsed time: 0 ms.
[2024-11-04T11:28:15.267+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileFormatWriter: Finished processing stats for write job 5b44d2af-25a3-471f-853f-19d9733b9838.
[2024-11-04T11:28:15.382+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_63_piece0 on eb88dbfa1959:36409 in memory (size: 129.8 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.385+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 172.18.0.5:35227 in memory (size: 129.8 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.419+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:15.420+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Job 40 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.000255 s
[2024-11-04T11:28:15.429+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO OptimisticTransaction: [tableId=60c587c3,txnId=2952a0f7] Attempting to commit version 0 with 4 actions with Serializable isolation level
[2024-11-04T11:28:15.500+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DeltaLog: Creating a new snapshot v0 for commit version 0
[2024-11-04T11:28:15.500+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DeltaLog: Loading version 0.
[2024-11-04T11:28:15.505+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 3003)
[2024-11-04T11:28:15.535+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:15.536+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:15.548+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 205.0 KiB, free 429.8 MiB)
[2024-11-04T11:28:15.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 429.7 MiB)
[2024-11-04T11:28:15.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.562+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Created broadcast 64 from toString at String.java:2951
[2024-11-04T11:28:15.563+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:15.578+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-04T11:28:15.579+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Got job 41 (toString at String.java:2951) with 1 output partitions
[2024-11-04T11:28:15.580+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Final stage: ResultStage 67 (toString at String.java:2951)
[2024-11-04T11:28:15.580+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:15.581+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:15.582+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[183] at toString at String.java:2951), which has no missing parents
[2024-11-04T11:28:15.583+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 50.4 KiB, free 429.7 MiB)
[2024-11-04T11:28:15.584+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 429.7 MiB)
[2024-11-04T11:28:15.585+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on eb88dbfa1959:36409 (size: 15.5 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.586+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:15.586+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[183] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:15.587+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
[2024-11-04T11:28:15.588+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 405) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7951 bytes)
[2024-11-04T11:28:15.612+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.18.0.5:35227 (size: 15.5 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.613+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_55_piece0 on eb88dbfa1959:36409 in memory (size: 19.7 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.614+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.18.0.5:35227 in memory (size: 19.7 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.618+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_61_piece0 on eb88dbfa1959:36409 in memory (size: 4.2 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.620+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.18.0.5:35227 in memory (size: 4.2 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.627+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1045.4 MiB)
[2024-11-04T11:28:15.644+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 405) in 57 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:15.645+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2024-11-04T11:28:15.646+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: ResultStage 67 (toString at String.java:2951) finished in 0.065 s
[2024-11-04T11:28:15.646+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:15.647+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
[2024-11-04T11:28:15.647+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Job 41 finished: toString at String.java:2951, took 0.067549 s
[2024-11-04T11:28:15.650+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO Snapshot: [tableId=60c587c3-8881-4ab5-973d-7555b4f8d2fb] Created snapshot Snapshot(path=s3a://lakehouse/gold/gold_data/_delta_log, version=0, metadata=Metadata(03dab920-7c80-4f99-a908-a0c39ac874f3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719691783)), logSegment=LogSegment(s3a://lakehouse/gold/gold_data/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://lakehouse/gold/gold_data/_delta_log/00000000000000000000.json; isDirectory=false; length=3003; replication=1; blocksize=33554432; modification_time=1730719695461; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=45782ec929353612f2058a29915e12f5 versionId=null),None,1730719695461), checksumOpt=None)
[2024-11-04T11:28:15.651+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/gold/gold_data/_delta_log, version=0, metadata=Metadata(03dab920-7c80-4f99-a908-a0c39ac874f3,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"imdb_id","type":"string","nullable":true,"metadata":{}},{"name":"budget","type":"integer","nullable":true,"metadata":{}},{"name":"genres_convert","type":"string","nullable":true,"metadata":{}},{"name":"original_language","type":"string","nullable":true,"metadata":{}},{"name":"release_date","type":"date","nullable":true,"metadata":{}},{"name":"revenue","type":"string","nullable":true,"metadata":{}},{"name":"title","type":"string","nullable":true,"metadata":{}},{"name":"time","type":"string","nullable":true,"metadata":{}},{"name":"overview","type":"string","nullable":true,"metadata":{}},{"name":"vote_average","type":"float","nullable":true,"metadata":{}},{"name":"vote_count","type":"integer","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730719691783)), logSegment=LogSegment(s3a://lakehouse/gold/gold_data/_delta_log,0,WrappedArray(S3AFileStatus{path=s3a://lakehouse/gold/gold_data/_delta_log/00000000000000000000.json; isDirectory=false; length=3003; replication=1; blocksize=33554432; modification_time=1730719695461; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=45782ec929353612f2058a29915e12f5 versionId=null),None,1730719695461), checksumOpt=None)
[2024-11-04T11:28:15.652+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO Snapshot: [tableId=03dab920-7c80-4f99-a908-a0c39ac874f3] DELTA: Compute snapshot for version: 0
[2024-11-04T11:28:15.653+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 204.7 KiB, free 429.6 MiB)
[2024-11-04T11:28:15.658+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 429.5 MiB)
[2024-11-04T11:28:15.659+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.660+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Created broadcast 66 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:15.790+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceStrategy: Pushed Filters:
[2024-11-04T11:28:15.791+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-04T11:28:15.816+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_64_piece0 on eb88dbfa1959:36409 in memory (size: 36.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.818+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 172.18.0.5:35227 in memory (size: 36.0 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.821+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_65_piece0 on eb88dbfa1959:36409 in memory (size: 15.5 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.823+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 172.18.0.5:35227 in memory (size: 15.5 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.835+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 205.0 KiB, free 429.6 MiB)
[2024-11-04T11:28:15.841+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 429.6 MiB)
[2024-11-04T11:28:15.842+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on eb88dbfa1959:36409 (size: 36.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.842+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Created broadcast 67 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:15.843+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-04T11:28:15.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Registering RDD 187 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 18
[2024-11-04T11:28:15.851+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Got map stage job 42 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:28:15.852+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Final stage: ShuffleMapStage 68 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:15.853+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Parents of final stage: List()
[2024-11-04T11:28:15.854+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:15.855+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Submitting ShuffleMapStage 68 (MapPartitionsRDD[187] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:15.856+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 138.3 KiB, free 429.5 MiB)
[2024-11-04T11:28:15.857+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 429.4 MiB)
[2024-11-04T11:28:15.858+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on eb88dbfa1959:36409 (size: 39.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.859+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:15.859+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 68 (MapPartitionsRDD[187] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:15.860+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0
[2024-11-04T11:28:15.861+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 406) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7940 bytes)
[2024-11-04T11:28:15.870+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 172.18.0.5:35227 (size: 39.0 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:15.891+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 172.18.0.5:35227 (size: 36.0 KiB, free: 1045.4 MiB)
[2024-11-04T11:28:15.917+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 406) in 57 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:15.918+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2024-11-04T11:28:15.918+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: ShuffleMapStage 68 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.065 s
[2024-11-04T11:28:15.919+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:15.919+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:15.920+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:15.920+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:15.975+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_68_piece0 on eb88dbfa1959:36409 in memory (size: 39.0 KiB, free: 431.0 MiB)
[2024-11-04T11:28:15.977+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:15 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 172.18.0.5:35227 in memory (size: 39.0 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:16.078+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Registering RDD 197 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 19
[2024-11-04T11:28:16.078+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Got map stage job 43 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-04T11:28:16.079+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Final stage: ShuffleMapStage 70 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:16.080+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 69)
[2024-11-04T11:28:16.080+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:16.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Submitting ShuffleMapStage 70 (MapPartitionsRDD[197] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:16.091+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 517.0 KiB, free 429.1 MiB)
[2024-11-04T11:28:16.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 429.0 MiB)
[2024-11-04T11:28:16.093+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on eb88dbfa1959:36409 (size: 123.3 KiB, free: 430.9 MiB)
[2024-11-04T11:28:16.094+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:16.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 70 (MapPartitionsRDD[197] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-04T11:28:16.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSchedulerImpl: Adding task set 70.0 with 50 tasks resource profile 0
[2024-11-04T11:28:16.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 2.0 in stage 70.0 (TID 407) (172.18.0.5, executor 0, partition 2, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 42.0 in stage 70.0 (TID 408) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.103+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 172.18.0.5:35227 (size: 123.3 KiB, free: 1045.3 MiB)
[2024-11-04T11:28:16.113+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 172.18.0.5:39048
[2024-11-04T11:28:16.135+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_2 in memory on 172.18.0.5:35227 (size: 873.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.135+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_42 in memory on 172.18.0.5:35227 (size: 692.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.150+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 409) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.151+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 42.0 in stage 70.0 (TID 408) in 54 ms on 172.18.0.5 (executor 0) (1/50)
[2024-11-04T11:28:16.152+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 1.0 in stage 70.0 (TID 410) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.152+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 2.0 in stage 70.0 (TID 407) in 56 ms on 172.18.0.5 (executor 0) (2/50)
[2024-11-04T11:28:16.177+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_0 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.178+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_1 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.190+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 3.0 in stage 70.0 (TID 411) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.191+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 409) in 41 ms on 172.18.0.5 (executor 0) (3/50)
[2024-11-04T11:28:16.192+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 4.0 in stage 70.0 (TID 412) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.192+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 1.0 in stage 70.0 (TID 410) in 41 ms on 172.18.0.5 (executor 0) (4/50)
[2024-11-04T11:28:16.217+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_3 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.219+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_4 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.231+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 5.0 in stage 70.0 (TID 413) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.232+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 3.0 in stage 70.0 (TID 411) in 41 ms on 172.18.0.5 (executor 0) (5/50)
[2024-11-04T11:28:16.233+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 6.0 in stage 70.0 (TID 414) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.234+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 4.0 in stage 70.0 (TID 412) in 41 ms on 172.18.0.5 (executor 0) (6/50)
[2024-11-04T11:28:16.259+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_5 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.260+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_6 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.273+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 7.0 in stage 70.0 (TID 415) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.274+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 5.0 in stage 70.0 (TID 413) in 42 ms on 172.18.0.5 (executor 0) (7/50)
[2024-11-04T11:28:16.274+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 8.0 in stage 70.0 (TID 416) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.275+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 6.0 in stage 70.0 (TID 414) in 43 ms on 172.18.0.5 (executor 0) (8/50)
[2024-11-04T11:28:16.300+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_8 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.301+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_7 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.314+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 9.0 in stage 70.0 (TID 417) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.315+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 7.0 in stage 70.0 (TID 415) in 42 ms on 172.18.0.5 (executor 0) (9/50)
[2024-11-04T11:28:16.316+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 10.0 in stage 70.0 (TID 418) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.317+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 8.0 in stage 70.0 (TID 416) in 42 ms on 172.18.0.5 (executor 0) (10/50)
[2024-11-04T11:28:16.343+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_9 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.344+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_10 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.357+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 11.0 in stage 70.0 (TID 419) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.358+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 9.0 in stage 70.0 (TID 417) in 43 ms on 172.18.0.5 (executor 0) (11/50)
[2024-11-04T11:28:16.358+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 12.0 in stage 70.0 (TID 420) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.359+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 10.0 in stage 70.0 (TID 418) in 43 ms on 172.18.0.5 (executor 0) (12/50)
[2024-11-04T11:28:16.395+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_11 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.397+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_12 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.426+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 13.0 in stage 70.0 (TID 421) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.427+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 12.0 in stage 70.0 (TID 420) in 68 ms on 172.18.0.5 (executor 0) (13/50)
[2024-11-04T11:28:16.429+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 14.0 in stage 70.0 (TID 422) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.430+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 11.0 in stage 70.0 (TID 419) in 71 ms on 172.18.0.5 (executor 0) (14/50)
[2024-11-04T11:28:16.465+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_13 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.465+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_14 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.480+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 15.0 in stage 70.0 (TID 423) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 14.0 in stage 70.0 (TID 422) in 54 ms on 172.18.0.5 (executor 0) (15/50)
[2024-11-04T11:28:16.481+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 16.0 in stage 70.0 (TID 424) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.482+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 13.0 in stage 70.0 (TID 421) in 56 ms on 172.18.0.5 (executor 0) (16/50)
[2024-11-04T11:28:16.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_16 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.508+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_15 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.531+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 17.0 in stage 70.0 (TID 425) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.531+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 15.0 in stage 70.0 (TID 423) in 51 ms on 172.18.0.5 (executor 0) (17/50)
[2024-11-04T11:28:16.532+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 18.0 in stage 70.0 (TID 426) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.533+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 16.0 in stage 70.0 (TID 424) in 51 ms on 172.18.0.5 (executor 0) (18/50)
[2024-11-04T11:28:16.561+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_17 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.564+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_18 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.582+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 19.0 in stage 70.0 (TID 427) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.583+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 20.0 in stage 70.0 (TID 428) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.584+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 18.0 in stage 70.0 (TID 426) in 52 ms on 172.18.0.5 (executor 0) (19/50)
[2024-11-04T11:28:16.585+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 17.0 in stage 70.0 (TID 425) in 54 ms on 172.18.0.5 (executor 0) (20/50)
[2024-11-04T11:28:16.612+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_20 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.617+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_19 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.630+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 21.0 in stage 70.0 (TID 429) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.631+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 20.0 in stage 70.0 (TID 428) in 47 ms on 172.18.0.5 (executor 0) (21/50)
[2024-11-04T11:28:16.634+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 22.0 in stage 70.0 (TID 430) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.635+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 19.0 in stage 70.0 (TID 427) in 52 ms on 172.18.0.5 (executor 0) (22/50)
[2024-11-04T11:28:16.673+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_21 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.674+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_22 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.690+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 23.0 in stage 70.0 (TID 431) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.692+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 21.0 in stage 70.0 (TID 429) in 62 ms on 172.18.0.5 (executor 0) (23/50)
[2024-11-04T11:28:16.698+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 24.0 in stage 70.0 (TID 432) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.699+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 22.0 in stage 70.0 (TID 430) in 65 ms on 172.18.0.5 (executor 0) (24/50)
[2024-11-04T11:28:16.723+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_23 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.728+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_24 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.747+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 25.0 in stage 70.0 (TID 433) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.748+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 23.0 in stage 70.0 (TID 431) in 57 ms on 172.18.0.5 (executor 0) (25/50)
[2024-11-04T11:28:16.748+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 26.0 in stage 70.0 (TID 434) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.749+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 24.0 in stage 70.0 (TID 432) in 51 ms on 172.18.0.5 (executor 0) (26/50)
[2024-11-04T11:28:16.779+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_26 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.780+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_25 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 27.0 in stage 70.0 (TID 435) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.806+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 25.0 in stage 70.0 (TID 433) in 59 ms on 172.18.0.5 (executor 0) (27/50)
[2024-11-04T11:28:16.807+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 28.0 in stage 70.0 (TID 436) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.808+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 26.0 in stage 70.0 (TID 434) in 59 ms on 172.18.0.5 (executor 0) (28/50)
[2024-11-04T11:28:16.831+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_27 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.833+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_28 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.844+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 29.0 in stage 70.0 (TID 437) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.844+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 27.0 in stage 70.0 (TID 435) in 39 ms on 172.18.0.5 (executor 0) (29/50)
[2024-11-04T11:28:16.848+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 30.0 in stage 70.0 (TID 438) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.849+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 28.0 in stage 70.0 (TID 436) in 42 ms on 172.18.0.5 (executor 0) (30/50)
[2024-11-04T11:28:16.870+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_29 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.873+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_30 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.883+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 31.0 in stage 70.0 (TID 439) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.884+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 29.0 in stage 70.0 (TID 437) in 40 ms on 172.18.0.5 (executor 0) (31/50)
[2024-11-04T11:28:16.886+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 32.0 in stage 70.0 (TID 440) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.886+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 30.0 in stage 70.0 (TID 438) in 38 ms on 172.18.0.5 (executor 0) (32/50)
[2024-11-04T11:28:16.910+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_31 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.912+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_32 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.923+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 33.0 in stage 70.0 (TID 441) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.923+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 31.0 in stage 70.0 (TID 439) in 40 ms on 172.18.0.5 (executor 0) (33/50)
[2024-11-04T11:28:16.924+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 34.0 in stage 70.0 (TID 442) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.925+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 32.0 in stage 70.0 (TID 440) in 39 ms on 172.18.0.5 (executor 0) (34/50)
[2024-11-04T11:28:16.951+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_33 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.952+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_34 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.967+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 35.0 in stage 70.0 (TID 443) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.968+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 34.0 in stage 70.0 (TID 442) in 43 ms on 172.18.0.5 (executor 0) (35/50)
[2024-11-04T11:28:16.969+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Starting task 36.0 in stage 70.0 (TID 444) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:16.970+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO TaskSetManager: Finished task 33.0 in stage 70.0 (TID 441) in 47 ms on 172.18.0.5 (executor 0) (36/50)
[2024-11-04T11:28:16.993+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_35 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:16.994+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:16 INFO BlockManagerInfo: Added rdd_194_36 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.007+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 37.0 in stage 70.0 (TID 445) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.008+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 35.0 in stage 70.0 (TID 443) in 40 ms on 172.18.0.5 (executor 0) (37/50)
[2024-11-04T11:28:17.008+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 38.0 in stage 70.0 (TID 446) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.009+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 36.0 in stage 70.0 (TID 444) in 40 ms on 172.18.0.5 (executor 0) (38/50)
[2024-11-04T11:28:17.033+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_37 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.034+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_38 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.048+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 39.0 in stage 70.0 (TID 447) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.049+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 37.0 in stage 70.0 (TID 445) in 42 ms on 172.18.0.5 (executor 0) (39/50)
[2024-11-04T11:28:17.050+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 40.0 in stage 70.0 (TID 448) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.051+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 38.0 in stage 70.0 (TID 446) in 43 ms on 172.18.0.5 (executor 0) (40/50)
[2024-11-04T11:28:17.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_39 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.081+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_40 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.095+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 41.0 in stage 70.0 (TID 449) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.096+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 39.0 in stage 70.0 (TID 447) in 47 ms on 172.18.0.5 (executor 0) (41/50)
[2024-11-04T11:28:17.101+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 43.0 in stage 70.0 (TID 450) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.102+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 40.0 in stage 70.0 (TID 448) in 52 ms on 172.18.0.5 (executor 0) (42/50)
[2024-11-04T11:28:17.131+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_41 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.138+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_43 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.149+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 44.0 in stage 70.0 (TID 451) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.149+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 41.0 in stage 70.0 (TID 449) in 54 ms on 172.18.0.5 (executor 0) (43/50)
[2024-11-04T11:28:17.156+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 45.0 in stage 70.0 (TID 452) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.156+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 43.0 in stage 70.0 (TID 450) in 55 ms on 172.18.0.5 (executor 0) (44/50)
[2024-11-04T11:28:17.182+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_44 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.184+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_45 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.196+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 46.0 in stage 70.0 (TID 453) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.197+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 44.0 in stage 70.0 (TID 451) in 48 ms on 172.18.0.5 (executor 0) (45/50)
[2024-11-04T11:28:17.202+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 47.0 in stage 70.0 (TID 454) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.202+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 45.0 in stage 70.0 (TID 452) in 47 ms on 172.18.0.5 (executor 0) (46/50)
[2024-11-04T11:28:17.231+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_46 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.241+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_47 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.249+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 48.0 in stage 70.0 (TID 455) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.250+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 46.0 in stage 70.0 (TID 453) in 53 ms on 172.18.0.5 (executor 0) (47/50)
[2024-11-04T11:28:17.253+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 49.0 in stage 70.0 (TID 456) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-04T11:28:17.254+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 47.0 in stage 70.0 (TID 454) in 52 ms on 172.18.0.5 (executor 0) (48/50)
[2024-11-04T11:28:17.279+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_48 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.282+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added rdd_194_49 in memory on 172.18.0.5:35227 (size: 46.0 B, free: 1045.3 MiB)
[2024-11-04T11:28:17.292+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 48.0 in stage 70.0 (TID 455) in 43 ms on 172.18.0.5 (executor 0) (49/50)
[2024-11-04T11:28:17.294+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 49.0 in stage 70.0 (TID 456) in 41 ms on 172.18.0.5 (executor 0) (50/50)
[2024-11-04T11:28:17.295+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool
[2024-11-04T11:28:17.295+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: ShuffleMapStage 70 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.214 s
[2024-11-04T11:28:17.296+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: looking for newly runnable stages
[2024-11-04T11:28:17.297+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: running: Set()
[2024-11-04T11:28:17.298+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: waiting: Set()
[2024-11-04T11:28:17.298+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: failed: Set()
[2024-11-04T11:28:17.315+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-04T11:28:17.316+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Got job 44 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-04T11:28:17.317+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Final stage: ResultStage 73 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-04T11:28:17.318+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 72)
[2024-11-04T11:28:17.319+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Missing parents: List()
[2024-11-04T11:28:17.319+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[200] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-04T11:28:17.326+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 452.2 KiB, free 428.5 MiB)
[2024-11-04T11:28:17.327+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Removed broadcast_69_piece0 on eb88dbfa1959:36409 in memory (size: 123.3 KiB, free: 431.0 MiB)
[2024-11-04T11:28:17.328+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 172.18.0.5:35227 in memory (size: 123.3 KiB, free: 1045.5 MiB)
[2024-11-04T11:28:17.330+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 429.0 MiB)
[2024-11-04T11:28:17.331+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on eb88dbfa1959:36409 (size: 108.1 KiB, free: 430.9 MiB)
[2024-11-04T11:28:17.332+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1540
[2024-11-04T11:28:17.332+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[200] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-04T11:28:17.333+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0
[2024-11-04T11:28:17.334+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 457) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-04T11:28:17.342+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 172.18.0.5:35227 (size: 108.1 KiB, free: 1045.4 MiB)
[2024-11-04T11:28:17.351+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 172.18.0.5:39048
[2024-11-04T11:28:17.376+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 457) in 43 ms on 172.18.0.5 (executor 0) (1/1)
[2024-11-04T11:28:17.377+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool
[2024-11-04T11:28:17.377+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: ResultStage 73 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.059 s
[2024-11-04T11:28:17.378+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-04T11:28:17.378+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished
[2024-11-04T11:28:17.379+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO DAGScheduler: Job 44 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.061499 s
[2024-11-04T11:28:17.385+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO Snapshot: [tableId=03dab920-7c80-4f99-a908-a0c39ac874f3] DELTA: Done
[2024-11-04T11:28:17.386+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO OptimisticTransaction: [tableId=60c587c3,txnId=2952a0f7] Committed delta #0 to s3a://lakehouse/gold/gold_data/_delta_log
[2024-11-04T11:28:17.389+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-11-04T11:28:17.407+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO SparkUI: Stopped Spark web UI at http://eb88dbfa1959:4040
[2024-11-04T11:28:17.416+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO StandaloneSchedulerBackend: Shutting down all executors
[2024-11-04T11:28:17.417+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2024-11-04T11:28:17.455+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-11-04T11:28:17.487+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MemoryStore: MemoryStore cleared
[2024-11-04T11:28:17.489+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManager: BlockManager stopped
[2024-11-04T11:28:17.494+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-11-04T11:28:17.500+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-11-04T11:28:17.549+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO SparkContext: Successfully stopped SparkContext
[2024-11-04T11:28:17.883+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO ShutdownHookManager: Shutdown hook called
[2024-11-04T11:28:17.883+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-50c2dd55-31ff-4064-add0-ee5fce999fcf
[2024-11-04T11:28:17.887+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-43698ca0-cbed-4345-8f88-1003670c872a
[2024-11-04T11:28:17.891+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-50c2dd55-31ff-4064-add0-ee5fce999fcf/pyspark-ef78673f-a6f0-4621-9396-01179abbd480
[2024-11-04T11:28:17.902+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2024-11-04T11:28:17.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2024-11-04T11:28:17.903+0000] {spark_submit.py:579} INFO - 24/11/04 11:28:17 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2024-11-04T11:28:18.006+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=mergeeeee, task_id=merge_id, execution_date=20241104T112713, start_date=20241104T112715, end_date=20241104T112818
[2024-11-04T11:28:18.039+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-11-04T11:28:18.052+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
