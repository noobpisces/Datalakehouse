[2024-11-13T06:57:50.631+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: test_flowwww.credits_cleaned scheduled__2024-11-12T00:00:00+00:00 [queued]>
[2024-11-13T06:57:50.639+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: test_flowwww.credits_cleaned scheduled__2024-11-12T00:00:00+00:00 [queued]>
[2024-11-13T06:57:50.640+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2024-11-13T06:57:50.653+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): credits_cleaned> on 2024-11-12 00:00:00+00:00
[2024-11-13T06:57:50.660+0000] {standard_task_runner.py:57} INFO - Started process 633 to run task
[2024-11-13T06:57:50.662+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'test_flowwww', 'credits_cleaned', 'scheduled__2024-11-12T00:00:00+00:00', '--job-id', '240', '--raw', '--subdir', 'DAGS_FOLDER/test_flow.py', '--cfg-path', '/tmp/tmpd6qqctew']
[2024-11-13T06:57:50.665+0000] {standard_task_runner.py:85} INFO - Job 240: Subtask credits_cleaned
[2024-11-13T06:57:50.973+0000] {task_command.py:415} INFO - Running <TaskInstance: test_flowwww.credits_cleaned scheduled__2024-11-12T00:00:00+00:00 [running]> on host bdc4422970d0
[2024-11-13T06:57:51.050+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='test_flowwww' AIRFLOW_CTX_TASK_ID='credits_cleaned' AIRFLOW_CTX_EXECUTION_DATE='2024-11-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-12T00:00:00+00:00'
[2024-11-13T06:57:51.058+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2024-11-13T06:57:51.060+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --jars /opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/s3-2.18.41.jar,/opt/***/jars/aws-java-sdk-1.12.367.jar,/opt/***/jars/delta-core_2.12-2.4.0.jar,/opt/***/jars/delta-storage-2.2.0.jar, --packages org.apache.hadoop:hadoop-aws:3.3.4 --num-executors 2 --total-executor-cores 2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name arrow-spark --deploy-mode client /opt/***/jobs/python/test_Credit.py s3a://lakehouse/bronze/credits.parquet s3a://lakehouse/sliver/credit
[2024-11-13T06:57:51.162+0000] {spark_submit.py:579} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-11-13T06:57:53.458+0000] {spark_submit.py:579} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-11-13T06:57:53.628+0000] {spark_submit.py:579} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-11-13T06:57:53.629+0000] {spark_submit.py:579} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-11-13T06:57:53.635+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-11-13T06:57:53.637+0000] {spark_submit.py:579} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-6059e5db-358c-4be9-b01a-d309db9858b0;1.0
[2024-11-13T06:57:53.637+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-13T06:57:53.823+0000] {spark_submit.py:579} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2024-11-13T06:57:53.859+0000] {spark_submit.py:579} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2024-11-13T06:57:53.885+0000] {spark_submit.py:579} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2024-11-13T06:57:53.911+0000] {spark_submit.py:579} INFO - :: resolution report :: resolve 265ms :: artifacts dl 9ms
[2024-11-13T06:57:53.911+0000] {spark_submit.py:579} INFO - :: modules in use:
[2024-11-13T06:57:53.912+0000] {spark_submit.py:579} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2024-11-13T06:57:53.913+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2024-11-13T06:57:53.913+0000] {spark_submit.py:579} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2024-11-13T06:57:53.914+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-13T06:57:53.914+0000] {spark_submit.py:579} INFO - |                  |            modules            ||   artifacts   |
[2024-11-13T06:57:53.915+0000] {spark_submit.py:579} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-11-13T06:57:53.916+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-13T06:57:53.916+0000] {spark_submit.py:579} INFO - |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2024-11-13T06:57:53.917+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-11-13T06:57:53.918+0000] {spark_submit.py:579} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-6059e5db-358c-4be9-b01a-d309db9858b0
[2024-11-13T06:57:53.918+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-11-13T06:57:53.925+0000] {spark_submit.py:579} INFO - 0 artifacts copied, 3 already retrieved (0kB/7ms)
[2024-11-13T06:57:54.188+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-11-13T06:57:55.598+0000] {spark_submit.py:579} INFO - 2024-11-13 06:57:55,598 INFO: Python version on driver: 3.11.5
[2024-11-13T06:57:55.599+0000] {spark_submit.py:579} INFO - 2024-11-13 06:57:55,598 INFO: Initializing Spark session...
[2024-11-13T06:57:55.761+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SparkContext: Running Spark version 3.4.3
[2024-11-13T06:57:55.795+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceUtils: ==============================================================
[2024-11-13T06:57:55.796+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-11-13T06:57:55.797+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceUtils: ==============================================================
[2024-11-13T06:57:55.798+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SparkContext: Submitted application: CleanCredits
[2024-11-13T06:57:55.829+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-11-13T06:57:55.847+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2024-11-13T06:57:55.851+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-11-13T06:57:55.964+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SecurityManager: Changing view acls to: ***
[2024-11-13T06:57:55.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SecurityManager: Changing modify acls to: ***
[2024-11-13T06:57:55.969+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SecurityManager: Changing view acls groups to:
[2024-11-13T06:57:55.972+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SecurityManager: Changing modify acls groups to:
[2024-11-13T06:57:55.974+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-11-13T06:57:56.344+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO Utils: Successfully started service 'sparkDriver' on port 45725.
[2024-11-13T06:57:56.386+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkEnv: Registering MapOutputTracker
[2024-11-13T06:57:56.424+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkEnv: Registering BlockManagerMaster
[2024-11-13T06:57:56.445+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-11-13T06:57:56.446+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-11-13T06:57:56.451+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-11-13T06:57:56.476+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f9e170c-f5e9-4d25-9458-e7370da4f085
[2024-11-13T06:57:56.495+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-11-13T06:57:56.512+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-11-13T06:57:56.685+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-11-13T06:57:56.762+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-11-13T06:57:56.812+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///opt/***/jars/hadoop-aws-3.3.4.jar at spark://bdc4422970d0:45725/jars/hadoop-aws-3.3.4.jar with timestamp 1731481075751
[2024-11-13T06:57:56.814+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///opt/***/jars/s3-2.18.41.jar at spark://bdc4422970d0:45725/jars/s3-2.18.41.jar with timestamp 1731481075751
[2024-11-13T06:57:56.815+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///opt/***/jars/aws-java-sdk-1.12.367.jar at spark://bdc4422970d0:45725/jars/aws-java-sdk-1.12.367.jar with timestamp 1731481075751
[2024-11-13T06:57:56.816+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///opt/***/jars/delta-core_2.12-2.4.0.jar at spark://bdc4422970d0:45725/jars/delta-core_2.12-2.4.0.jar with timestamp 1731481075751
[2024-11-13T06:57:56.817+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///opt/***/jars/delta-storage-2.2.0.jar at spark://bdc4422970d0:45725/jars/delta-storage-2.2.0.jar with timestamp 1731481075751
[2024-11-13T06:57:56.817+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://bdc4422970d0:45725/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1731481075751
[2024-11-13T06:57:56.818+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://bdc4422970d0:45725/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1731481075751
[2024-11-13T06:57:56.819+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://bdc4422970d0:45725/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1731481075751
[2024-11-13T06:57:56.822+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://bdc4422970d0:45725/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1731481075751
[2024-11-13T06:57:56.824+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-efb67535-0044-4759-84c8-f5931ded17bb/userFiles-c45c4ce9-5bbf-447b-b54a-3905c0065b88/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-11-13T06:57:56.845+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://bdc4422970d0:45725/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1731481075751
[2024-11-13T06:57:56.846+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:56 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-efb67535-0044-4759-84c8-f5931ded17bb/userFiles-c45c4ce9-5bbf-447b-b54a-3905c0065b88/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2024-11-13T06:57:59.156+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://bdc4422970d0:45725/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1731481075751
[2024-11-13T06:57:59.156+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-efb67535-0044-4759-84c8-f5931ded17bb/userFiles-c45c4ce9-5bbf-447b-b54a-3905c0065b88/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-11-13T06:57:59.258+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-11-13T06:57:59.315+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO TransportClientFactory: Successfully created connection to spark-master/172.21.0.2:7077 after 34 ms (0 ms spent in bootstraps)
[2024-11-13T06:57:59.456+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241113065759-0001
[2024-11-13T06:57:59.460+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241113065759-0001/0 on worker-20241113065644-172.21.0.9-38423 (172.21.0.9:38423) with 2 core(s)
[2024-11-13T06:57:59.465+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneSchedulerBackend: Granted executor ID app-20241113065759-0001/0 on hostPort 172.21.0.9:38423 with 2 core(s), 2.0 GiB RAM
[2024-11-13T06:57:59.472+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36285.
[2024-11-13T06:57:59.472+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO NettyBlockTransferService: Server created on bdc4422970d0:36285
[2024-11-13T06:57:59.476+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-11-13T06:57:59.488+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, bdc4422970d0, 36285, None)
[2024-11-13T06:57:59.493+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO BlockManagerMasterEndpoint: Registering block manager bdc4422970d0:36285 with 434.4 MiB RAM, BlockManagerId(driver, bdc4422970d0, 36285, None)
[2024-11-13T06:57:59.497+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, bdc4422970d0, 36285, None)
[2024-11-13T06:57:59.498+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, bdc4422970d0, 36285, None)
[2024-11-13T06:57:59.589+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241113065759-0001/0 is now RUNNING
[2024-11-13T06:57:59.774+0000] {spark_submit.py:579} INFO - 24/11/13 06:57:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-11-13T06:58:00.280+0000] {spark_submit.py:579} INFO - 2024-11-13 06:58:00,279 INFO: Spark session initialized successfully.
[2024-11-13T06:58:00.289+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-11-13T06:58:00.642+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2024-11-13T06:58:00.683+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:00 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2024-11-13T06:58:00.684+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:00 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2024-11-13T06:58:02.526+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:02 INFO SharedState: Warehouse path is 's3a://lakehouse/'.
[2024-11-13T06:58:03.372+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:03 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.21.0.9:58990) with ID 0,  ResourceProfileId 0
[2024-11-13T06:58:03.469+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:03 INFO BlockManagerMasterEndpoint: Registering block manager 172.21.0.9:41811 with 1048.8 MiB RAM, BlockManagerId(0, 172.21.0.9, 41811, None)
[2024-11-13T06:58:26.373+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:26 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(0,WrappedArray(),Map()) by listener AppStatusListener took 5.238351951s.
[2024-11-13T06:58:37.537+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:37 INFO InMemoryFileIndex: It took 73 ms to list leaf files for 1 paths.
[2024-11-13T06:58:37.994+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:37 INFO SparkContext: Starting job: load at NativeMethodAccessorImpl.java:0
[2024-11-13T06:58:38.012+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Got job 0 (load at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-13T06:58:38.012+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Final stage: ResultStage 0 (load at NativeMethodAccessorImpl.java:0)
[2024-11-13T06:58:38.013+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:58:38.014+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:38.019+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-13T06:58:38.071+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 106.9 KiB, free 434.3 MiB)
[2024-11-13T06:58:38.112+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.9 KiB, free 434.3 MiB)
[2024-11-13T06:58:38.116+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on bdc4422970d0:36285 (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-13T06:58:38.121+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:38.139+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at load at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-13T06:58:38.140+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-11-13T06:58:38.178+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7500 bytes)
[2024-11-13T06:58:38.433+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.21.0.9:41811 (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:40.334+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2166 ms on 172.21.0.9 (executor 0) (1/1)
[2024-11-13T06:58:40.336+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-11-13T06:58:40.343+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO DAGScheduler: ResultStage 0 (load at NativeMethodAccessorImpl.java:0) finished in 2.310 s
[2024-11-13T06:58:40.347+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:58:40.347+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-11-13T06:58:40.350+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO DAGScheduler: Job 0 finished: load at NativeMethodAccessorImpl.java:0, took 2.355073 s
[2024-11-13T06:58:40.544+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on bdc4422970d0:36285 in memory (size: 38.9 KiB, free: 434.4 MiB)
[2024-11-13T06:58:40.546+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:40 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.21.0.9:41811 in memory (size: 38.9 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:43.006+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:58:43.008+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-13T06:58:43.506+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO CodeGenerator: Code generated in 281.691342 ms
[2024-11-13T06:58:43.538+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO CodeGenerator: Code generated in 21.002437 ms
[2024-11-13T06:58:43.567+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 206.5 KiB, free 434.2 MiB)
[2024-11-13T06:58:43.581+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 434.2 MiB)
[2024-11-13T06:58:43.583+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on bdc4422970d0:36285 (size: 36.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:43.584+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO SparkContext: Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0
[2024-11-13T06:58:43.606+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:58:43.658+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2024-11-13T06:58:43.660+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2024-11-13T06:58:43.660+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2024-11-13T06:58:43.661+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:58:43.663+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:43.665+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-13T06:58:43.729+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 28.8 KiB, free 434.1 MiB)
[2024-11-13T06:58:43.736+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 12.6 KiB, free 434.1 MiB)
[2024-11-13T06:58:43.738+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on bdc4422970d0:36285 (size: 12.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:43.739+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:43.740+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-11-13T06:58:43.741+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-11-13T06:58:43.746+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7922 bytes)
[2024-11-13T06:58:43.795+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.21.0.9:41811 (size: 12.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:44.511+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.21.0.9:41811 (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:45.223+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1481 ms on 172.21.0.9 (executor 0) (1/1)
[2024-11-13T06:58:45.224+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-11-13T06:58:45.225+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.556 s
[2024-11-13T06:58:45.225+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:58:45.226+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2024-11-13T06:58:45.227+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.567291 s
[2024-11-13T06:58:45.258+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO CodeGenerator: Code generated in 14.763194 ms
[2024-11-13T06:58:45.320+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-13T06:58:45.321+0000] {spark_submit.py:579} INFO - |id   |cast_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |director                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
[2024-11-13T06:58:45.321+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-13T06:58:45.322+0000] {spark_submit.py:579} INFO - |862  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.322+0000] {spark_submit.py:579} INFO - |8844 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.322+0000] {spark_submit.py:579} INFO - |15602|[Walter Matthau, Jack Lemmon, Ann-Margret, Sophia Loren, Daryl Hannah, Burgess Meredith, Kevin Pollak]                                                                                                                                                                                                                                                                                                                                                                                                            |[Walter Matthau, Jack Lemmon, Ann-Margret, Sophia Loren, Daryl Hannah, Burgess Meredith, Kevin Pollak]                                                                                                                                                                                                                                                                                                                                                                                                            |
[2024-11-13T06:58:45.323+0000] {spark_submit.py:579} INFO - |31357|[Whitney Houston, Angela Bassett, Loretta Devine, Lela Rochon, Gregory Hines, Dennis Haysbert, Michael Beach, Mykelti Williamson, Lamont Johnson, Wesley Snipes]                                                                                                                                                                                                                                                                                                                                                  |[Whitney Houston, Angela Bassett, Loretta Devine, Lela Rochon, Gregory Hines, Dennis Haysbert, Michael Beach, Mykelti Williamson, Lamont Johnson, Wesley Snipes]                                                                                                                                                                                                                                                                                                                                                  |
[2024-11-13T06:58:45.323+0000] {spark_submit.py:579} INFO - |11862|[Steve Martin, Diane Keaton, Martin Short, Kimberly Williams-Paisley, George Newbern, Kieran Culkin, BD Wong, Peter Michael Goetz, Kate McGregor-Stewart, Jane Adams, Eugene Levy, Lori Alan]                                                                                                                                                                                                                                                                                                                     |[Steve Martin, Diane Keaton, Martin Short, Kimberly Williams-Paisley, George Newbern, Kieran Culkin, BD Wong, Peter Michael Goetz, Kate McGregor-Stewart, Jane Adams, Eugene Levy, Lori Alan]                                                                                                                                                                                                                                                                                                                     |
[2024-11-13T06:58:45.324+0000] {spark_submit.py:579} INFO - |949  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.324+0000] {spark_submit.py:579} INFO - |11860|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.325+0000] {spark_submit.py:579} INFO - |45325|[Jonathan Taylor Thomas, Brad Renfro, Rachael Leigh Cook, Michael McShane, Amy Wright, Eric Schweig, Tamara Mello]                                                                                                                                                                                                                                                                                                                                                                                                |[Jonathan Taylor Thomas, Brad Renfro, Rachael Leigh Cook, Michael McShane, Amy Wright, Eric Schweig, Tamara Mello]                                                                                                                                                                                                                                                                                                                                                                                                |
[2024-11-13T06:58:45.325+0000] {spark_submit.py:579} INFO - |9091 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.326+0000] {spark_submit.py:579} INFO - |710  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.326+0000] {spark_submit.py:579} INFO - |9087 |[Michael Douglas, Annette Bening, Michael J. Fox, Martin Sheen, Anna Deavere Smith, Shawna Waldron, Samantha Mathis, David Paymer, Richard Dreyfuss, Nina Siemaszko, Wendie Malick, Beau Billingslea, Gail Strickland, Joshua Malina, Clement von Franckenstein, John Mahoney, John Mahon, Gabriel Jarret]                                                                                                                                                                                                        |[Michael Douglas, Annette Bening, Michael J. Fox, Martin Sheen, Anna Deavere Smith, Shawna Waldron, Samantha Mathis, David Paymer, Richard Dreyfuss, Nina Siemaszko, Wendie Malick, Beau Billingslea, Gail Strickland, Joshua Malina, Clement von Franckenstein, John Mahoney, John Mahon, Gabriel Jarret]                                                                                                                                                                                                        |
[2024-11-13T06:58:45.326+0000] {spark_submit.py:579} INFO - |12110|[Leslie Nielsen, Mel Brooks, Amy Yasbeck, Peter MacNicol, Lysette Anthony, Harvey Korman, Steven Weber, Mark Blankfield, Megan Cavanagh, Gregg Binkley, Anne Bancroft]                                                                                                                                                                                                                                                                                                                                            |[Leslie Nielsen, Mel Brooks, Amy Yasbeck, Peter MacNicol, Lysette Anthony, Harvey Korman, Steven Weber, Mark Blankfield, Megan Cavanagh, Gregg Binkley, Anne Bancroft]                                                                                                                                                                                                                                                                                                                                            |
[2024-11-13T06:58:45.327+0000] {spark_submit.py:579} INFO - |21032|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.327+0000] {spark_submit.py:579} INFO - |10858|[Anthony Hopkins, Joan Allen, Powers Boothe, Ed Harris, Bob Hoskins, E.G. Marshall, David Paymer, David Hyde Pierce, Paul Sorvino, Mary Steenburgen, J.T. Walsh, James Woods, Brian Bedford, Kevin Dunn, Fyvush Finkel, Annabeth Gish, Larry Hagman, Madeline Kahn, Dan Hedaya, Bridgette Wilson, Tom Bower, Tony Goldwyn, Edward Herrmann, Tony Lo Bianco, Saul Rubinek, Robert Beltran, John Cunningham, John Diehl, John C. McGinley, Michael Chiklis, Ric Young, Boris Sichkin, Sam Waterston, Marley Shelton]|[Anthony Hopkins, Joan Allen, Powers Boothe, Ed Harris, Bob Hoskins, E.G. Marshall, David Paymer, David Hyde Pierce, Paul Sorvino, Mary Steenburgen, J.T. Walsh, James Woods, Brian Bedford, Kevin Dunn, Fyvush Finkel, Annabeth Gish, Larry Hagman, Madeline Kahn, Dan Hedaya, Bridgette Wilson, Tom Bower, Tony Goldwyn, Edward Herrmann, Tony Lo Bianco, Saul Rubinek, Robert Beltran, John Cunningham, John Diehl, John C. McGinley, Michael Chiklis, Ric Young, Boris Sichkin, Sam Waterston, Marley Shelton]|
[2024-11-13T06:58:45.328+0000] {spark_submit.py:579} INFO - |1408 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.328+0000] {spark_submit.py:579} INFO - |524  |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.328+0000] {spark_submit.py:579} INFO - |4584 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.329+0000] {spark_submit.py:579} INFO - |5    |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.329+0000] {spark_submit.py:579} INFO - |9273 |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.329+0000] {spark_submit.py:579} INFO - |11517|null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
[2024-11-13T06:58:45.330+0000] {spark_submit.py:579} INFO - +-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
[2024-11-13T06:58:45.330+0000] {spark_submit.py:579} INFO - only showing top 20 rows
[2024-11-13T06:58:45.330+0000] {spark_submit.py:579} INFO - 
[2024-11-13T06:58:45.338+0000] {spark_submit.py:579} INFO - 2024-11-13 06:58:45,338 INFO: Saving cleaned data to s3a://lakehouse/sliver/credit
[2024-11-13T06:58:45.614+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO DeltaLog: Loading version 4.
[2024-11-13T06:58:45.662+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on bdc4422970d0:36285 in memory (size: 12.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:45.665+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:45 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.21.0.9:41811 in memory (size: 12.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:46.177+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 5, totalFileSize: 8514)
[2024-11-13T06:58:46.195+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on bdc4422970d0:36285 in memory (size: 36.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:46.198+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.21.0.9:41811 in memory (size: 36.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:46.344+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:58:46.344+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-13T06:58:46.484+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO CodeGenerator: Code generated in 98.261126 ms
[2024-11-13T06:58:46.488+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 205.0 KiB, free 434.2 MiB)
[2024-11-13T06:58:46.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
[2024-11-13T06:58:46.503+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on bdc4422970d0:36285 (size: 36.1 KiB, free: 434.4 MiB)
[2024-11-13T06:58:46.504+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO SparkContext: Created broadcast 3 from toString at String.java:2951
[2024-11-13T06:58:46.529+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10490017 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:58:46.582+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-13T06:58:46.583+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Got job 2 (toString at String.java:2951) with 2 output partitions
[2024-11-13T06:58:46.583+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Final stage: ResultStage 2 (toString at String.java:2951)
[2024-11-13T06:58:46.584+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:58:46.585+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:46.585+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at toString at String.java:2951), which has no missing parents
[2024-11-13T06:58:46.588+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 50.4 KiB, free 434.1 MiB)
[2024-11-13T06:58:46.596+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 434.1 MiB)
[2024-11-13T06:58:46.598+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on bdc4422970d0:36285 (size: 15.6 KiB, free: 434.3 MiB)
[2024-11-13T06:58:46.599+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:46.600+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:58:46.600+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2024-11-13T06:58:46.602+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 8186 bytes)
[2024-11-13T06:58:46.603+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 8068 bytes)
[2024-11-13T06:58:46.635+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.21.0.9:41811 (size: 15.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:46.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.21.0.9:41811 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:58:47.109+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 507 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:58:47.132+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 531 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:58:47.133+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-11-13T06:58:47.134+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO DAGScheduler: ResultStage 2 (toString at String.java:2951) finished in 0.547 s
[2024-11-13T06:58:47.135+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:58:47.136+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2024-11-13T06:58:47.136+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO DAGScheduler: Job 2 finished: toString at String.java:2951, took 0.553254 s
[2024-11-13T06:58:47.250+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO CodeGenerator: Code generated in 62.438219 ms
[2024-11-13T06:58:47.261+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO Snapshot: [tableId=f333b7c9-2835-49f1-bf85-7121a36fc47f] Created snapshot Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=4, metadata=Metadata(1ebd1207-c4be-4b86-929b-b7bddbdf5b30,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730720660307)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,4,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=2106; replication=1; blocksize=33554432; modification_time=1730720671601; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=fe6387437d4956fa3dbcb1c86afaf67d versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730904135368; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=de49fd58c24b8681224c3e8f6c962415 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000002.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730951282026; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a6ff16bd4dbd01c9414de62ba0c96922 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000003.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731072300417; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=e0fd6ca08e417c3ec17a4db96be277c1 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000004.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731381081702; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a5a81a463c53a4c4ec181471ebfe21e7 versionId=null),None,1731381081702), checksumOpt=None)
[2024-11-13T06:58:47.572+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on bdc4422970d0:36285 in memory (size: 15.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:47.575+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.21.0.9:41811 in memory (size: 15.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:47.899+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:58:47.900+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(transform(from_json(ArrayType(StructType(StructField(name,StringType,true)),true), cast#0, Some(Etc/UTC)), lambdafunction(lambda x#12.name, lambda x#12, false)))
[2024-11-13T06:58:47.968+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on bdc4422970d0:36285 in memory (size: 36.1 KiB, free: 434.4 MiB)
[2024-11-13T06:58:47.970+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:47 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.21.0.9:41811 in memory (size: 36.1 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:48.002+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-11-13T06:58:48.053+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-13T06:58:48.095+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO CodeGenerator: Code generated in 27.248481 ms
[2024-11-13T06:58:48.105+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO CodeGenerator: Code generated in 6.919143 ms
[2024-11-13T06:58:48.109+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 206.6 KiB, free 434.2 MiB)
[2024-11-13T06:58:48.123+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 434.2 MiB)
[2024-11-13T06:58:48.125+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on bdc4422970d0:36285 (size: 36.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:48.126+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO SparkContext: Created broadcast 5 from save at NativeMethodAccessorImpl.java:0
[2024-11-13T06:58:48.128+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:58:48.186+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Registering RDD 19 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-11-13T06:58:48.190+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Got map stage job 3 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-13T06:58:48.190+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (save at NativeMethodAccessorImpl.java:0)
[2024-11-13T06:58:48.191+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:58:48.194+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:48.196+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[19] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-13T06:58:48.210+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 49.4 KiB, free 434.1 MiB)
[2024-11-13T06:58:48.218+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 434.1 MiB)
[2024-11-13T06:58:48.220+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on bdc4422970d0:36285 (size: 19.6 KiB, free: 434.3 MiB)
[2024-11-13T06:58:48.221+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:48.223+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[19] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:58:48.224+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2024-11-13T06:58:48.226+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-11-13T06:58:48.227+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-11-13T06:58:48.259+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.21.0.9:41811 (size: 19.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:48.523+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.21.0.9:41811 (size: 36.6 KiB, free: 1048.7 MiB)
[2024-11-13T06:58:48.601+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:48 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 375 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:58:51.706+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 3482 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:58:51.707+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-11-13T06:58:51.708+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: ShuffleMapStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 3.508 s
[2024-11-13T06:58:51.709+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: looking for newly runnable stages
[2024-11-13T06:58:51.710+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: running: Set()
[2024-11-13T06:58:51.711+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: waiting: Set()
[2024-11-13T06:58:51.712+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: failed: Set()
[2024-11-13T06:58:51.733+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 5348362, minimum partition size: 1048576
[2024-11-13T06:58:51.762+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-11-13T06:58:51.789+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO CodeGenerator: Code generated in 23.004918 ms
[2024-11-13T06:58:51.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on bdc4422970d0:36285 in memory (size: 19.6 KiB, free: 434.4 MiB)
[2024-11-13T06:58:51.858+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.21.0.9:41811 in memory (size: 19.6 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:51.891+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-11-13T06:58:51.893+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-11-13T06:58:51.894+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
[2024-11-13T06:58:51.894+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
[2024-11-13T06:58:51.895+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:51.897+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-11-13T06:58:51.935+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 371.5 KiB, free 433.8 MiB)
[2024-11-13T06:58:51.944+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 135.0 KiB, free 433.7 MiB)
[2024-11-13T06:58:51.946+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on bdc4422970d0:36285 (size: 135.0 KiB, free: 434.2 MiB)
[2024-11-13T06:58:51.946+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:51.947+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:58:51.948+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2024-11-13T06:58:51.965+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (172.21.0.9, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-13T06:58:51.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (172.21.0.9, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-11-13T06:58:51.987+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.21.0.9:41811 (size: 135.0 KiB, free: 1048.6 MiB)
[2024-11-13T06:58:52.200+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.21.0.9:58990
[2024-11-13T06:58:55.446+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 3493 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:58:55.447+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 3481 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:58:55.448+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2024-11-13T06:58:55.451+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 3.541 s
[2024-11-13T06:58:55.452+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:58:55.452+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2024-11-13T06:58:55.453+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 3.560688 s
[2024-11-13T06:58:55.456+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO FileFormatWriter: Start to commit write Job 26b4f138-4ff3-46fe-b763-141e2d05b3e8.
[2024-11-13T06:58:55.461+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO FileFormatWriter: Write Job 26b4f138-4ff3-46fe-b763-141e2d05b3e8 committed. Elapsed time: 2 ms.
[2024-11-13T06:58:55.469+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO FileFormatWriter: Finished processing stats for write job 26b4f138-4ff3-46fe-b763-141e2d05b3e8.
[2024-11-13T06:58:55.499+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO Snapshot: [tableId=1ebd1207-c4be-4b86-929b-b7bddbdf5b30] DELTA: Compute snapshot for version: 4
[2024-11-13T06:58:55.516+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 204.6 KiB, free 433.5 MiB)
[2024-11-13T06:58:55.529+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on bdc4422970d0:36285 in memory (size: 135.0 KiB, free: 434.4 MiB)
[2024-11-13T06:58:55.532+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)
[2024-11-13T06:58:55.533+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.21.0.9:41811 in memory (size: 135.0 KiB, free: 1048.8 MiB)
[2024-11-13T06:58:55.535+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on bdc4422970d0:36285 (size: 36.0 KiB, free: 434.3 MiB)
[2024-11-13T06:58:55.536+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:55 INFO SparkContext: Created broadcast 8 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:58:56.189+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:58:56.189+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-13T06:58:56.211+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-11-13T06:58:56.401+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO CodeGenerator: Code generated in 112.634747 ms
[2024-11-13T06:58:56.404+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 205.0 KiB, free 433.7 MiB)
[2024-11-13T06:58:56.412+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.7 MiB)
[2024-11-13T06:58:56.414+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on bdc4422970d0:36285 (size: 36.1 KiB, free: 434.3 MiB)
[2024-11-13T06:58:56.415+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO SparkContext: Created broadcast 9 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:58:56.416+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10490017 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:58:56.433+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Registering RDD 25 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2024-11-13T06:58:56.434+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Got map stage job 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-13T06:58:56.434+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Final stage: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:58:56.435+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:58:56.436+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:56.437+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[25] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:58:56.448+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 138.3 KiB, free 433.6 MiB)
[2024-11-13T06:58:56.463+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.5 MiB)
[2024-11-13T06:58:56.465+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on bdc4422970d0:36285 (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-13T06:58:56.466+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:56.467+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[25] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:58:56.468+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0
[2024-11-13T06:58:56.471+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 8175 bytes)
[2024-11-13T06:58:56.472+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 8057 bytes)
[2024-11-13T06:58:56.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.21.0.9:41811 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-13T06:58:56.766+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.21.0.9:41811 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:58:56.821+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 349 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:58:56.833+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 362 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:58:56.833+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-11-13T06:58:56.834+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.395 s
[2024-11-13T06:58:56.834+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: looking for newly runnable stages
[2024-11-13T06:58:56.835+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: running: Set()
[2024-11-13T06:58:56.836+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: waiting: Set()
[2024-11-13T06:58:56.836+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:56 INFO DAGScheduler: failed: Set()
[2024-11-13T06:58:57.304+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 19938 bytes
[2024-11-13T06:58:57.305+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO CodeGenerator: Code generated in 292.575069 ms
[2024-11-13T06:58:57.411+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on bdc4422970d0:36285 in memory (size: 39.0 KiB, free: 434.3 MiB)
[2024-11-13T06:58:57.414+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.21.0.9:41811 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-13T06:58:57.416+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO CodeGenerator: Code generated in 85.265619 ms
[2024-11-13T06:58:57.707+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO CodeGenerator: Code generated in 46.025075 ms
[2024-11-13T06:58:57.715+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Registering RDD 35 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2024-11-13T06:58:57.716+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Got map stage job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-13T06:58:57.716+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:58:57.717+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
[2024-11-13T06:58:57.723+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:58:57.724+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[35] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:58:57.842+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 517.1 KiB, free 433.2 MiB)
[2024-11-13T06:58:57.852+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 433.1 MiB)
[2024-11-13T06:58:57.854+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on bdc4422970d0:36285 (size: 123.3 KiB, free: 434.2 MiB)
[2024-11-13T06:58:57.855+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:58:57.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[35] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-13T06:58:57.857+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO TaskSchedulerImpl: Adding task set 8.0 with 50 tasks resource profile 0
[2024-11-13T06:58:57.870+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 10) (172.21.0.9, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:57.871+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO TaskSetManager: Starting task 7.0 in stage 8.0 (TID 11) (172.21.0.9, executor 0, partition 7, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:57.890+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:57 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.21.0.9:41811 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-11-13T06:58:58.043+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.21.0.9:58990
[2024-11-13T06:58:58.496+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_3 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.496+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_7 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.662+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 14.0 in stage 8.0 (TID 12) (172.21.0.9, executor 0, partition 14, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.664+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 10) in 793 ms on 172.21.0.9 (executor 0) (1/50)
[2024-11-13T06:58:58.665+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 24.0 in stage 8.0 (TID 13) (172.21.0.9, executor 0, partition 24, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.666+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 7.0 in stage 8.0 (TID 11) in 795 ms on 172.21.0.9 (executor 0) (2/50)
[2024-11-13T06:58:58.743+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_24 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.744+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_14 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.781+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 26.0 in stage 8.0 (TID 14) (172.21.0.9, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.782+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 14.0 in stage 8.0 (TID 12) in 119 ms on 172.21.0.9 (executor 0) (3/50)
[2024-11-13T06:58:58.786+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 32.0 in stage 8.0 (TID 15) (172.21.0.9, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.787+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 24.0 in stage 8.0 (TID 13) in 123 ms on 172.21.0.9 (executor 0) (4/50)
[2024-11-13T06:58:58.844+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_26 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.851+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_32 in memory on 172.21.0.9:41811 (size: 469.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.876+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 38.0 in stage 8.0 (TID 16) (172.21.0.9, executor 0, partition 38, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.877+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 26.0 in stage 8.0 (TID 14) in 97 ms on 172.21.0.9 (executor 0) (5/50)
[2024-11-13T06:58:58.884+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 40.0 in stage 8.0 (TID 17) (172.21.0.9, executor 0, partition 40, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.886+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 32.0 in stage 8.0 (TID 15) in 99 ms on 172.21.0.9 (executor 0) (6/50)
[2024-11-13T06:58:58.960+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_38 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO BlockManagerInfo: Added rdd_32_40 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:58.994+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Starting task 41.0 in stage 8.0 (TID 18) (172.21.0.9, executor 0, partition 41, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:58.996+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:58 INFO TaskSetManager: Finished task 38.0 in stage 8.0 (TID 16) in 120 ms on 172.21.0.9 (executor 0) (7/50)
[2024-11-13T06:58:59.004+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 42.0 in stage 8.0 (TID 19) (172.21.0.9, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.005+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 40.0 in stage 8.0 (TID 17) in 121 ms on 172.21.0.9 (executor 0) (8/50)
[2024-11-13T06:58:59.054+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_41 in memory on 172.21.0.9:41811 (size: 470.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.087+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 46.0 in stage 8.0 (TID 20) (172.21.0.9, executor 0, partition 46, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.088+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 41.0 in stage 8.0 (TID 18) in 93 ms on 172.21.0.9 (executor 0) (9/50)
[2024-11-13T06:58:59.144+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_46 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.173+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 21) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.174+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 46.0 in stage 8.0 (TID 20) in 88 ms on 172.21.0.9 (executor 0) (10/50)
[2024-11-13T06:58:59.227+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_0 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.257+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 22) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.257+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 21) in 84 ms on 172.21.0.9 (executor 0) (11/50)
[2024-11-13T06:58:59.309+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_1 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.323+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_42 in memory on 172.21.0.9:41811 (size: 566.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.340+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 23) (172.21.0.9, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.342+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 22) in 86 ms on 172.21.0.9 (executor 0) (12/50)
[2024-11-13T06:58:59.353+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 4.0 in stage 8.0 (TID 24) (172.21.0.9, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.354+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 42.0 in stage 8.0 (TID 19) in 351 ms on 172.21.0.9 (executor 0) (13/50)
[2024-11-13T06:58:59.406+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_2 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.416+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_4 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.437+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 5.0 in stage 8.0 (TID 25) (172.21.0.9, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.437+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 23) in 97 ms on 172.21.0.9 (executor 0) (14/50)
[2024-11-13T06:58:59.445+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 6.0 in stage 8.0 (TID 26) (172.21.0.9, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.446+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 4.0 in stage 8.0 (TID 24) in 93 ms on 172.21.0.9 (executor 0) (15/50)
[2024-11-13T06:58:59.492+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_5 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_6 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.521+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 8.0 in stage 8.0 (TID 27) (172.21.0.9, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.522+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 5.0 in stage 8.0 (TID 25) in 85 ms on 172.21.0.9 (executor 0) (16/50)
[2024-11-13T06:58:59.529+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 9.0 in stage 8.0 (TID 28) (172.21.0.9, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.530+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 6.0 in stage 8.0 (TID 26) in 85 ms on 172.21.0.9 (executor 0) (17/50)
[2024-11-13T06:58:59.571+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_8 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.583+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_9 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.597+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 10.0 in stage 8.0 (TID 29) (172.21.0.9, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.598+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 8.0 in stage 8.0 (TID 27) in 77 ms on 172.21.0.9 (executor 0) (18/50)
[2024-11-13T06:58:59.613+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 11.0 in stage 8.0 (TID 30) (172.21.0.9, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.614+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 9.0 in stage 8.0 (TID 28) in 85 ms on 172.21.0.9 (executor 0) (19/50)
[2024-11-13T06:58:59.649+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_10 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.677+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_11 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.678+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 12.0 in stage 8.0 (TID 31) (172.21.0.9, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.679+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 10.0 in stage 8.0 (TID 29) in 83 ms on 172.21.0.9 (executor 0) (20/50)
[2024-11-13T06:58:59.707+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 13.0 in stage 8.0 (TID 32) (172.21.0.9, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.708+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 11.0 in stage 8.0 (TID 30) in 96 ms on 172.21.0.9 (executor 0) (21/50)
[2024-11-13T06:58:59.732+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_12 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.762+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_13 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.764+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 15.0 in stage 8.0 (TID 33) (172.21.0.9, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.764+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 12.0 in stage 8.0 (TID 31) in 86 ms on 172.21.0.9 (executor 0) (22/50)
[2024-11-13T06:58:59.791+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 16.0 in stage 8.0 (TID 34) (172.21.0.9, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.791+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 13.0 in stage 8.0 (TID 32) in 84 ms on 172.21.0.9 (executor 0) (23/50)
[2024-11-13T06:58:59.818+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_15 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.844+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 17.0 in stage 8.0 (TID 35) (172.21.0.9, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.845+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 15.0 in stage 8.0 (TID 33) in 81 ms on 172.21.0.9 (executor 0) (24/50)
[2024-11-13T06:58:59.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_16 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.885+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 18.0 in stage 8.0 (TID 36) (172.21.0.9, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.886+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 16.0 in stage 8.0 (TID 34) in 95 ms on 172.21.0.9 (executor 0) (25/50)
[2024-11-13T06:58:59.893+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_17 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.917+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 19.0 in stage 8.0 (TID 37) (172.21.0.9, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.918+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 17.0 in stage 8.0 (TID 35) in 74 ms on 172.21.0.9 (executor 0) (26/50)
[2024-11-13T06:58:59.937+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_18 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.965+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 20.0 in stage 8.0 (TID 38) (172.21.0.9, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 18.0 in stage 8.0 (TID 36) in 81 ms on 172.21.0.9 (executor 0) (27/50)
[2024-11-13T06:58:59.971+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO BlockManagerInfo: Added rdd_32_19 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:58:59.998+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Starting task 21.0 in stage 8.0 (TID 39) (172.21.0.9, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:58:59.999+0000] {spark_submit.py:579} INFO - 24/11/13 06:58:59 INFO TaskSetManager: Finished task 19.0 in stage 8.0 (TID 37) in 81 ms on 172.21.0.9 (executor 0) (28/50)
[2024-11-13T06:59:00.017+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_20 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.052+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 22.0 in stage 8.0 (TID 40) (172.21.0.9, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.053+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 20.0 in stage 8.0 (TID 38) in 89 ms on 172.21.0.9 (executor 0) (29/50)
[2024-11-13T06:59:00.061+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_21 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.086+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 23.0 in stage 8.0 (TID 41) (172.21.0.9, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.087+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 21.0 in stage 8.0 (TID 39) in 89 ms on 172.21.0.9 (executor 0) (30/50)
[2024-11-13T06:59:00.105+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_22 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.129+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 25.0 in stage 8.0 (TID 42) (172.21.0.9, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.130+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 22.0 in stage 8.0 (TID 40) in 78 ms on 172.21.0.9 (executor 0) (31/50)
[2024-11-13T06:59:00.133+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_23 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.161+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 27.0 in stage 8.0 (TID 43) (172.21.0.9, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.162+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 23.0 in stage 8.0 (TID 41) in 77 ms on 172.21.0.9 (executor 0) (32/50)
[2024-11-13T06:59:00.182+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_25 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.209+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 28.0 in stage 8.0 (TID 44) (172.21.0.9, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.210+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 25.0 in stage 8.0 (TID 42) in 80 ms on 172.21.0.9 (executor 0) (33/50)
[2024-11-13T06:59:00.210+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_27 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.236+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 29.0 in stage 8.0 (TID 45) (172.21.0.9, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.236+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 27.0 in stage 8.0 (TID 43) in 75 ms on 172.21.0.9 (executor 0) (34/50)
[2024-11-13T06:59:00.257+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_28 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.281+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 30.0 in stage 8.0 (TID 46) (172.21.0.9, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.282+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 28.0 in stage 8.0 (TID 44) in 74 ms on 172.21.0.9 (executor 0) (35/50)
[2024-11-13T06:59:00.292+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_29 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.323+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 31.0 in stage 8.0 (TID 47) (172.21.0.9, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.324+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 29.0 in stage 8.0 (TID 45) in 89 ms on 172.21.0.9 (executor 0) (36/50)
[2024-11-13T06:59:00.340+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_30 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.364+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 33.0 in stage 8.0 (TID 48) (172.21.0.9, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.365+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 30.0 in stage 8.0 (TID 46) in 84 ms on 172.21.0.9 (executor 0) (37/50)
[2024-11-13T06:59:00.370+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_31 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.394+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 34.0 in stage 8.0 (TID 49) (172.21.0.9, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.395+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 31.0 in stage 8.0 (TID 47) in 72 ms on 172.21.0.9 (executor 0) (38/50)
[2024-11-13T06:59:00.412+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_33 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.434+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 35.0 in stage 8.0 (TID 50) (172.21.0.9, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.435+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 33.0 in stage 8.0 (TID 48) in 71 ms on 172.21.0.9 (executor 0) (39/50)
[2024-11-13T06:59:00.439+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_34 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.462+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 36.0 in stage 8.0 (TID 51) (172.21.0.9, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.462+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 34.0 in stage 8.0 (TID 49) in 68 ms on 172.21.0.9 (executor 0) (40/50)
[2024-11-13T06:59:00.479+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_35 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 37.0 in stage 8.0 (TID 52) (172.21.0.9, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.503+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 35.0 in stage 8.0 (TID 50) in 68 ms on 172.21.0.9 (executor 0) (41/50)
[2024-11-13T06:59:00.505+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_36 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.529+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 39.0 in stage 8.0 (TID 53) (172.21.0.9, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.529+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 36.0 in stage 8.0 (TID 51) in 68 ms on 172.21.0.9 (executor 0) (42/50)
[2024-11-13T06:59:00.546+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_37 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.571+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 43.0 in stage 8.0 (TID 54) (172.21.0.9, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.572+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 37.0 in stage 8.0 (TID 52) in 70 ms on 172.21.0.9 (executor 0) (43/50)
[2024-11-13T06:59:00.574+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_39 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.597+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 44.0 in stage 8.0 (TID 55) (172.21.0.9, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.598+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 39.0 in stage 8.0 (TID 53) in 70 ms on 172.21.0.9 (executor 0) (44/50)
[2024-11-13T06:59:00.617+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_43 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.641+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_44 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.642+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 45.0 in stage 8.0 (TID 56) (172.21.0.9, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.643+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 43.0 in stage 8.0 (TID 54) in 71 ms on 172.21.0.9 (executor 0) (45/50)
[2024-11-13T06:59:00.666+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 47.0 in stage 8.0 (TID 57) (172.21.0.9, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.666+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 44.0 in stage 8.0 (TID 55) in 69 ms on 172.21.0.9 (executor 0) (46/50)
[2024-11-13T06:59:00.686+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_45 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.709+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 48.0 in stage 8.0 (TID 58) (172.21.0.9, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.710+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_47 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.711+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 45.0 in stage 8.0 (TID 56) in 67 ms on 172.21.0.9 (executor 0) (47/50)
[2024-11-13T06:59:00.733+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 49.0 in stage 8.0 (TID 59) (172.21.0.9, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:00.734+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 47.0 in stage 8.0 (TID 57) in 69 ms on 172.21.0.9 (executor 0) (48/50)
[2024-11-13T06:59:00.757+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_48 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.778+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added rdd_32_49 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:00.782+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 48.0 in stage 8.0 (TID 58) in 73 ms on 172.21.0.9 (executor 0) (49/50)
[2024-11-13T06:59:00.801+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 49.0 in stage 8.0 (TID 59) in 67 ms on 172.21.0.9 (executor 0) (50/50)
[2024-11-13T06:59:00.801+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2024-11-13T06:59:00.802+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 3.071 s
[2024-11-13T06:59:00.802+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: looking for newly runnable stages
[2024-11-13T06:59:00.803+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: running: Set()
[2024-11-13T06:59:00.804+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: waiting: Set()
[2024-11-13T06:59:00.804+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: failed: Set()
[2024-11-13T06:59:00.829+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on bdc4422970d0:36285 in memory (size: 123.3 KiB, free: 434.3 MiB)
[2024-11-13T06:59:00.831+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.21.0.9:41811 in memory (size: 123.3 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:00.840+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:59:00.842+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Got job 7 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-13T06:59:00.842+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:59:00.843+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2024-11-13T06:59:00.843+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:00.844+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[38] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:59:00.851+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 452.2 KiB, free 433.3 MiB)
[2024-11-13T06:59:00.866+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 433.1 MiB)
[2024-11-13T06:59:00.868+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on bdc4422970d0:36285 (size: 108.1 KiB, free: 434.2 MiB)
[2024-11-13T06:59:00.869+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:00.870+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[38] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-13T06:59:00.870+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2024-11-13T06:59:00.872+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 60) (172.21.0.9, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-13T06:59:00.885+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.21.0.9:41811 (size: 108.1 KiB, free: 1048.6 MiB)
[2024-11-13T06:59:00.896+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.21.0.9:58990
[2024-11-13T06:59:00.966+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 60) in 95 ms on 172.21.0.9 (executor 0) (1/1)
[2024-11-13T06:59:00.967+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-11-13T06:59:00.968+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.122 s
[2024-11-13T06:59:00.969+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:59:00.969+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-11-13T06:59:00.970+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:00 INFO DAGScheduler: Job 7 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.128755 s
[2024-11-13T06:59:01.024+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO CodeGenerator: Code generated in 36.0834 ms
[2024-11-13T06:59:01.027+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO Snapshot: [tableId=1ebd1207-c4be-4b86-929b-b7bddbdf5b30] DELTA: Done
[2024-11-13T06:59:01.288+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO BlockManagerInfo: Removed broadcast_12_piece0 on bdc4422970d0:36285 in memory (size: 108.1 KiB, free: 434.3 MiB)
[2024-11-13T06:59:01.290+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.21.0.9:41811 in memory (size: 108.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:01.398+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO CodeGenerator: Code generated in 94.279595 ms
[2024-11-13T06:59:01.413+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:59:01.414+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Got job 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-13T06:59:01.415+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:59:01.416+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2024-11-13T06:59:01.416+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:01.417+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[40] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:59:01.429+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 592.1 KiB, free 433.1 MiB)
[2024-11-13T06:59:01.434+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 138.5 KiB, free 433.0 MiB)
[2024-11-13T06:59:01.436+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on bdc4422970d0:36285 (size: 138.5 KiB, free: 434.2 MiB)
[2024-11-13T06:59:01.437+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:01.438+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 13 (MapPartitionsRDD[40] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-13T06:59:01.439+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSchedulerImpl: Adding task set 13.0 with 50 tasks resource profile 0
[2024-11-13T06:59:01.441+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 61) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.442+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 62) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.453+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.21.0.9:41811 (size: 138.5 KiB, free: 1048.6 MiB)
[2024-11-13T06:59:01.632+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 63) (172.21.0.9, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.633+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 62) in 191 ms on 172.21.0.9 (executor 0) (1/50)
[2024-11-13T06:59:01.634+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 64) (172.21.0.9, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.635+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 61) in 193 ms on 172.21.0.9 (executor 0) (2/50)
[2024-11-13T06:59:01.648+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 4.0 in stage 13.0 (TID 65) (172.21.0.9, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.650+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 5.0 in stage 13.0 (TID 66) (172.21.0.9, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.650+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 63) in 19 ms on 172.21.0.9 (executor 0) (3/50)
[2024-11-13T06:59:01.651+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 64) in 17 ms on 172.21.0.9 (executor 0) (4/50)
[2024-11-13T06:59:01.665+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 6.0 in stage 13.0 (TID 67) (172.21.0.9, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.666+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 4.0 in stage 13.0 (TID 65) in 18 ms on 172.21.0.9 (executor 0) (5/50)
[2024-11-13T06:59:01.667+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 7.0 in stage 13.0 (TID 68) (172.21.0.9, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.668+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 5.0 in stage 13.0 (TID 66) in 18 ms on 172.21.0.9 (executor 0) (6/50)
[2024-11-13T06:59:01.682+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 8.0 in stage 13.0 (TID 69) (172.21.0.9, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.683+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 6.0 in stage 13.0 (TID 67) in 17 ms on 172.21.0.9 (executor 0) (7/50)
[2024-11-13T06:59:01.684+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 9.0 in stage 13.0 (TID 70) (172.21.0.9, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.685+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 7.0 in stage 13.0 (TID 68) in 17 ms on 172.21.0.9 (executor 0) (8/50)
[2024-11-13T06:59:01.698+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 10.0 in stage 13.0 (TID 71) (172.21.0.9, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.699+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 8.0 in stage 13.0 (TID 69) in 18 ms on 172.21.0.9 (executor 0) (9/50)
[2024-11-13T06:59:01.701+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 11.0 in stage 13.0 (TID 72) (172.21.0.9, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.701+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 9.0 in stage 13.0 (TID 70) in 18 ms on 172.21.0.9 (executor 0) (10/50)
[2024-11-13T06:59:01.714+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 12.0 in stage 13.0 (TID 73) (172.21.0.9, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.715+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 10.0 in stage 13.0 (TID 71) in 17 ms on 172.21.0.9 (executor 0) (11/50)
[2024-11-13T06:59:01.716+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 13.0 in stage 13.0 (TID 74) (172.21.0.9, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.717+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 11.0 in stage 13.0 (TID 72) in 17 ms on 172.21.0.9 (executor 0) (12/50)
[2024-11-13T06:59:01.731+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 14.0 in stage 13.0 (TID 75) (172.21.0.9, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.732+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 12.0 in stage 13.0 (TID 73) in 18 ms on 172.21.0.9 (executor 0) (13/50)
[2024-11-13T06:59:01.734+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 15.0 in stage 13.0 (TID 76) (172.21.0.9, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.735+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 13.0 in stage 13.0 (TID 74) in 19 ms on 172.21.0.9 (executor 0) (14/50)
[2024-11-13T06:59:01.747+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 16.0 in stage 13.0 (TID 77) (172.21.0.9, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.748+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 14.0 in stage 13.0 (TID 75) in 17 ms on 172.21.0.9 (executor 0) (15/50)
[2024-11-13T06:59:01.749+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 17.0 in stage 13.0 (TID 78) (172.21.0.9, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.750+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 15.0 in stage 13.0 (TID 76) in 16 ms on 172.21.0.9 (executor 0) (16/50)
[2024-11-13T06:59:01.764+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 18.0 in stage 13.0 (TID 79) (172.21.0.9, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.765+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 19.0 in stage 13.0 (TID 80) (172.21.0.9, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.766+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 16.0 in stage 13.0 (TID 77) in 18 ms on 172.21.0.9 (executor 0) (17/50)
[2024-11-13T06:59:01.766+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 17.0 in stage 13.0 (TID 78) in 18 ms on 172.21.0.9 (executor 0) (18/50)
[2024-11-13T06:59:01.778+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 20.0 in stage 13.0 (TID 81) (172.21.0.9, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.779+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 18.0 in stage 13.0 (TID 79) in 16 ms on 172.21.0.9 (executor 0) (19/50)
[2024-11-13T06:59:01.781+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 21.0 in stage 13.0 (TID 82) (172.21.0.9, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.782+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 19.0 in stage 13.0 (TID 80) in 16 ms on 172.21.0.9 (executor 0) (20/50)
[2024-11-13T06:59:01.794+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 22.0 in stage 13.0 (TID 83) (172.21.0.9, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.795+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 20.0 in stage 13.0 (TID 81) in 16 ms on 172.21.0.9 (executor 0) (21/50)
[2024-11-13T06:59:01.799+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 23.0 in stage 13.0 (TID 84) (172.21.0.9, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.799+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 21.0 in stage 13.0 (TID 82) in 19 ms on 172.21.0.9 (executor 0) (22/50)
[2024-11-13T06:59:01.808+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 24.0 in stage 13.0 (TID 85) (172.21.0.9, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.809+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 22.0 in stage 13.0 (TID 83) in 16 ms on 172.21.0.9 (executor 0) (23/50)
[2024-11-13T06:59:01.813+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 25.0 in stage 13.0 (TID 86) (172.21.0.9, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.814+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 23.0 in stage 13.0 (TID 84) in 16 ms on 172.21.0.9 (executor 0) (24/50)
[2024-11-13T06:59:01.823+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 26.0 in stage 13.0 (TID 87) (172.21.0.9, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.824+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 24.0 in stage 13.0 (TID 85) in 15 ms on 172.21.0.9 (executor 0) (25/50)
[2024-11-13T06:59:01.828+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 27.0 in stage 13.0 (TID 88) (172.21.0.9, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.828+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 25.0 in stage 13.0 (TID 86) in 15 ms on 172.21.0.9 (executor 0) (26/50)
[2024-11-13T06:59:01.837+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 28.0 in stage 13.0 (TID 89) (172.21.0.9, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.838+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 26.0 in stage 13.0 (TID 87) in 16 ms on 172.21.0.9 (executor 0) (27/50)
[2024-11-13T06:59:01.841+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 29.0 in stage 13.0 (TID 90) (172.21.0.9, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.842+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 27.0 in stage 13.0 (TID 88) in 15 ms on 172.21.0.9 (executor 0) (28/50)
[2024-11-13T06:59:01.852+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 30.0 in stage 13.0 (TID 91) (172.21.0.9, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.853+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 28.0 in stage 13.0 (TID 89) in 15 ms on 172.21.0.9 (executor 0) (29/50)
[2024-11-13T06:59:01.855+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 31.0 in stage 13.0 (TID 92) (172.21.0.9, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 29.0 in stage 13.0 (TID 90) in 15 ms on 172.21.0.9 (executor 0) (30/50)
[2024-11-13T06:59:01.867+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 32.0 in stage 13.0 (TID 93) (172.21.0.9, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.868+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 30.0 in stage 13.0 (TID 91) in 16 ms on 172.21.0.9 (executor 0) (31/50)
[2024-11-13T06:59:01.869+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 33.0 in stage 13.0 (TID 94) (172.21.0.9, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.870+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 31.0 in stage 13.0 (TID 92) in 15 ms on 172.21.0.9 (executor 0) (32/50)
[2024-11-13T06:59:01.882+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 34.0 in stage 13.0 (TID 95) (172.21.0.9, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.883+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 32.0 in stage 13.0 (TID 93) in 16 ms on 172.21.0.9 (executor 0) (33/50)
[2024-11-13T06:59:01.884+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 35.0 in stage 13.0 (TID 96) (172.21.0.9, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.885+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 33.0 in stage 13.0 (TID 94) in 15 ms on 172.21.0.9 (executor 0) (34/50)
[2024-11-13T06:59:01.897+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 36.0 in stage 13.0 (TID 97) (172.21.0.9, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.898+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 34.0 in stage 13.0 (TID 95) in 17 ms on 172.21.0.9 (executor 0) (35/50)
[2024-11-13T06:59:01.899+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 37.0 in stage 13.0 (TID 98) (172.21.0.9, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.900+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 35.0 in stage 13.0 (TID 96) in 17 ms on 172.21.0.9 (executor 0) (36/50)
[2024-11-13T06:59:01.912+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 38.0 in stage 13.0 (TID 99) (172.21.0.9, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.916+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 36.0 in stage 13.0 (TID 97) in 18 ms on 172.21.0.9 (executor 0) (37/50)
[2024-11-13T06:59:01.925+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 39.0 in stage 13.0 (TID 100) (172.21.0.9, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.926+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 37.0 in stage 13.0 (TID 98) in 27 ms on 172.21.0.9 (executor 0) (38/50)
[2024-11-13T06:59:01.938+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 40.0 in stage 13.0 (TID 101) (172.21.0.9, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.939+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 38.0 in stage 13.0 (TID 99) in 26 ms on 172.21.0.9 (executor 0) (39/50)
[2024-11-13T06:59:01.941+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 41.0 in stage 13.0 (TID 102) (172.21.0.9, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.942+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 39.0 in stage 13.0 (TID 100) in 16 ms on 172.21.0.9 (executor 0) (40/50)
[2024-11-13T06:59:01.955+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 42.0 in stage 13.0 (TID 103) (172.21.0.9, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.956+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 41.0 in stage 13.0 (TID 102) in 16 ms on 172.21.0.9 (executor 0) (41/50)
[2024-11-13T06:59:01.957+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 43.0 in stage 13.0 (TID 104) (172.21.0.9, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.958+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 40.0 in stage 13.0 (TID 101) in 21 ms on 172.21.0.9 (executor 0) (42/50)
[2024-11-13T06:59:01.971+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 44.0 in stage 13.0 (TID 105) (172.21.0.9, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.972+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 45.0 in stage 13.0 (TID 106) (172.21.0.9, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.973+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 43.0 in stage 13.0 (TID 104) in 15 ms on 172.21.0.9 (executor 0) (43/50)
[2024-11-13T06:59:01.974+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 42.0 in stage 13.0 (TID 103) in 19 ms on 172.21.0.9 (executor 0) (44/50)
[2024-11-13T06:59:01.985+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 46.0 in stage 13.0 (TID 107) (172.21.0.9, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.985+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 44.0 in stage 13.0 (TID 105) in 15 ms on 172.21.0.9 (executor 0) (45/50)
[2024-11-13T06:59:01.988+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 47.0 in stage 13.0 (TID 108) (172.21.0.9, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:01.989+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Finished task 45.0 in stage 13.0 (TID 106) in 17 ms on 172.21.0.9 (executor 0) (46/50)
[2024-11-13T06:59:01.999+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:01 INFO TaskSetManager: Starting task 48.0 in stage 13.0 (TID 109) (172.21.0.9, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:02.000+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 46.0 in stage 13.0 (TID 107) in 16 ms on 172.21.0.9 (executor 0) (47/50)
[2024-11-13T06:59:02.002+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Starting task 49.0 in stage 13.0 (TID 110) (172.21.0.9, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-11-13T06:59:02.003+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 47.0 in stage 13.0 (TID 108) in 16 ms on 172.21.0.9 (executor 0) (48/50)
[2024-11-13T06:59:02.014+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 48.0 in stage 13.0 (TID 109) in 15 ms on 172.21.0.9 (executor 0) (49/50)
[2024-11-13T06:59:02.016+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 49.0 in stage 13.0 (TID 110) in 14 ms on 172.21.0.9 (executor 0) (50/50)
[2024-11-13T06:59:02.017+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-11-13T06:59:02.017+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: ResultStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.598 s
[2024-11-13T06:59:02.018+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:59:02.018+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2024-11-13T06:59:02.019+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Job 8 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.604421 s
[2024-11-13T06:59:02.037+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO CodeGenerator: Code generated in 12.679952 ms
[2024-11-13T06:59:02.083+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO OptimisticTransaction: [tableId=1ebd1207,txnId=bb5bf5d6] Attempting to commit version 5 with 5 actions with Serializable isolation level
[2024-11-13T06:59:02.430+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DeltaLog: Creating a new snapshot v5 for commit version 5
[2024-11-13T06:59:02.430+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DeltaLog: Loading version 5.
[2024-11-13T06:59:02.435+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 6, totalFileSize: 10116)
[2024-11-13T06:59:02.479+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:59:02.480+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-13T06:59:02.497+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 205.1 KiB, free 432.8 MiB)
[2024-11-13T06:59:02.519+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on bdc4422970d0:36285 in memory (size: 138.5 KiB, free: 434.3 MiB)
[2024-11-13T06:59:02.520+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
[2024-11-13T06:59:02.521+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.21.0.9:41811 in memory (size: 138.5 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:02.522+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on bdc4422970d0:36285 (size: 36.1 KiB, free: 434.3 MiB)
[2024-11-13T06:59:02.523+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Created broadcast 14 from toString at String.java:2951
[2024-11-13T06:59:02.524+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12587970 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:59:02.539+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Starting job: toString at String.java:2951
[2024-11-13T06:59:02.540+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Got job 9 (toString at String.java:2951) with 2 output partitions
[2024-11-13T06:59:02.541+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Final stage: ResultStage 14 (toString at String.java:2951)
[2024-11-13T06:59:02.542+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:59:02.543+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:02.543+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[44] at toString at String.java:2951), which has no missing parents
[2024-11-13T06:59:02.555+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 50.4 KiB, free 433.4 MiB)
[2024-11-13T06:59:02.556+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 433.4 MiB)
[2024-11-13T06:59:02.557+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on bdc4422970d0:36285 (size: 15.6 KiB, free: 434.2 MiB)
[2024-11-13T06:59:02.558+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:02.558+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[44] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:59:02.559+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2024-11-13T06:59:02.560+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 111) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 8186 bytes)
[2024-11-13T06:59:02.561+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 112) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 8186 bytes)
[2024-11-13T06:59:02.571+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.21.0.9:41811 (size: 15.6 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:02.586+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.21.0.9:41811 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:02.631+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 112) in 71 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:59:02.636+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 111) in 77 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:59:02.637+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2024-11-13T06:59:02.637+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: ResultStage 14 (toString at String.java:2951) finished in 0.084 s
[2024-11-13T06:59:02.638+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:59:02.638+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2024-11-13T06:59:02.639+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Job 9 finished: toString at String.java:2951, took 0.098046 s
[2024-11-13T06:59:02.645+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO Snapshot: [tableId=1ebd1207-c4be-4b86-929b-b7bddbdf5b30] Created snapshot Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=5, metadata=Metadata(1ebd1207-c4be-4b86-929b-b7bddbdf5b30,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730720660307)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,5,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=2106; replication=1; blocksize=33554432; modification_time=1730720671601; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=fe6387437d4956fa3dbcb1c86afaf67d versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730904135368; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=de49fd58c24b8681224c3e8f6c962415 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000002.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730951282026; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a6ff16bd4dbd01c9414de62ba0c96922 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000003.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731072300417; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=e0fd6ca08e417c3ec17a4db96be277c1 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000004.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731381081702; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a5a81a463c53a4c4ec181471ebfe21e7 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000005.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731481142379; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=7f7cdca7105712f6663fbe11d030fc58 versionId=null),None,1731481142379), checksumOpt=None)
[2024-11-13T06:59:02.647+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=5, metadata=Metadata(1ebd1207-c4be-4b86-929b-b7bddbdf5b30,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"cast_name","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}},{"name":"director","type":{"type":"array","elementType":"string","containsNull":true},"nullable":true,"metadata":{}}]},List(),Map(),Some(1730720660307)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,5,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=2106; replication=1; blocksize=33554432; modification_time=1730720671601; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=fe6387437d4956fa3dbcb1c86afaf67d versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730904135368; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=de49fd58c24b8681224c3e8f6c962415 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000002.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1730951282026; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a6ff16bd4dbd01c9414de62ba0c96922 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000003.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731072300417; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=e0fd6ca08e417c3ec17a4db96be277c1 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000004.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731381081702; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=a5a81a463c53a4c4ec181471ebfe21e7 versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000005.json; isDirectory=false; length=1602; replication=1; blocksize=33554432; modification_time=1731481142379; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=7f7cdca7105712f6663fbe11d030fc58 versionId=null),None,1731481142379), checksumOpt=None)
[2024-11-13T06:59:02.648+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MapPartitionsRDD: Removing RDD 32 from persistence list
[2024-11-13T06:59:02.654+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManager: Removing RDD 32
[2024-11-13T06:59:02.662+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO Snapshot: [tableId=1ebd1207-c4be-4b86-929b-b7bddbdf5b30] DELTA: Compute snapshot for version: 5
[2024-11-13T06:59:02.665+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 204.8 KiB, free 433.2 MiB)
[2024-11-13T06:59:02.679+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on bdc4422970d0:36285 in memory (size: 15.6 KiB, free: 434.3 MiB)
[2024-11-13T06:59:02.680+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.21.0.9:41811 in memory (size: 15.6 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:02.681+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.2 MiB)
[2024-11-13T06:59:02.682+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on bdc4422970d0:36285 (size: 36.1 KiB, free: 434.2 MiB)
[2024-11-13T06:59:02.683+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Created broadcast 16 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:59:02.688+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_14_piece0 on bdc4422970d0:36285 in memory (size: 36.1 KiB, free: 434.3 MiB)
[2024-11-13T06:59:02.689+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.21.0.9:41811 in memory (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:02.866+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceStrategy: Pushed Filters:
[2024-11-13T06:59:02.867+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceStrategy: Post-Scan Filters:
[2024-11-13T06:59:02.917+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 205.1 KiB, free 433.3 MiB)
[2024-11-13T06:59:02.929+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.2 MiB)
[2024-11-13T06:59:02.930+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on bdc4422970d0:36285 (size: 36.1 KiB, free: 434.2 MiB)
[2024-11-13T06:59:02.931+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Created broadcast 17 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:59:02.932+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12587970 bytes, open cost is considered as scanning 4194304 bytes.
[2024-11-13T06:59:02.946+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Registering RDD 48 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 3
[2024-11-13T06:59:02.947+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Got map stage job 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-11-13T06:59:02.947+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Final stage: ShuffleMapStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:59:02.948+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Parents of final stage: List()
[2024-11-13T06:59:02.949+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:02.950+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[48] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:59:02.954+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 138.3 KiB, free 433.1 MiB)
[2024-11-13T06:59:02.969+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.0 MiB)
[2024-11-13T06:59:02.969+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on bdc4422970d0:36285 (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-13T06:59:02.970+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:02.971+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[48] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-11-13T06:59:02.971+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0
[2024-11-13T06:59:02.972+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 113) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 8175 bytes)
[2024-11-13T06:59:02.973+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 114) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 8175 bytes)
[2024-11-13T06:59:02.985+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:02 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.21.0.9:41811 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:03.008+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.21.0.9:41811 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:03.060+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 113) in 87 ms on 172.21.0.9 (executor 0) (1/2)
[2024-11-13T06:59:03.062+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 114) in 90 ms on 172.21.0.9 (executor 0) (2/2)
[2024-11-13T06:59:03.063+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-11-13T06:59:03.063+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: ShuffleMapStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.114 s
[2024-11-13T06:59:03.064+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: looking for newly runnable stages
[2024-11-13T06:59:03.064+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: running: Set()
[2024-11-13T06:59:03.065+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: waiting: Set()
[2024-11-13T06:59:03.066+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: failed: Set()
[2024-11-13T06:59:03.120+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Removed broadcast_18_piece0 on bdc4422970d0:36285 in memory (size: 39.0 KiB, free: 434.2 MiB)
[2024-11-13T06:59:03.121+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.21.0.9:41811 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:03.230+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Registering RDD 58 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 4
[2024-11-13T06:59:03.231+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Got map stage job 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-11-13T06:59:03.232+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Final stage: ShuffleMapStage 17 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:59:03.232+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
[2024-11-13T06:59:03.233+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:03.234+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[58] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:59:03.300+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 517.1 KiB, free 432.7 MiB)
[2024-11-13T06:59:03.305+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 123.4 KiB, free 432.6 MiB)
[2024-11-13T06:59:03.306+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on bdc4422970d0:36285 (size: 123.4 KiB, free: 434.1 MiB)
[2024-11-13T06:59:03.307+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:03.308+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[58] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-11-13T06:59:03.308+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSchedulerImpl: Adding task set 17.0 with 50 tasks resource profile 0
[2024-11-13T06:59:03.310+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 115) (172.21.0.9, executor 0, partition 3, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.311+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 7.0 in stage 17.0 (TID 116) (172.21.0.9, executor 0, partition 7, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.321+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.21.0.9:41811 (size: 123.4 KiB, free: 1048.6 MiB)
[2024-11-13T06:59:03.346+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.21.0.9:58990
[2024-11-13T06:59:03.372+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_7 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.373+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_3 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.391+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 14.0 in stage 17.0 (TID 117) (172.21.0.9, executor 0, partition 14, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.392+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 7.0 in stage 17.0 (TID 116) in 80 ms on 172.21.0.9 (executor 0) (1/50)
[2024-11-13T06:59:03.392+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 18.0 in stage 17.0 (TID 118) (172.21.0.9, executor 0, partition 18, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.393+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 115) in 83 ms on 172.21.0.9 (executor 0) (2/50)
[2024-11-13T06:59:03.424+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_14 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.425+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_18 in memory on 172.21.0.9:41811 (size: 469.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.446+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 20.0 in stage 17.0 (TID 119) (172.21.0.9, executor 0, partition 20, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.447+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 18.0 in stage 17.0 (TID 118) in 54 ms on 172.21.0.9 (executor 0) (3/50)
[2024-11-13T06:59:03.448+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 24.0 in stage 17.0 (TID 120) (172.21.0.9, executor 0, partition 24, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.448+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 14.0 in stage 17.0 (TID 117) in 58 ms on 172.21.0.9 (executor 0) (4/50)
[2024-11-13T06:59:03.478+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_20 in memory on 172.21.0.9:41811 (size: 471.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.485+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_24 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.495+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 26.0 in stage 17.0 (TID 121) (172.21.0.9, executor 0, partition 26, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.496+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 20.0 in stage 17.0 (TID 119) in 50 ms on 172.21.0.9 (executor 0) (5/50)
[2024-11-13T06:59:03.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 32.0 in stage 17.0 (TID 122) (172.21.0.9, executor 0, partition 32, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.502+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 24.0 in stage 17.0 (TID 120) in 55 ms on 172.21.0.9 (executor 0) (6/50)
[2024-11-13T06:59:03.530+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_26 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.533+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_32 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.548+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 38.0 in stage 17.0 (TID 123) (172.21.0.9, executor 0, partition 38, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.548+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 26.0 in stage 17.0 (TID 121) in 53 ms on 172.21.0.9 (executor 0) (7/50)
[2024-11-13T06:59:03.549+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 40.0 in stage 17.0 (TID 124) (172.21.0.9, executor 0, partition 40, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.550+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 32.0 in stage 17.0 (TID 122) in 49 ms on 172.21.0.9 (executor 0) (8/50)
[2024-11-13T06:59:03.580+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_40 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.580+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_38 in memory on 172.21.0.9:41811 (size: 301.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.597+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 41.0 in stage 17.0 (TID 125) (172.21.0.9, executor 0, partition 41, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.598+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 40.0 in stage 17.0 (TID 124) in 48 ms on 172.21.0.9 (executor 0) (9/50)
[2024-11-13T06:59:03.599+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 42.0 in stage 17.0 (TID 126) (172.21.0.9, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.599+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 38.0 in stage 17.0 (TID 123) in 52 ms on 172.21.0.9 (executor 0) (10/50)
[2024-11-13T06:59:03.632+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_42 in memory on 172.21.0.9:41811 (size: 566.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.633+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_41 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.651+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 46.0 in stage 17.0 (TID 127) (172.21.0.9, executor 0, partition 46, NODE_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.652+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 42.0 in stage 17.0 (TID 126) in 54 ms on 172.21.0.9 (executor 0) (11/50)
[2024-11-13T06:59:03.653+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 128) (172.21.0.9, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.654+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 41.0 in stage 17.0 (TID 125) in 57 ms on 172.21.0.9 (executor 0) (12/50)
[2024-11-13T06:59:03.683+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_0 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.686+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_46 in memory on 172.21.0.9:41811 (size: 302.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.700+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 129) (172.21.0.9, executor 0, partition 1, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.701+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 128) in 48 ms on 172.21.0.9 (executor 0) (13/50)
[2024-11-13T06:59:03.704+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 130) (172.21.0.9, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.705+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 46.0 in stage 17.0 (TID 127) in 53 ms on 172.21.0.9 (executor 0) (14/50)
[2024-11-13T06:59:03.732+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_1 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.734+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_2 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.748+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 4.0 in stage 17.0 (TID 131) (172.21.0.9, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.749+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 129) in 49 ms on 172.21.0.9 (executor 0) (15/50)
[2024-11-13T06:59:03.750+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 5.0 in stage 17.0 (TID 132) (172.21.0.9, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.751+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 130) in 48 ms on 172.21.0.9 (executor 0) (16/50)
[2024-11-13T06:59:03.788+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_5 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.789+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_4 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.804+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 6.0 in stage 17.0 (TID 133) (172.21.0.9, executor 0, partition 6, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.805+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 5.0 in stage 17.0 (TID 132) in 54 ms on 172.21.0.9 (executor 0) (17/50)
[2024-11-13T06:59:03.806+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 8.0 in stage 17.0 (TID 134) (172.21.0.9, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.807+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 4.0 in stage 17.0 (TID 131) in 58 ms on 172.21.0.9 (executor 0) (18/50)
[2024-11-13T06:59:03.835+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_6 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.836+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_8 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.851+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 9.0 in stage 17.0 (TID 135) (172.21.0.9, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.852+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 10.0 in stage 17.0 (TID 136) (172.21.0.9, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.853+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 8.0 in stage 17.0 (TID 134) in 47 ms on 172.21.0.9 (executor 0) (19/50)
[2024-11-13T06:59:03.854+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 6.0 in stage 17.0 (TID 133) in 49 ms on 172.21.0.9 (executor 0) (20/50)
[2024-11-13T06:59:03.886+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_10 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.887+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_9 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.902+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 11.0 in stage 17.0 (TID 137) (172.21.0.9, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.902+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 10.0 in stage 17.0 (TID 136) in 50 ms on 172.21.0.9 (executor 0) (21/50)
[2024-11-13T06:59:03.903+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 12.0 in stage 17.0 (TID 138) (172.21.0.9, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.904+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 9.0 in stage 17.0 (TID 135) in 53 ms on 172.21.0.9 (executor 0) (22/50)
[2024-11-13T06:59:03.932+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_12 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.934+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_11 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.950+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 13.0 in stage 17.0 (TID 139) (172.21.0.9, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.951+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 15.0 in stage 17.0 (TID 140) (172.21.0.9, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.951+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 11.0 in stage 17.0 (TID 137) in 50 ms on 172.21.0.9 (executor 0) (23/50)
[2024-11-13T06:59:03.952+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 12.0 in stage 17.0 (TID 138) in 48 ms on 172.21.0.9 (executor 0) (24/50)
[2024-11-13T06:59:03.979+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_15 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.981+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO BlockManagerInfo: Added rdd_55_13 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:03.996+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 16.0 in stage 17.0 (TID 141) (172.21.0.9, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.996+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 15.0 in stage 17.0 (TID 140) in 46 ms on 172.21.0.9 (executor 0) (25/50)
[2024-11-13T06:59:03.997+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Starting task 17.0 in stage 17.0 (TID 142) (172.21.0.9, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:03.998+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:03 INFO TaskSetManager: Finished task 13.0 in stage 17.0 (TID 139) in 49 ms on 172.21.0.9 (executor 0) (26/50)
[2024-11-13T06:59:04.025+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_16 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.026+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_17 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.043+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 19.0 in stage 17.0 (TID 143) (172.21.0.9, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.044+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 21.0 in stage 17.0 (TID 144) (172.21.0.9, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.045+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 17.0 in stage 17.0 (TID 142) in 47 ms on 172.21.0.9 (executor 0) (27/50)
[2024-11-13T06:59:04.045+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 16.0 in stage 17.0 (TID 141) in 49 ms on 172.21.0.9 (executor 0) (28/50)
[2024-11-13T06:59:04.072+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_19 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.073+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_21 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.089+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 22.0 in stage 17.0 (TID 145) (172.21.0.9, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.089+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 21.0 in stage 17.0 (TID 144) in 46 ms on 172.21.0.9 (executor 0) (29/50)
[2024-11-13T06:59:04.090+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 23.0 in stage 17.0 (TID 146) (172.21.0.9, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.091+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 19.0 in stage 17.0 (TID 143) in 48 ms on 172.21.0.9 (executor 0) (30/50)
[2024-11-13T06:59:04.120+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_23 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.123+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_22 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.136+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 25.0 in stage 17.0 (TID 147) (172.21.0.9, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.137+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 23.0 in stage 17.0 (TID 146) in 47 ms on 172.21.0.9 (executor 0) (31/50)
[2024-11-13T06:59:04.139+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 27.0 in stage 17.0 (TID 148) (172.21.0.9, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.140+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 22.0 in stage 17.0 (TID 145) in 51 ms on 172.21.0.9 (executor 0) (32/50)
[2024-11-13T06:59:04.169+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_25 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.170+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_27 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.187+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 28.0 in stage 17.0 (TID 149) (172.21.0.9, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.188+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 25.0 in stage 17.0 (TID 147) in 52 ms on 172.21.0.9 (executor 0) (33/50)
[2024-11-13T06:59:04.189+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 29.0 in stage 17.0 (TID 150) (172.21.0.9, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.190+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 27.0 in stage 17.0 (TID 148) in 50 ms on 172.21.0.9 (executor 0) (34/50)
[2024-11-13T06:59:04.224+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_28 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.227+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_29 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.241+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 30.0 in stage 17.0 (TID 151) (172.21.0.9, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.241+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 28.0 in stage 17.0 (TID 149) in 54 ms on 172.21.0.9 (executor 0) (35/50)
[2024-11-13T06:59:04.244+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 31.0 in stage 17.0 (TID 152) (172.21.0.9, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.244+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 29.0 in stage 17.0 (TID 150) in 55 ms on 172.21.0.9 (executor 0) (36/50)
[2024-11-13T06:59:04.271+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_30 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.273+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_31 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.288+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 33.0 in stage 17.0 (TID 153) (172.21.0.9, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.289+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 31.0 in stage 17.0 (TID 152) in 46 ms on 172.21.0.9 (executor 0) (37/50)
[2024-11-13T06:59:04.290+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 34.0 in stage 17.0 (TID 154) (172.21.0.9, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.291+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 30.0 in stage 17.0 (TID 151) in 50 ms on 172.21.0.9 (executor 0) (38/50)
[2024-11-13T06:59:04.329+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_34 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.330+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_33 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.345+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 35.0 in stage 17.0 (TID 155) (172.21.0.9, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.346+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 34.0 in stage 17.0 (TID 154) in 57 ms on 172.21.0.9 (executor 0) (39/50)
[2024-11-13T06:59:04.347+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 36.0 in stage 17.0 (TID 156) (172.21.0.9, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.348+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 33.0 in stage 17.0 (TID 153) in 60 ms on 172.21.0.9 (executor 0) (40/50)
[2024-11-13T06:59:04.401+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_35 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.402+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_36 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.431+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 37.0 in stage 17.0 (TID 157) (172.21.0.9, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.432+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 36.0 in stage 17.0 (TID 156) in 85 ms on 172.21.0.9 (executor 0) (41/50)
[2024-11-13T06:59:04.433+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 39.0 in stage 17.0 (TID 158) (172.21.0.9, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.434+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 35.0 in stage 17.0 (TID 155) in 89 ms on 172.21.0.9 (executor 0) (42/50)
[2024-11-13T06:59:04.471+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_39 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.473+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_37 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.489+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 43.0 in stage 17.0 (TID 159) (172.21.0.9, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.491+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 39.0 in stage 17.0 (TID 158) in 57 ms on 172.21.0.9 (executor 0) (43/50)
[2024-11-13T06:59:04.492+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 44.0 in stage 17.0 (TID 160) (172.21.0.9, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.493+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 37.0 in stage 17.0 (TID 157) in 61 ms on 172.21.0.9 (executor 0) (44/50)
[2024-11-13T06:59:04.524+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_43 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.524+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_44 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.543+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 45.0 in stage 17.0 (TID 161) (172.21.0.9, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.544+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 43.0 in stage 17.0 (TID 159) in 54 ms on 172.21.0.9 (executor 0) (45/50)
[2024-11-13T06:59:04.544+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 47.0 in stage 17.0 (TID 162) (172.21.0.9, executor 0, partition 47, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.545+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 44.0 in stage 17.0 (TID 160) in 54 ms on 172.21.0.9 (executor 0) (46/50)
[2024-11-13T06:59:04.576+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_45 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.582+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_47 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.598+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 48.0 in stage 17.0 (TID 163) (172.21.0.9, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.599+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 45.0 in stage 17.0 (TID 161) in 55 ms on 172.21.0.9 (executor 0) (47/50)
[2024-11-13T06:59:04.601+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 49.0 in stage 17.0 (TID 164) (172.21.0.9, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-11-13T06:59:04.602+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 47.0 in stage 17.0 (TID 162) in 58 ms on 172.21.0.9 (executor 0) (48/50)
[2024-11-13T06:59:04.635+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_48 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.639+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added rdd_55_49 in memory on 172.21.0.9:41811 (size: 46.0 B, free: 1048.6 MiB)
[2024-11-13T06:59:04.653+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 48.0 in stage 17.0 (TID 163) in 55 ms on 172.21.0.9 (executor 0) (49/50)
[2024-11-13T06:59:04.656+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 49.0 in stage 17.0 (TID 164) in 55 ms on 172.21.0.9 (executor 0) (50/50)
[2024-11-13T06:59:04.657+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2024-11-13T06:59:04.657+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: ShuffleMapStage 17 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.421 s
[2024-11-13T06:59:04.658+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: looking for newly runnable stages
[2024-11-13T06:59:04.658+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: running: Set()
[2024-11-13T06:59:04.659+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: waiting: Set()
[2024-11-13T06:59:04.659+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: failed: Set()
[2024-11-13T06:59:04.689+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on bdc4422970d0:36285 in memory (size: 123.4 KiB, free: 434.2 MiB)
[2024-11-13T06:59:04.690+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.21.0.9:41811 in memory (size: 123.4 KiB, free: 1048.7 MiB)
[2024-11-13T06:59:04.695+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-11-13T06:59:04.696+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Got job 12 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-11-13T06:59:04.696+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Final stage: ResultStage 20 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-11-13T06:59:04.697+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)
[2024-11-13T06:59:04.697+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Missing parents: List()
[2024-11-13T06:59:04.698+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[61] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-11-13T06:59:04.703+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 452.3 KiB, free 432.8 MiB)
[2024-11-13T06:59:04.708+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 108.2 KiB, free 432.7 MiB)
[2024-11-13T06:59:04.710+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on bdc4422970d0:36285 (size: 108.2 KiB, free: 434.1 MiB)
[2024-11-13T06:59:04.711+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1540
[2024-11-13T06:59:04.711+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[61] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-11-13T06:59:04.712+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2024-11-13T06:59:04.713+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 165) (172.21.0.9, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-11-13T06:59:04.730+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.21.0.9:41811 (size: 108.2 KiB, free: 1048.6 MiB)
[2024-11-13T06:59:04.740+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.21.0.9:58990
[2024-11-13T06:59:04.766+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 165) in 53 ms on 172.21.0.9 (executor 0) (1/1)
[2024-11-13T06:59:04.766+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2024-11-13T06:59:04.767+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: ResultStage 20 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.068 s
[2024-11-13T06:59:04.767+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-11-13T06:59:04.768+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2024-11-13T06:59:04.769+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO DAGScheduler: Job 12 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.072060 s
[2024-11-13T06:59:04.776+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO Snapshot: [tableId=1ebd1207-c4be-4b86-929b-b7bddbdf5b30] DELTA: Done
[2024-11-13T06:59:04.825+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO OptimisticTransaction: [tableId=1ebd1207,txnId=bb5bf5d6] Committed delta #5 to s3a://lakehouse/sliver/credit/_delta_log
[2024-11-13T06:59:04.832+0000] {spark_submit.py:579} INFO - 2024-11-13 06:59:04,831 INFO: Data cleaning and saving process completed successfully.
[2024-11-13T06:59:04.832+0000] {spark_submit.py:579} INFO - 2024-11-13 06:59:04,831 INFO: Data cleaning and saving process completed successfully.
[2024-11-13T06:59:04.833+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-11-13T06:59:04.850+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO SparkUI: Stopped Spark web UI at http://bdc4422970d0:4040
[2024-11-13T06:59:04.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO StandaloneSchedulerBackend: Shutting down all executors
[2024-11-13T06:59:04.856+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2024-11-13T06:59:04.885+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-11-13T06:59:04.912+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO MemoryStore: MemoryStore cleared
[2024-11-13T06:59:04.913+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManager: BlockManager stopped
[2024-11-13T06:59:04.917+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-11-13T06:59:04.924+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-11-13T06:59:04.951+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:04 INFO SparkContext: Successfully stopped SparkContext
[2024-11-13T06:59:05.397+0000] {spark_submit.py:579} INFO - 2024-11-13 06:59:05,397 INFO: Closing down clientserver connection
[2024-11-13T06:59:05.466+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO ShutdownHookManager: Shutdown hook called
[2024-11-13T06:59:05.467+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-31d28d5c-b055-4672-9d13-15d358fbedf7
[2024-11-13T06:59:05.471+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-efb67535-0044-4759-84c8-f5931ded17bb
[2024-11-13T06:59:05.474+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-efb67535-0044-4759-84c8-f5931ded17bb/pyspark-27c45abb-6e0a-46c3-8c91-8781f07b8d49
[2024-11-13T06:59:05.483+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2024-11-13T06:59:05.483+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2024-11-13T06:59:05.484+0000] {spark_submit.py:579} INFO - 24/11/13 06:59:05 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2024-11-13T06:59:05.563+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=test_flowwww, task_id=credits_cleaned, execution_date=20241112T000000, start_date=20241113T065750, end_date=20241113T065905
[2024-11-13T06:59:05.597+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-11-13T06:59:05.613+0000] {taskinstance.py:2776} INFO - 1 downstream tasks scheduled from follow-on schedule check
