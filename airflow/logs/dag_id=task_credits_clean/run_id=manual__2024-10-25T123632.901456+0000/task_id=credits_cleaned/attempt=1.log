[2024-10-25T12:36:34.595+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: task_credits_clean.credits_cleaned manual__2024-10-25T12:36:32.901456+00:00 [queued]>
[2024-10-25T12:36:34.605+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: task_credits_clean.credits_cleaned manual__2024-10-25T12:36:32.901456+00:00 [queued]>
[2024-10-25T12:36:34.606+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2
[2024-10-25T12:36:34.619+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): credits_cleaned> on 2024-10-25 12:36:32.901456+00:00
[2024-10-25T12:36:34.626+0000] {standard_task_runner.py:57} INFO - Started process 7007 to run task
[2024-10-25T12:36:34.628+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'task_credits_clean', 'credits_cleaned', 'manual__2024-10-25T12:36:32.901456+00:00', '--job-id', '164', '--raw', '--subdir', 'DAGS_FOLDER/credits_test.py', '--cfg-path', '/tmp/tmpv3l75ydi']
[2024-10-25T12:36:34.631+0000] {standard_task_runner.py:85} INFO - Job 164: Subtask credits_cleaned
[2024-10-25T12:36:34.679+0000] {task_command.py:415} INFO - Running <TaskInstance: task_credits_clean.credits_cleaned manual__2024-10-25T12:36:32.901456+00:00 [running]> on host 84076e7a47fe
[2024-10-25T12:36:34.763+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='task_credits_clean' AIRFLOW_CTX_TASK_ID='credits_cleaned' AIRFLOW_CTX_EXECUTION_DATE='2024-10-25T12:36:32.901456+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-10-25T12:36:32.901456+00:00'
[2024-10-25T12:36:34.773+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2024-10-25T12:36:34.776+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog --jars /opt/***/jars/hadoop-aws-3.3.4.jar,/opt/***/jars/s3-2.18.41.jar,/opt/***/jars/aws-java-sdk-1.12.367.jar,/opt/***/jars/delta-core_2.12-2.4.0.jar,/opt/***/jars/delta-storage-2.2.0.jar, --packages org.apache.hadoop:hadoop-aws:3.3.4 --num-executors 2 --total-executor-cores 2 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name arrow-spark --deploy-mode client /opt/***/jobs/python/test_Credit.py s3a://lakehouse/bronze/credits.parquet s3a://lakehouse/sliver/credit
[2024-10-25T12:36:34.875+0000] {spark_submit.py:579} INFO - /home/***/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2024-10-25T12:36:36.713+0000] {spark_submit.py:579} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-10-25T12:36:36.828+0000] {spark_submit.py:579} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2024-10-25T12:36:36.829+0000] {spark_submit.py:579} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2024-10-25T12:36:36.834+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2024-10-25T12:36:36.835+0000] {spark_submit.py:579} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-ed945232-9cd2-42ed-991e-f3468dce1269;1.0
[2024-10-25T12:36:36.836+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-10-25T12:36:36.967+0000] {spark_submit.py:579} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in spark-list
[2024-10-25T12:36:36.996+0000] {spark_submit.py:579} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.262 in central
[2024-10-25T12:36:37.019+0000] {spark_submit.py:579} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in spark-list
[2024-10-25T12:36:37.042+0000] {spark_submit.py:579} INFO - :: resolution report :: resolve 199ms :: artifacts dl 8ms
[2024-10-25T12:36:37.043+0000] {spark_submit.py:579} INFO - :: modules in use:
[2024-10-25T12:36:37.043+0000] {spark_submit.py:579} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]
[2024-10-25T12:36:37.044+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from spark-list in [default]
[2024-10-25T12:36:37.045+0000] {spark_submit.py:579} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from spark-list in [default]
[2024-10-25T12:36:37.045+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-10-25T12:36:37.046+0000] {spark_submit.py:579} INFO - |                  |            modules            ||   artifacts   |
[2024-10-25T12:36:37.047+0000] {spark_submit.py:579} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2024-10-25T12:36:37.047+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-10-25T12:36:37.048+0000] {spark_submit.py:579} INFO - |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
[2024-10-25T12:36:37.048+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2024-10-25T12:36:37.049+0000] {spark_submit.py:579} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-ed945232-9cd2-42ed-991e-f3468dce1269
[2024-10-25T12:36:37.049+0000] {spark_submit.py:579} INFO - confs: [default]
[2024-10-25T12:36:37.057+0000] {spark_submit.py:579} INFO - 0 artifacts copied, 3 already retrieved (0kB/8ms)
[2024-10-25T12:36:37.236+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-10-25T12:36:38.457+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:38,456 INFO: Initializing Spark session...
[2024-10-25T12:36:38.619+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SparkContext: Running Spark version 3.4.3
[2024-10-25T12:36:38.650+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceUtils: ==============================================================
[2024-10-25T12:36:38.651+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-10-25T12:36:38.651+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceUtils: ==============================================================
[2024-10-25T12:36:38.652+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SparkContext: Submitted application: CleanCredits
[2024-10-25T12:36:38.674+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 2, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-10-25T12:36:38.687+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceProfile: Limiting resource is cpus at 2 tasks per executor
[2024-10-25T12:36:38.689+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-10-25T12:36:38.739+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SecurityManager: Changing view acls to: ***
[2024-10-25T12:36:38.740+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SecurityManager: Changing modify acls to: ***
[2024-10-25T12:36:38.740+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SecurityManager: Changing view acls groups to:
[2024-10-25T12:36:38.741+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SecurityManager: Changing modify acls groups to:
[2024-10-25T12:36:38.741+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2024-10-25T12:36:39.046+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO Utils: Successfully started service 'sparkDriver' on port 44955.
[2024-10-25T12:36:39.081+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkEnv: Registering MapOutputTracker
[2024-10-25T12:36:39.115+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkEnv: Registering BlockManagerMaster
[2024-10-25T12:36:39.132+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-10-25T12:36:39.133+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-10-25T12:36:39.137+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-10-25T12:36:39.155+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-67a4b9ff-f8e2-4cec-b28f-b29363880373
[2024-10-25T12:36:39.169+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-10-25T12:36:39.186+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-10-25T12:36:39.321+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-10-25T12:36:39.392+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-10-25T12:36:39.435+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///opt/***/jars/hadoop-aws-3.3.4.jar at spark://84076e7a47fe:44955/jars/hadoop-aws-3.3.4.jar with timestamp 1729859798610
[2024-10-25T12:36:39.436+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///opt/***/jars/s3-2.18.41.jar at spark://84076e7a47fe:44955/jars/s3-2.18.41.jar with timestamp 1729859798610
[2024-10-25T12:36:39.438+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///opt/***/jars/aws-java-sdk-1.12.367.jar at spark://84076e7a47fe:44955/jars/aws-java-sdk-1.12.367.jar with timestamp 1729859798610
[2024-10-25T12:36:39.439+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///opt/***/jars/delta-core_2.12-2.4.0.jar at spark://84076e7a47fe:44955/jars/delta-core_2.12-2.4.0.jar with timestamp 1729859798610
[2024-10-25T12:36:39.440+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///opt/***/jars/delta-storage-2.2.0.jar at spark://84076e7a47fe:44955/jars/delta-storage-2.2.0.jar with timestamp 1729859798610
[2024-10-25T12:36:39.441+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://84076e7a47fe:44955/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1729859798610
[2024-10-25T12:36:39.441+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://84076e7a47fe:44955/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1729859798610
[2024-10-25T12:36:39.441+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://84076e7a47fe:44955/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1729859798610
[2024-10-25T12:36:39.443+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://84076e7a47fe:44955/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1729859798610
[2024-10-25T12:36:39.444+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-ac63235e-418e-4c0e-85e8-07e5750958c2/userFiles-b6c3b31f-bb7a-4726-8f69-ed115fc51ab2/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2024-10-25T12:36:39.456+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar at spark://84076e7a47fe:44955/files/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar with timestamp 1729859798610
[2024-10-25T12:36:39.456+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar to /tmp/spark-ac63235e-418e-4c0e-85e8-07e5750958c2/userFiles-b6c3b31f-bb7a-4726-8f69-ed115fc51ab2/com.amazonaws_aws-java-sdk-bundle-1.12.262.jar
[2024-10-25T12:36:39.811+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://84076e7a47fe:44955/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1729859798610
[2024-10-25T12:36:39.811+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-ac63235e-418e-4c0e-85e8-07e5750958c2/userFiles-b6c3b31f-bb7a-4726-8f69-ed115fc51ab2/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2024-10-25T12:36:39.908+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2024-10-25T12:36:39.958+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:39 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 29 ms (0 ms spent in bootstraps)
[2024-10-25T12:36:40.065+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241025123640-0014
[2024-10-25T12:36:40.066+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241025123640-0014/0 on worker-20241025115841-172.18.0.5-46301 (172.18.0.5:46301) with 2 core(s)
[2024-10-25T12:36:40.068+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20241025123640-0014/0 on hostPort 172.18.0.5:46301 with 2 core(s), 2.0 GiB RAM
[2024-10-25T12:36:40.074+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44219.
[2024-10-25T12:36:40.074+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO NettyBlockTransferService: Server created on 84076e7a47fe:44219
[2024-10-25T12:36:40.076+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-10-25T12:36:40.084+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 84076e7a47fe, 44219, None)
[2024-10-25T12:36:40.088+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO BlockManagerMasterEndpoint: Registering block manager 84076e7a47fe:44219 with 434.4 MiB RAM, BlockManagerId(driver, 84076e7a47fe, 44219, None)
[2024-10-25T12:36:40.091+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 84076e7a47fe, 44219, None)
[2024-10-25T12:36:40.092+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 84076e7a47fe, 44219, None)
[2024-10-25T12:36:40.290+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2024-10-25T12:36:40.387+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241025123640-0014/0 is now RUNNING
[2024-10-25T12:36:40.590+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:40,590 INFO: Spark session initialized successfully.
[2024-10-25T12:36:40.591+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:40,590 INFO: Reading data from s3a://lakehouse/bronze/credits.parquet
[2024-10-25T12:36:40.598+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-10-25T12:36:40.812+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2024-10-25T12:36:40.835+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2024-10-25T12:36:40.836+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:40 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2024-10-25T12:36:41.941+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:41 INFO SharedState: Warehouse path is 's3a://lakehouse/'.
[2024-10-25T12:36:43.125+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:43 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:35124) with ID 0,  ResourceProfileId 0
[2024-10-25T12:36:43.219+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:43 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.5:39707 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.5, 39707, None)
[2024-10-25T12:36:43.760+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:43 INFO InMemoryFileIndex: It took 116 ms to list leaf files for 1 paths.
[2024-10-25T12:36:45.816+0000] {spark_submit.py:579} INFO - root
[2024-10-25T12:36:45.816+0000] {spark_submit.py:579} INFO - |-- cast: string (nullable = true)
[2024-10-25T12:36:45.816+0000] {spark_submit.py:579} INFO - |-- crew: string (nullable = true)
[2024-10-25T12:36:45.817+0000] {spark_submit.py:579} INFO - |-- id: long (nullable = true)
[2024-10-25T12:36:45.817+0000] {spark_submit.py:579} INFO - 
[2024-10-25T12:36:45.818+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:45,815 INFO: None
[2024-10-25T12:36:45.818+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:45,816 INFO: Cleaning data...
[2024-10-25T12:36:45.885+0000] {spark_submit.py:579} INFO - 2024-10-25 12:36:45,885 INFO: Saving cleaned data to s3a://lakehouse/sliver/credit
[2024-10-25T12:36:46.571+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:46 INFO DeltaLog: Loading version 1.
[2024-10-25T12:36:47.362+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:47 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 2, totalFileSize: 3428)
[2024-10-25T12:36:47.669+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:47 INFO FileSourceStrategy: Pushed Filters:
[2024-10-25T12:36:47.670+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:47 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-25T12:36:48.299+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO CodeGenerator: Code generated in 387.206725 ms
[2024-10-25T12:36:48.359+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 205.0 KiB, free 434.2 MiB)
[2024-10-25T12:36:48.419+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.2 MiB)
[2024-10-25T12:36:48.423+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 84076e7a47fe:44219 (size: 36.0 KiB, free: 434.4 MiB)
[2024-10-25T12:36:48.429+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO SparkContext: Created broadcast 0 from toString at String.java:2951
[2024-10-25T12:36:48.455+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196018 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-25T12:36:48.597+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO SparkContext: Starting job: toString at String.java:2951
[2024-10-25T12:36:48.615+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Got job 0 (toString at String.java:2951) with 2 output partitions
[2024-10-25T12:36:48.616+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Final stage: ResultStage 0 (toString at String.java:2951)
[2024-10-25T12:36:48.617+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Parents of final stage: List()
[2024-10-25T12:36:48.619+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:36:48.624+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at toString at String.java:2951), which has no missing parents
[2024-10-25T12:36:48.646+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 50.4 KiB, free 434.1 MiB)
[2024-10-25T12:36:48.656+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.1 MiB)
[2024-10-25T12:36:48.657+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 84076e7a47fe:44219 (size: 15.5 KiB, free: 434.3 MiB)
[2024-10-25T12:36:48.658+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:36:48.684+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:36:48.686+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
[2024-10-25T12:36:48.727+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7950 bytes)
[2024-10-25T12:36:48.731+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7950 bytes)
[2024-10-25T12:36:48.996+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.5:39707 (size: 15.5 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:50.167+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.5:39707 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:36:51.460+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2744 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:36:51.462+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2732 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:36:51.463+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-10-25T12:36:51.470+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO DAGScheduler: ResultStage 0 (toString at String.java:2951) finished in 2.830 s
[2024-10-25T12:36:51.475+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:36:51.476+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2024-10-25T12:36:51.479+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO DAGScheduler: Job 0 finished: toString at String.java:2951, took 2.881947 s
[2024-10-25T12:36:51.574+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO CodeGenerator: Code generated in 56.952852 ms
[2024-10-25T12:36:51.584+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO Snapshot: [tableId=495f553b-fed0-4fb1-a709-3b5952d20a68] Created snapshot Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=1, metadata=Metadata(27869bd8-0dad-499e-87a1-62767741a16b,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":"string","nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}}]},List(),Map(),Some(1729859584322)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,1,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=1749; replication=1; blocksize=33554432; modification_time=1729859601578; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=0fcd107ecb55a2da324d6d2253a4bc8e versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1679; replication=1; blocksize=33554432; modification_time=1729859662749; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=72195ab0cb0a49898cfc56a271971e2b versionId=null),None,1729859662749), checksumOpt=None)
[2024-10-25T12:36:51.729+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 84076e7a47fe:44219 in memory (size: 15.5 KiB, free: 434.4 MiB)
[2024-10-25T12:36:51.738+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.5:39707 in memory (size: 15.5 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:52.077+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(cast)
[2024-10-25T12:36:52.078+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(cast#0)
[2024-10-25T12:36:52.204+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 84076e7a47fe:44219 in memory (size: 36.0 KiB, free: 434.4 MiB)
[2024-10-25T12:36:52.209+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.5:39707 in memory (size: 36.0 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:52.239+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2024-10-25T12:36:52.338+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-10-25T12:36:52.404+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO CodeGenerator: Code generated in 35.997831 ms
[2024-10-25T12:36:52.414+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 206.6 KiB, free 434.2 MiB)
[2024-10-25T12:36:52.429+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 434.2 MiB)
[2024-10-25T12:36:52.431+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 84076e7a47fe:44219 (size: 36.6 KiB, free: 434.4 MiB)
[2024-10-25T12:36:52.433+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO SparkContext: Created broadcast 2 from save at NativeMethodAccessorImpl.java:0
[2024-10-25T12:36:52.445+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 36317959 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-25T12:36:52.536+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Registering RDD 7 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-10-25T12:36:52.540+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Got map stage job 1 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-10-25T12:36:52.541+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (save at NativeMethodAccessorImpl.java:0)
[2024-10-25T12:36:52.541+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Parents of final stage: List()
[2024-10-25T12:36:52.543+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:36:52.546+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-25T12:36:52.561+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 32.7 KiB, free 434.1 MiB)
[2024-10-25T12:36:52.570+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 14.4 KiB, free 434.1 MiB)
[2024-10-25T12:36:52.572+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 84076e7a47fe:44219 (size: 14.4 KiB, free: 434.4 MiB)
[2024-10-25T12:36:52.573+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:36:52.575+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:36:52.576+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0
[2024-10-25T12:36:52.579+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7911 bytes)
[2024-10-25T12:36:52.580+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7911 bytes)
[2024-10-25T12:36:52.631+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.18.0.5:39707 (size: 14.4 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:53.017+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.18.0.5:39707 (size: 36.6 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:53.608+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:53 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1028 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:36:55.668+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 3091 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:36:55.669+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-10-25T12:36:55.670+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: ShuffleMapStage 1 (save at NativeMethodAccessorImpl.java:0) finished in 3.121 s
[2024-10-25T12:36:55.671+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: looking for newly runnable stages
[2024-10-25T12:36:55.671+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: running: Set()
[2024-10-25T12:36:55.672+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: waiting: Set()
[2024-10-25T12:36:55.673+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: failed: Set()
[2024-10-25T12:36:55.692+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 40572466, minimum partition size: 1048576
[2024-10-25T12:36:55.717+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2024-10-25T12:36:55.733+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO CodeGenerator: Code generated in 12.061373 ms
[2024-10-25T12:36:55.822+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
[2024-10-25T12:36:55.825+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2024-10-25T12:36:55.825+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
[2024-10-25T12:36:55.826+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2024-10-25T12:36:55.826+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:36:55.827+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-10-25T12:36:55.870+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 84076e7a47fe:44219 in memory (size: 14.4 KiB, free: 434.4 MiB)
[2024-10-25T12:36:55.873+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 357.6 KiB, free 433.8 MiB)
[2024-10-25T12:36:55.873+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.18.0.5:39707 in memory (size: 14.4 KiB, free: 1048.8 MiB)
[2024-10-25T12:36:55.877+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 130.7 KiB, free 433.7 MiB)
[2024-10-25T12:36:55.879+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 84076e7a47fe:44219 (size: 130.7 KiB, free: 434.2 MiB)
[2024-10-25T12:36:55.880+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:36:55.881+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:36:55.882+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0
[2024-10-25T12:36:55.908+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-10-25T12:36:55.909+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7367 bytes)
[2024-10-25T12:36:55.933+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.18.0.5:39707 (size: 130.7 KiB, free: 1048.6 MiB)
[2024-10-25T12:36:56.135+0000] {spark_submit.py:579} INFO - 24/10/25 12:36:56 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.18.0.5:35124
[2024-10-25T12:37:00.797+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 4910 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:37:00.798+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 4889 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:37:00.798+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2024-10-25T12:37:00.801+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 4.957 s
[2024-10-25T12:37:00.801+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:37:00.802+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2024-10-25T12:37:00.803+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 4.979111 s
[2024-10-25T12:37:00.804+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO FileFormatWriter: Start to commit write Job 59db448d-2a11-48cc-8df3-087302a4ff77.
[2024-10-25T12:37:00.806+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO FileFormatWriter: Write Job 59db448d-2a11-48cc-8df3-087302a4ff77 committed. Elapsed time: 0 ms.
[2024-10-25T12:37:00.810+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO FileFormatWriter: Finished processing stats for write job 59db448d-2a11-48cc-8df3-087302a4ff77.
[2024-10-25T12:37:00.829+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO Snapshot: [tableId=27869bd8-0dad-499e-87a1-62767741a16b] DELTA: Compute snapshot for version: 1
[2024-10-25T12:37:00.843+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 204.6 KiB, free 433.5 MiB)
[2024-10-25T12:37:00.859+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 84076e7a47fe:44219 in memory (size: 130.7 KiB, free: 434.4 MiB)
[2024-10-25T12:37:00.863+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.9 MiB)
[2024-10-25T12:37:00.864+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.18.0.5:39707 in memory (size: 130.7 KiB, free: 1048.8 MiB)
[2024-10-25T12:37:00.866+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 84076e7a47fe:44219 (size: 36.0 KiB, free: 434.3 MiB)
[2024-10-25T12:37:00.868+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:00 INFO SparkContext: Created broadcast 5 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:01.520+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO FileSourceStrategy: Pushed Filters:
[2024-10-25T12:37:01.520+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-25T12:37:01.544+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2024-10-25T12:37:01.743+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO CodeGenerator: Code generated in 123.579303 ms
[2024-10-25T12:37:01.747+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 205.0 KiB, free 433.7 MiB)
[2024-10-25T12:37:01.762+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.7 MiB)
[2024-10-25T12:37:01.765+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 84076e7a47fe:44219 (size: 36.0 KiB, free: 434.3 MiB)
[2024-10-25T12:37:01.767+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO SparkContext: Created broadcast 6 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:01.769+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4196018 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-25T12:37:01.807+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Registering RDD 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 1
[2024-10-25T12:37:01.807+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Got map stage job 3 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-10-25T12:37:01.808+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Final stage: ShuffleMapStage 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:01.808+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Parents of final stage: List()
[2024-10-25T12:37:01.811+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:01.812+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:01.826+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 138.3 KiB, free 433.6 MiB)
[2024-10-25T12:37:01.834+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.5 MiB)
[2024-10-25T12:37:01.835+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 84076e7a47fe:44219 (size: 39.0 KiB, free: 434.3 MiB)
[2024-10-25T12:37:01.836+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:01.837+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:37:01.838+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0
[2024-10-25T12:37:01.839+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7939 bytes)
[2024-10-25T12:37:01.840+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7939 bytes)
[2024-10-25T12:37:01.869+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:01 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.18.0.5:39707 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:02.179+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.18.0.5:39707 (size: 36.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:02.235+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 396 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:37:02.240+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 399 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:37:02.240+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2024-10-25T12:37:02.241+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO DAGScheduler: ShuffleMapStage 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.427 s
[2024-10-25T12:37:02.242+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO DAGScheduler: looking for newly runnable stages
[2024-10-25T12:37:02.243+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO DAGScheduler: running: Set()
[2024-10-25T12:37:02.243+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO DAGScheduler: waiting: Set()
[2024-10-25T12:37:02.244+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO DAGScheduler: failed: Set()
[2024-10-25T12:37:02.366+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 84076e7a47fe:44219 in memory (size: 39.0 KiB, free: 434.3 MiB)
[2024-10-25T12:37:02.369+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.18.0.5:39707 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:02.696+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 19938 bytes
[2024-10-25T12:37:02.697+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO CodeGenerator: Code generated in 254.227541 ms
[2024-10-25T12:37:02.780+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:02 INFO CodeGenerator: Code generated in 51.739886 ms
[2024-10-25T12:37:03.147+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO CodeGenerator: Code generated in 56.583214 ms
[2024-10-25T12:37:03.155+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Registering RDD 23 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 2
[2024-10-25T12:37:03.156+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Got map stage job 4 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-10-25T12:37:03.156+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Final stage: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:03.157+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[2024-10-25T12:37:03.165+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:03.166+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[23] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:03.292+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 517.1 KiB, free 433.2 MiB)
[2024-10-25T12:37:03.295+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 123.3 KiB, free 433.1 MiB)
[2024-10-25T12:37:03.297+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 84076e7a47fe:44219 (size: 123.3 KiB, free: 434.2 MiB)
[2024-10-25T12:37:03.298+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:03.299+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[23] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-10-25T12:37:03.300+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 50 tasks resource profile 0
[2024-10-25T12:37:03.302+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 8) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:03.303+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO TaskSetManager: Starting task 6.0 in stage 6.0 (TID 9) (172.18.0.5, executor 0, partition 6, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:03.332+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.18.0.5:39707 (size: 123.3 KiB, free: 1048.6 MiB)
[2024-10-25T12:37:03.474+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.18.0.5:35124
[2024-10-25T12:37:03.980+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO BlockManagerInfo: Added rdd_20_6 in memory on 172.18.0.5:39707 (size: 303.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:03.981+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:03 INFO BlockManagerInfo: Added rdd_20_1 in memory on 172.18.0.5:39707 (size: 302.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.222+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 42.0 in stage 6.0 (TID 10) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.224+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 8) in 921 ms on 172.18.0.5 (executor 0) (1/50)
[2024-10-25T12:37:04.225+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 47.0 in stage 6.0 (TID 11) (172.18.0.5, executor 0, partition 47, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.226+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 6.0 in stage 6.0 (TID 9) in 924 ms on 172.18.0.5 (executor 0) (2/50)
[2024-10-25T12:37:04.318+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_47 in memory on 172.18.0.5:39707 (size: 482.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.377+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.378+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 47.0 in stage 6.0 (TID 11) in 153 ms on 172.18.0.5 (executor 0) (3/50)
[2024-10-25T12:37:04.452+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_0 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.495+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 13) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.496+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 119 ms on 172.18.0.5 (executor 0) (4/50)
[2024-10-25T12:37:04.579+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_2 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.635+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_42 in memory on 172.18.0.5:39707 (size: 763.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.658+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 14) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.659+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 13) in 165 ms on 172.18.0.5 (executor 0) (5/50)
[2024-10-25T12:37:04.704+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 15) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.705+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 42.0 in stage 6.0 (TID 10) in 482 ms on 172.18.0.5 (executor 0) (6/50)
[2024-10-25T12:37:04.753+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_3 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.777+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_4 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.796+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 16) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.797+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 14) in 139 ms on 172.18.0.5 (executor 0) (7/50)
[2024-10-25T12:37:04.819+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 7.0 in stage 6.0 (TID 17) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.820+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 15) in 117 ms on 172.18.0.5 (executor 0) (8/50)
[2024-10-25T12:37:04.885+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_5 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.907+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO BlockManagerInfo: Added rdd_20_7 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:04.933+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 8.0 in stage 6.0 (TID 18) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.935+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 16) in 139 ms on 172.18.0.5 (executor 0) (9/50)
[2024-10-25T12:37:04.961+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Starting task 9.0 in stage 6.0 (TID 19) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:04.963+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:04 INFO TaskSetManager: Finished task 7.0 in stage 6.0 (TID 17) in 144 ms on 172.18.0.5 (executor 0) (10/50)
[2024-10-25T12:37:05.031+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_8 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.050+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_9 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.067+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 10.0 in stage 6.0 (TID 20) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.068+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 8.0 in stage 6.0 (TID 18) in 135 ms on 172.18.0.5 (executor 0) (11/50)
[2024-10-25T12:37:05.090+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 11.0 in stage 6.0 (TID 21) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.092+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 9.0 in stage 6.0 (TID 19) in 131 ms on 172.18.0.5 (executor 0) (12/50)
[2024-10-25T12:37:05.132+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_10 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.165+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_11 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.170+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 12.0 in stage 6.0 (TID 22) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.171+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 10.0 in stage 6.0 (TID 20) in 105 ms on 172.18.0.5 (executor 0) (13/50)
[2024-10-25T12:37:05.207+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 13.0 in stage 6.0 (TID 23) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.208+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 11.0 in stage 6.0 (TID 21) in 117 ms on 172.18.0.5 (executor 0) (14/50)
[2024-10-25T12:37:05.234+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_12 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.276+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 14.0 in stage 6.0 (TID 24) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.277+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 12.0 in stage 6.0 (TID 22) in 107 ms on 172.18.0.5 (executor 0) (15/50)
[2024-10-25T12:37:05.278+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_13 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.321+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 15.0 in stage 6.0 (TID 25) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.322+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 13.0 in stage 6.0 (TID 23) in 115 ms on 172.18.0.5 (executor 0) (16/50)
[2024-10-25T12:37:05.349+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_14 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.388+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 16.0 in stage 6.0 (TID 26) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.388+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 14.0 in stage 6.0 (TID 24) in 113 ms on 172.18.0.5 (executor 0) (17/50)
[2024-10-25T12:37:05.398+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_15 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.435+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 17.0 in stage 6.0 (TID 27) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.436+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 15.0 in stage 6.0 (TID 25) in 115 ms on 172.18.0.5 (executor 0) (18/50)
[2024-10-25T12:37:05.455+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_16 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.484+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 18.0 in stage 6.0 (TID 28) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.485+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 16.0 in stage 6.0 (TID 26) in 98 ms on 172.18.0.5 (executor 0) (19/50)
[2024-10-25T12:37:05.497+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_17 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.538+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 19.0 in stage 6.0 (TID 29) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.539+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 17.0 in stage 6.0 (TID 27) in 104 ms on 172.18.0.5 (executor 0) (20/50)
[2024-10-25T12:37:05.547+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_18 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.588+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 20.0 in stage 6.0 (TID 30) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.589+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 18.0 in stage 6.0 (TID 28) in 104 ms on 172.18.0.5 (executor 0) (21/50)
[2024-10-25T12:37:05.613+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_19 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.641+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 21.0 in stage 6.0 (TID 31) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.642+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_20 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.642+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 19.0 in stage 6.0 (TID 29) in 104 ms on 172.18.0.5 (executor 0) (22/50)
[2024-10-25T12:37:05.668+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 22.0 in stage 6.0 (TID 32) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.669+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 20.0 in stage 6.0 (TID 30) in 82 ms on 172.18.0.5 (executor 0) (23/50)
[2024-10-25T12:37:05.696+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_21 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.726+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 23.0 in stage 6.0 (TID 33) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.727+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_22 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.728+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 21.0 in stage 6.0 (TID 31) in 86 ms on 172.18.0.5 (executor 0) (24/50)
[2024-10-25T12:37:05.756+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 24.0 in stage 6.0 (TID 34) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.757+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 22.0 in stage 6.0 (TID 32) in 89 ms on 172.18.0.5 (executor 0) (25/50)
[2024-10-25T12:37:05.780+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_23 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.807+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 25.0 in stage 6.0 (TID 35) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.808+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 23.0 in stage 6.0 (TID 33) in 83 ms on 172.18.0.5 (executor 0) (26/50)
[2024-10-25T12:37:05.809+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_24 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.841+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 26.0 in stage 6.0 (TID 36) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.842+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 24.0 in stage 6.0 (TID 34) in 86 ms on 172.18.0.5 (executor 0) (27/50)
[2024-10-25T12:37:05.869+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_25 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.900+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 27.0 in stage 6.0 (TID 37) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.900+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 25.0 in stage 6.0 (TID 35) in 93 ms on 172.18.0.5 (executor 0) (28/50)
[2024-10-25T12:37:05.902+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_26 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.932+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Starting task 28.0 in stage 6.0 (TID 38) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:05.934+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO TaskSetManager: Finished task 26.0 in stage 6.0 (TID 36) in 93 ms on 172.18.0.5 (executor 0) (29/50)
[2024-10-25T12:37:05.959+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_27 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:05.997+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:05 INFO BlockManagerInfo: Added rdd_20_28 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.005+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 29.0 in stage 6.0 (TID 39) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.006+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 27.0 in stage 6.0 (TID 37) in 107 ms on 172.18.0.5 (executor 0) (30/50)
[2024-10-25T12:37:06.032+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 30.0 in stage 6.0 (TID 40) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.033+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 28.0 in stage 6.0 (TID 38) in 100 ms on 172.18.0.5 (executor 0) (31/50)
[2024-10-25T12:37:06.073+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_29 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.100+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_30 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.111+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 31.0 in stage 6.0 (TID 41) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.112+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 29.0 in stage 6.0 (TID 39) in 106 ms on 172.18.0.5 (executor 0) (32/50)
[2024-10-25T12:37:06.135+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 32.0 in stage 6.0 (TID 42) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.136+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 30.0 in stage 6.0 (TID 40) in 105 ms on 172.18.0.5 (executor 0) (33/50)
[2024-10-25T12:37:06.187+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_31 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.206+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_32 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.227+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 33.0 in stage 6.0 (TID 43) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.229+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 31.0 in stage 6.0 (TID 41) in 118 ms on 172.18.0.5 (executor 0) (34/50)
[2024-10-25T12:37:06.250+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 34.0 in stage 6.0 (TID 44) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.251+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 32.0 in stage 6.0 (TID 42) in 116 ms on 172.18.0.5 (executor 0) (35/50)
[2024-10-25T12:37:06.287+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_33 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.317+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_34 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.322+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 35.0 in stage 6.0 (TID 45) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.323+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 33.0 in stage 6.0 (TID 43) in 96 ms on 172.18.0.5 (executor 0) (36/50)
[2024-10-25T12:37:06.361+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 36.0 in stage 6.0 (TID 46) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.362+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 34.0 in stage 6.0 (TID 44) in 113 ms on 172.18.0.5 (executor 0) (37/50)
[2024-10-25T12:37:06.389+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_35 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.417+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 37.0 in stage 6.0 (TID 47) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.418+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_36 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.418+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 35.0 in stage 6.0 (TID 45) in 96 ms on 172.18.0.5 (executor 0) (38/50)
[2024-10-25T12:37:06.447+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 38.0 in stage 6.0 (TID 48) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.448+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 36.0 in stage 6.0 (TID 46) in 87 ms on 172.18.0.5 (executor 0) (39/50)
[2024-10-25T12:37:06.476+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_37 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.508+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 39.0 in stage 6.0 (TID 49) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.510+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 37.0 in stage 6.0 (TID 47) in 93 ms on 172.18.0.5 (executor 0) (40/50)
[2024-10-25T12:37:06.523+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_38 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.563+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 40.0 in stage 6.0 (TID 50) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.565+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 38.0 in stage 6.0 (TID 48) in 118 ms on 172.18.0.5 (executor 0) (41/50)
[2024-10-25T12:37:06.574+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_39 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.612+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 41.0 in stage 6.0 (TID 51) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.613+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 39.0 in stage 6.0 (TID 49) in 105 ms on 172.18.0.5 (executor 0) (42/50)
[2024-10-25T12:37:06.630+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_40 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.674+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 43.0 in stage 6.0 (TID 52) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.675+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 40.0 in stage 6.0 (TID 50) in 111 ms on 172.18.0.5 (executor 0) (43/50)
[2024-10-25T12:37:06.690+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_41 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.725+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 44.0 in stage 6.0 (TID 53) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.726+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 41.0 in stage 6.0 (TID 51) in 114 ms on 172.18.0.5 (executor 0) (44/50)
[2024-10-25T12:37:06.734+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_43 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.770+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 45.0 in stage 6.0 (TID 54) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.771+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 43.0 in stage 6.0 (TID 52) in 97 ms on 172.18.0.5 (executor 0) (45/50)
[2024-10-25T12:37:06.785+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_44 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.827+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 46.0 in stage 6.0 (TID 55) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.828+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 44.0 in stage 6.0 (TID 53) in 102 ms on 172.18.0.5 (executor 0) (46/50)
[2024-10-25T12:37:06.841+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_45 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.891+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 48.0 in stage 6.0 (TID 56) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.892+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 45.0 in stage 6.0 (TID 54) in 123 ms on 172.18.0.5 (executor 0) (47/50)
[2024-10-25T12:37:06.916+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_46 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:06.969+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Starting task 49.0 in stage 6.0 (TID 57) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:06.970+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO TaskSetManager: Finished task 46.0 in stage 6.0 (TID 55) in 143 ms on 172.18.0.5 (executor 0) (48/50)
[2024-10-25T12:37:06.976+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:06 INFO BlockManagerInfo: Added rdd_20_48 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:07.010+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSetManager: Finished task 48.0 in stage 6.0 (TID 56) in 119 ms on 172.18.0.5 (executor 0) (49/50)
[2024-10-25T12:37:07.023+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Added rdd_20_49 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:07.051+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSetManager: Finished task 49.0 in stage 6.0 (TID 57) in 83 ms on 172.18.0.5 (executor 0) (50/50)
[2024-10-25T12:37:07.052+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2024-10-25T12:37:07.052+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: ShuffleMapStage 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 3.880 s
[2024-10-25T12:37:07.053+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: looking for newly runnable stages
[2024-10-25T12:37:07.054+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: running: Set()
[2024-10-25T12:37:07.055+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: waiting: Set()
[2024-10-25T12:37:07.055+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: failed: Set()
[2024-10-25T12:37:07.094+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:07.096+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Got job 5 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-10-25T12:37:07.097+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Final stage: ResultStage 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:07.098+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2024-10-25T12:37:07.099+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:07.100+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[26] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:07.109+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 452.2 KiB, free 432.6 MiB)
[2024-10-25T12:37:07.114+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 432.5 MiB)
[2024-10-25T12:37:07.116+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 84076e7a47fe:44219 (size: 108.1 KiB, free: 434.1 MiB)
[2024-10-25T12:37:07.117+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:07.118+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[26] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-10-25T12:37:07.119+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2024-10-25T12:37:07.121+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 58) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-10-25T12:37:07.146+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 84076e7a47fe:44219 in memory (size: 123.3 KiB, free: 434.2 MiB)
[2024-10-25T12:37:07.154+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.18.0.5:39707 in memory (size: 123.3 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:07.158+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.18.0.5:39707 (size: 108.1 KiB, free: 1048.6 MiB)
[2024-10-25T12:37:07.178+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.18.0.5:35124
[2024-10-25T12:37:07.280+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 58) in 160 ms on 172.18.0.5 (executor 0) (1/1)
[2024-10-25T12:37:07.281+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2024-10-25T12:37:07.282+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: ResultStage 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.180 s
[2024-10-25T12:37:07.283+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:37:07.284+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2024-10-25T12:37:07.285+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Job 5 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.188607 s
[2024-10-25T12:37:07.352+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO CodeGenerator: Code generated in 43.435992 ms
[2024-10-25T12:37:07.355+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO Snapshot: [tableId=27869bd8-0dad-499e-87a1-62767741a16b] DELTA: Done
[2024-10-25T12:37:07.612+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 84076e7a47fe:44219 in memory (size: 108.1 KiB, free: 434.3 MiB)
[2024-10-25T12:37:07.615+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.18.0.5:39707 in memory (size: 108.1 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:07.945+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO CodeGenerator: Code generated in 208.391333 ms
[2024-10-25T12:37:07.975+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:07.977+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Got job 6 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-10-25T12:37:07.978+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Final stage: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:07.978+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2024-10-25T12:37:07.979+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:07.981+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:07.988+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 592.1 KiB, free 433.1 MiB)
[2024-10-25T12:37:07.992+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 138.5 KiB, free 433.0 MiB)
[2024-10-25T12:37:07.995+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 84076e7a47fe:44219 (size: 138.5 KiB, free: 434.2 MiB)
[2024-10-25T12:37:07.996+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:07.997+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO DAGScheduler: Submitting 50 missing tasks from ResultStage 11 (MapPartitionsRDD[28] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-10-25T12:37:07.998+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:07 INFO TaskSchedulerImpl: Adding task set 11.0 with 50 tasks resource profile 0
[2024-10-25T12:37:08.003+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 59) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.004+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 60) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.020+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.18.0.5:39707 (size: 138.5 KiB, free: 1048.6 MiB)
[2024-10-25T12:37:08.235+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 61) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.236+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 60) in 233 ms on 172.18.0.5 (executor 0) (1/50)
[2024-10-25T12:37:08.238+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 62) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.239+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 59) in 236 ms on 172.18.0.5 (executor 0) (2/50)
[2024-10-25T12:37:08.258+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 63) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.260+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 64) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.261+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 62) in 24 ms on 172.18.0.5 (executor 0) (3/50)
[2024-10-25T12:37:08.262+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 61) in 26 ms on 172.18.0.5 (executor 0) (4/50)
[2024-10-25T12:37:08.285+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 65) (172.18.0.5, executor 0, partition 6, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.286+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 63) in 29 ms on 172.18.0.5 (executor 0) (5/50)
[2024-10-25T12:37:08.288+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 66) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.289+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 64) in 30 ms on 172.18.0.5 (executor 0) (6/50)
[2024-10-25T12:37:08.309+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 8.0 in stage 11.0 (TID 67) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.311+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 65) in 25 ms on 172.18.0.5 (executor 0) (7/50)
[2024-10-25T12:37:08.313+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 9.0 in stage 11.0 (TID 68) (172.18.0.5, executor 0, partition 9, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.314+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 66) in 26 ms on 172.18.0.5 (executor 0) (8/50)
[2024-10-25T12:37:08.336+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 10.0 in stage 11.0 (TID 69) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.337+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 8.0 in stage 11.0 (TID 67) in 28 ms on 172.18.0.5 (executor 0) (9/50)
[2024-10-25T12:37:08.339+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 11.0 in stage 11.0 (TID 70) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.340+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 9.0 in stage 11.0 (TID 68) in 28 ms on 172.18.0.5 (executor 0) (10/50)
[2024-10-25T12:37:08.362+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 12.0 in stage 11.0 (TID 71) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.364+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 10.0 in stage 11.0 (TID 69) in 28 ms on 172.18.0.5 (executor 0) (11/50)
[2024-10-25T12:37:08.365+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 13.0 in stage 11.0 (TID 72) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.366+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 11.0 in stage 11.0 (TID 70) in 28 ms on 172.18.0.5 (executor 0) (12/50)
[2024-10-25T12:37:08.385+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 14.0 in stage 11.0 (TID 73) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.386+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 12.0 in stage 11.0 (TID 71) in 24 ms on 172.18.0.5 (executor 0) (13/50)
[2024-10-25T12:37:08.397+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 15.0 in stage 11.0 (TID 74) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.398+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 13.0 in stage 11.0 (TID 72) in 33 ms on 172.18.0.5 (executor 0) (14/50)
[2024-10-25T12:37:08.409+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 16.0 in stage 11.0 (TID 75) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.411+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 14.0 in stage 11.0 (TID 73) in 25 ms on 172.18.0.5 (executor 0) (15/50)
[2024-10-25T12:37:08.417+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 17.0 in stage 11.0 (TID 76) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.418+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 15.0 in stage 11.0 (TID 74) in 21 ms on 172.18.0.5 (executor 0) (16/50)
[2024-10-25T12:37:08.428+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 18.0 in stage 11.0 (TID 77) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.429+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 16.0 in stage 11.0 (TID 75) in 20 ms on 172.18.0.5 (executor 0) (17/50)
[2024-10-25T12:37:08.442+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 19.0 in stage 11.0 (TID 78) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.443+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 17.0 in stage 11.0 (TID 76) in 26 ms on 172.18.0.5 (executor 0) (18/50)
[2024-10-25T12:37:08.447+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 20.0 in stage 11.0 (TID 79) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.448+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 18.0 in stage 11.0 (TID 77) in 20 ms on 172.18.0.5 (executor 0) (19/50)
[2024-10-25T12:37:08.461+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 21.0 in stage 11.0 (TID 80) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.462+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 19.0 in stage 11.0 (TID 78) in 20 ms on 172.18.0.5 (executor 0) (20/50)
[2024-10-25T12:37:08.466+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 22.0 in stage 11.0 (TID 81) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.467+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 20.0 in stage 11.0 (TID 79) in 20 ms on 172.18.0.5 (executor 0) (21/50)
[2024-10-25T12:37:08.482+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 23.0 in stage 11.0 (TID 82) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.483+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 21.0 in stage 11.0 (TID 80) in 23 ms on 172.18.0.5 (executor 0) (22/50)
[2024-10-25T12:37:08.487+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 24.0 in stage 11.0 (TID 83) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.488+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 22.0 in stage 11.0 (TID 81) in 23 ms on 172.18.0.5 (executor 0) (23/50)
[2024-10-25T12:37:08.500+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 25.0 in stage 11.0 (TID 84) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.501+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 23.0 in stage 11.0 (TID 82) in 20 ms on 172.18.0.5 (executor 0) (24/50)
[2024-10-25T12:37:08.505+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 26.0 in stage 11.0 (TID 85) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.506+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 24.0 in stage 11.0 (TID 83) in 19 ms on 172.18.0.5 (executor 0) (25/50)
[2024-10-25T12:37:08.520+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 27.0 in stage 11.0 (TID 86) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.521+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 25.0 in stage 11.0 (TID 84) in 21 ms on 172.18.0.5 (executor 0) (26/50)
[2024-10-25T12:37:08.528+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 28.0 in stage 11.0 (TID 87) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.529+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 26.0 in stage 11.0 (TID 85) in 23 ms on 172.18.0.5 (executor 0) (27/50)
[2024-10-25T12:37:08.547+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 29.0 in stage 11.0 (TID 88) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.548+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 27.0 in stage 11.0 (TID 86) in 28 ms on 172.18.0.5 (executor 0) (28/50)
[2024-10-25T12:37:08.555+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 30.0 in stage 11.0 (TID 89) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.556+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 28.0 in stage 11.0 (TID 87) in 28 ms on 172.18.0.5 (executor 0) (29/50)
[2024-10-25T12:37:08.575+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 31.0 in stage 11.0 (TID 90) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.577+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 29.0 in stage 11.0 (TID 88) in 30 ms on 172.18.0.5 (executor 0) (30/50)
[2024-10-25T12:37:08.579+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 32.0 in stage 11.0 (TID 91) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.581+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 30.0 in stage 11.0 (TID 89) in 26 ms on 172.18.0.5 (executor 0) (31/50)
[2024-10-25T12:37:08.601+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 33.0 in stage 11.0 (TID 92) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.603+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 34.0 in stage 11.0 (TID 93) (172.18.0.5, executor 0, partition 34, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.605+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 32.0 in stage 11.0 (TID 91) in 24 ms on 172.18.0.5 (executor 0) (32/50)
[2024-10-25T12:37:08.605+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 31.0 in stage 11.0 (TID 90) in 29 ms on 172.18.0.5 (executor 0) (33/50)
[2024-10-25T12:37:08.625+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 35.0 in stage 11.0 (TID 94) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.626+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 33.0 in stage 11.0 (TID 92) in 25 ms on 172.18.0.5 (executor 0) (34/50)
[2024-10-25T12:37:08.632+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 36.0 in stage 11.0 (TID 95) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.633+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 34.0 in stage 11.0 (TID 93) in 30 ms on 172.18.0.5 (executor 0) (35/50)
[2024-10-25T12:37:08.649+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 37.0 in stage 11.0 (TID 96) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.650+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 35.0 in stage 11.0 (TID 94) in 26 ms on 172.18.0.5 (executor 0) (36/50)
[2024-10-25T12:37:08.653+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 38.0 in stage 11.0 (TID 97) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.654+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 36.0 in stage 11.0 (TID 95) in 21 ms on 172.18.0.5 (executor 0) (37/50)
[2024-10-25T12:37:08.668+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 39.0 in stage 11.0 (TID 98) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.669+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 37.0 in stage 11.0 (TID 96) in 21 ms on 172.18.0.5 (executor 0) (38/50)
[2024-10-25T12:37:08.676+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 40.0 in stage 11.0 (TID 99) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.677+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 38.0 in stage 11.0 (TID 97) in 25 ms on 172.18.0.5 (executor 0) (39/50)
[2024-10-25T12:37:08.687+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 41.0 in stage 11.0 (TID 100) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.688+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 39.0 in stage 11.0 (TID 98) in 21 ms on 172.18.0.5 (executor 0) (40/50)
[2024-10-25T12:37:08.695+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 42.0 in stage 11.0 (TID 101) (172.18.0.5, executor 0, partition 42, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.696+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 40.0 in stage 11.0 (TID 99) in 20 ms on 172.18.0.5 (executor 0) (41/50)
[2024-10-25T12:37:08.709+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 43.0 in stage 11.0 (TID 102) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.710+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 41.0 in stage 11.0 (TID 100) in 22 ms on 172.18.0.5 (executor 0) (42/50)
[2024-10-25T12:37:08.715+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 44.0 in stage 11.0 (TID 103) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.716+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 42.0 in stage 11.0 (TID 101) in 21 ms on 172.18.0.5 (executor 0) (43/50)
[2024-10-25T12:37:08.730+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 45.0 in stage 11.0 (TID 104) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.731+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 43.0 in stage 11.0 (TID 102) in 22 ms on 172.18.0.5 (executor 0) (44/50)
[2024-10-25T12:37:08.738+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 46.0 in stage 11.0 (TID 105) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.739+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 44.0 in stage 11.0 (TID 103) in 24 ms on 172.18.0.5 (executor 0) (45/50)
[2024-10-25T12:37:08.750+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 47.0 in stage 11.0 (TID 106) (172.18.0.5, executor 0, partition 47, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.751+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 45.0 in stage 11.0 (TID 104) in 21 ms on 172.18.0.5 (executor 0) (46/50)
[2024-10-25T12:37:08.755+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 48.0 in stage 11.0 (TID 107) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.756+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 46.0 in stage 11.0 (TID 105) in 19 ms on 172.18.0.5 (executor 0) (47/50)
[2024-10-25T12:37:08.767+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Starting task 49.0 in stage 11.0 (TID 108) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7367 bytes)
[2024-10-25T12:37:08.768+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 47.0 in stage 11.0 (TID 106) in 19 ms on 172.18.0.5 (executor 0) (48/50)
[2024-10-25T12:37:08.773+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 48.0 in stage 11.0 (TID 107) in 18 ms on 172.18.0.5 (executor 0) (49/50)
[2024-10-25T12:37:08.784+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSetManager: Finished task 49.0 in stage 11.0 (TID 108) in 17 ms on 172.18.0.5 (executor 0) (50/50)
[2024-10-25T12:37:08.785+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2024-10-25T12:37:08.786+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO DAGScheduler: ResultStage 11 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.803 s
[2024-10-25T12:37:08.786+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:37:08.787+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2024-10-25T12:37:08.788+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO DAGScheduler: Job 6 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.811158 s
[2024-10-25T12:37:08.817+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO CodeGenerator: Code generated in 20.37756 ms
[2024-10-25T12:37:08.884+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO OptimisticTransaction: [tableId=27869bd8,txnId=61d67e1c] Attempting to commit version 2 with 5 actions with Serializable isolation level
[2024-10-25T12:37:08.900+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 84076e7a47fe:44219 in memory (size: 138.5 KiB, free: 434.3 MiB)
[2024-10-25T12:37:08.902+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.18.0.5:39707 in memory (size: 138.5 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:09.293+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DeltaLog: Creating a new snapshot v2 for commit version 2
[2024-10-25T12:37:09.293+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DeltaLog: Loading version 2.
[2024-10-25T12:37:09.298+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 3, totalFileSize: 5107)
[2024-10-25T12:37:09.368+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceStrategy: Pushed Filters:
[2024-10-25T12:37:09.369+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-25T12:37:09.392+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 205.1 KiB, free 433.5 MiB)
[2024-10-25T12:37:09.400+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.5 MiB)
[2024-10-25T12:37:09.402+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 84076e7a47fe:44219 (size: 36.1 KiB, free: 434.3 MiB)
[2024-10-25T12:37:09.403+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Created broadcast 11 from toString at String.java:2951
[2024-10-25T12:37:09.404+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6294009 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-25T12:37:09.419+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Starting job: toString at String.java:2951
[2024-10-25T12:37:09.421+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Got job 7 (toString at String.java:2951) with 2 output partitions
[2024-10-25T12:37:09.421+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Final stage: ResultStage 12 (toString at String.java:2951)
[2024-10-25T12:37:09.422+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Parents of final stage: List()
[2024-10-25T12:37:09.423+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:09.423+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[32] at toString at String.java:2951), which has no missing parents
[2024-10-25T12:37:09.425+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 50.4 KiB, free 433.4 MiB)
[2024-10-25T12:37:09.428+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 433.4 MiB)
[2024-10-25T12:37:09.429+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 84076e7a47fe:44219 (size: 15.5 KiB, free: 434.2 MiB)
[2024-10-25T12:37:09.430+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:09.431+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 12 (MapPartitionsRDD[32] at toString at String.java:2951) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:37:09.431+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSchedulerImpl: Adding task set 12.0 with 2 tasks resource profile 0
[2024-10-25T12:37:09.433+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 109) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8068 bytes)
[2024-10-25T12:37:09.433+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 110) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7950 bytes)
[2024-10-25T12:37:09.448+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.18.0.5:39707 (size: 15.5 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:09.467+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.18.0.5:39707 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:09.504+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 110) in 71 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:37:09.522+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 109) in 90 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:37:09.524+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2024-10-25T12:37:09.524+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: ResultStage 12 (toString at String.java:2951) finished in 0.100 s
[2024-10-25T12:37:09.525+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:37:09.526+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2024-10-25T12:37:09.527+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Job 7 finished: toString at String.java:2951, took 0.105139 s
[2024-10-25T12:37:09.538+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO Snapshot: [tableId=27869bd8-0dad-499e-87a1-62767741a16b] Created snapshot Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=2, metadata=Metadata(27869bd8-0dad-499e-87a1-62767741a16b,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":"string","nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}}]},List(),Map(),Some(1729859584322)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,2,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=1749; replication=1; blocksize=33554432; modification_time=1729859601578; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=0fcd107ecb55a2da324d6d2253a4bc8e versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1679; replication=1; blocksize=33554432; modification_time=1729859662749; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=72195ab0cb0a49898cfc56a271971e2b versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000002.json; isDirectory=false; length=1679; replication=1; blocksize=33554432; modification_time=1729859829225; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1bf51531e6702ab2f46cf547d979e1fe versionId=null),None,1729859829225), checksumOpt=None)
[2024-10-25T12:37:09.539+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DeltaLog: Updated snapshot to Snapshot(path=s3a://lakehouse/sliver/credit/_delta_log, version=2, metadata=Metadata(27869bd8-0dad-499e-87a1-62767741a16b,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"cast","type":"string","nullable":true,"metadata":{}},{"name":"crew","type":"string","nullable":true,"metadata":{}},{"name":"id","type":"long","nullable":true,"metadata":{}}]},List(),Map(),Some(1729859584322)), logSegment=LogSegment(s3a://lakehouse/sliver/credit/_delta_log,2,WrappedArray(S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000000.json; isDirectory=false; length=1749; replication=1; blocksize=33554432; modification_time=1729859601578; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=0fcd107ecb55a2da324d6d2253a4bc8e versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000001.json; isDirectory=false; length=1679; replication=1; blocksize=33554432; modification_time=1729859662749; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=72195ab0cb0a49898cfc56a271971e2b versionId=null, S3AFileStatus{path=s3a://lakehouse/sliver/credit/_delta_log/00000000000000000002.json; isDirectory=false; length=1679; replication=1; blocksize=33554432; modification_time=1729859829225; access_time=0; owner=***; group=***; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=true; isErasureCoded=false} isEmptyDirectory=FALSE eTag=1bf51531e6702ab2f46cf547d979e1fe versionId=null),None,1729859829225), checksumOpt=None)
[2024-10-25T12:37:09.541+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MapPartitionsRDD: Removing RDD 20 from persistence list
[2024-10-25T12:37:09.552+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManager: Removing RDD 20
[2024-10-25T12:37:09.554+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO Snapshot: [tableId=27869bd8-0dad-499e-87a1-62767741a16b] DELTA: Compute snapshot for version: 2
[2024-10-25T12:37:09.561+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 204.8 KiB, free 433.2 MiB)
[2024-10-25T12:37:09.572+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.2 MiB)
[2024-10-25T12:37:09.575+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 84076e7a47fe:44219 (size: 36.1 KiB, free: 434.2 MiB)
[2024-10-25T12:37:09.576+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Created broadcast 13 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:09.645+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 84076e7a47fe:44219 in memory (size: 36.1 KiB, free: 434.2 MiB)
[2024-10-25T12:37:09.648+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.18.0.5:39707 in memory (size: 36.1 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:09.656+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 84076e7a47fe:44219 in memory (size: 15.5 KiB, free: 434.3 MiB)
[2024-10-25T12:37:09.659+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.18.0.5:39707 in memory (size: 15.5 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:09.884+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceStrategy: Pushed Filters:
[2024-10-25T12:37:09.885+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceStrategy: Post-Scan Filters:
[2024-10-25T12:37:09.946+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 205.1 KiB, free 433.3 MiB)
[2024-10-25T12:37:09.952+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 433.2 MiB)
[2024-10-25T12:37:09.953+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 84076e7a47fe:44219 (size: 36.1 KiB, free: 434.2 MiB)
[2024-10-25T12:37:09.954+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Created broadcast 14 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:09.955+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 6294009 bytes, open cost is considered as scanning 4194304 bytes.
[2024-10-25T12:37:09.967+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Registering RDD 36 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 3
[2024-10-25T12:37:09.968+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Got map stage job 8 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 2 output partitions
[2024-10-25T12:37:09.969+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Final stage: ShuffleMapStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:09.970+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Parents of final stage: List()
[2024-10-25T12:37:09.970+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:09.971+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[36] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:09.977+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 138.3 KiB, free 433.1 MiB)
[2024-10-25T12:37:09.978+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 39.0 KiB, free 433.0 MiB)
[2024-10-25T12:37:09.979+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 84076e7a47fe:44219 (size: 39.0 KiB, free: 434.2 MiB)
[2024-10-25T12:37:09.980+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:09.980+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[36] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1))
[2024-10-25T12:37:09.981+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSchedulerImpl: Adding task set 13.0 with 2 tasks resource profile 0
[2024-10-25T12:37:09.982+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 111) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 8057 bytes)
[2024-10-25T12:37:09.982+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 112) (172.18.0.5, executor 0, partition 1, PROCESS_LOCAL, 7939 bytes)
[2024-10-25T12:37:09.991+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.18.0.5:39707 (size: 39.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:10.017+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.18.0.5:39707 (size: 36.1 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:10.049+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 112) in 68 ms on 172.18.0.5 (executor 0) (1/2)
[2024-10-25T12:37:10.063+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 111) in 82 ms on 172.18.0.5 (executor 0) (2/2)
[2024-10-25T12:37:10.064+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2024-10-25T12:37:10.064+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: ShuffleMapStage 13 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.091 s
[2024-10-25T12:37:10.065+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: looking for newly runnable stages
[2024-10-25T12:37:10.066+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: running: Set()
[2024-10-25T12:37:10.066+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: waiting: Set()
[2024-10-25T12:37:10.067+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: failed: Set()
[2024-10-25T12:37:10.106+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 84076e7a47fe:44219 in memory (size: 39.0 KiB, free: 434.2 MiB)
[2024-10-25T12:37:10.107+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.18.0.5:39707 in memory (size: 39.0 KiB, free: 1048.7 MiB)
[2024-10-25T12:37:10.252+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Registering RDD 46 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) as input to shuffle 4
[2024-10-25T12:37:10.253+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Got map stage job 9 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 50 output partitions
[2024-10-25T12:37:10.253+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Final stage: ShuffleMapStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:10.254+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)
[2024-10-25T12:37:10.257+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:10.258+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[46] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:10.325+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 517.1 KiB, free 432.7 MiB)
[2024-10-25T12:37:10.328+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 123.4 KiB, free 432.6 MiB)
[2024-10-25T12:37:10.328+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 84076e7a47fe:44219 (size: 123.4 KiB, free: 434.1 MiB)
[2024-10-25T12:37:10.329+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:10.330+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO DAGScheduler: Submitting 50 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[46] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-10-25T12:37:10.330+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSchedulerImpl: Adding task set 15.0 with 50 tasks resource profile 0
[2024-10-25T12:37:10.332+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 113) (172.18.0.5, executor 0, partition 1, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.332+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 6.0 in stage 15.0 (TID 114) (172.18.0.5, executor 0, partition 6, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.342+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.18.0.5:39707 (size: 123.4 KiB, free: 1048.6 MiB)
[2024-10-25T12:37:10.356+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.18.0.5:35124
[2024-10-25T12:37:10.404+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_6 in memory on 172.18.0.5:39707 (size: 303.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.405+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_1 in memory on 172.18.0.5:39707 (size: 302.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.426+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 9.0 in stage 15.0 (TID 115) (172.18.0.5, executor 0, partition 9, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.427+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 6.0 in stage 15.0 (TID 114) in 95 ms on 172.18.0.5 (executor 0) (1/50)
[2024-10-25T12:37:10.429+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 34.0 in stage 15.0 (TID 116) (172.18.0.5, executor 0, partition 34, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.430+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 113) in 98 ms on 172.18.0.5 (executor 0) (2/50)
[2024-10-25T12:37:10.482+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_9 in memory on 172.18.0.5:39707 (size: 487.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.483+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_34 in memory on 172.18.0.5:39707 (size: 482.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.503+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 42.0 in stage 15.0 (TID 117) (172.18.0.5, executor 0, partition 42, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.504+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 34.0 in stage 15.0 (TID 116) in 76 ms on 172.18.0.5 (executor 0) (3/50)
[2024-10-25T12:37:10.507+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 47.0 in stage 15.0 (TID 118) (172.18.0.5, executor 0, partition 47, NODE_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.508+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 9.0 in stage 15.0 (TID 115) in 81 ms on 172.18.0.5 (executor 0) (4/50)
[2024-10-25T12:37:10.536+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_42 in memory on 172.18.0.5:39707 (size: 595.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.541+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_47 in memory on 172.18.0.5:39707 (size: 302.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.553+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 119) (172.18.0.5, executor 0, partition 0, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.554+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 42.0 in stage 15.0 (TID 117) in 51 ms on 172.18.0.5 (executor 0) (5/50)
[2024-10-25T12:37:10.559+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 2.0 in stage 15.0 (TID 120) (172.18.0.5, executor 0, partition 2, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.560+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 47.0 in stage 15.0 (TID 118) in 52 ms on 172.18.0.5 (executor 0) (6/50)
[2024-10-25T12:37:10.587+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_0 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.594+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_2 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.605+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 3.0 in stage 15.0 (TID 121) (172.18.0.5, executor 0, partition 3, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.606+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 119) in 52 ms on 172.18.0.5 (executor 0) (7/50)
[2024-10-25T12:37:10.614+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 4.0 in stage 15.0 (TID 122) (172.18.0.5, executor 0, partition 4, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.615+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 2.0 in stage 15.0 (TID 120) in 56 ms on 172.18.0.5 (executor 0) (8/50)
[2024-10-25T12:37:10.642+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_3 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.658+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_4 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.665+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 5.0 in stage 15.0 (TID 123) (172.18.0.5, executor 0, partition 5, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.666+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 3.0 in stage 15.0 (TID 121) in 61 ms on 172.18.0.5 (executor 0) (9/50)
[2024-10-25T12:37:10.679+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 7.0 in stage 15.0 (TID 124) (172.18.0.5, executor 0, partition 7, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.680+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 4.0 in stage 15.0 (TID 122) in 65 ms on 172.18.0.5 (executor 0) (10/50)
[2024-10-25T12:37:10.711+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_5 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.728+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_7 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.740+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 8.0 in stage 15.0 (TID 125) (172.18.0.5, executor 0, partition 8, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.741+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 5.0 in stage 15.0 (TID 123) in 77 ms on 172.18.0.5 (executor 0) (11/50)
[2024-10-25T12:37:10.749+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 10.0 in stage 15.0 (TID 126) (172.18.0.5, executor 0, partition 10, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.750+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 7.0 in stage 15.0 (TID 124) in 72 ms on 172.18.0.5 (executor 0) (12/50)
[2024-10-25T12:37:10.783+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_8 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.791+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_10 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.811+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 11.0 in stage 15.0 (TID 127) (172.18.0.5, executor 0, partition 11, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.812+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 8.0 in stage 15.0 (TID 125) in 72 ms on 172.18.0.5 (executor 0) (13/50)
[2024-10-25T12:37:10.815+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 12.0 in stage 15.0 (TID 128) (172.18.0.5, executor 0, partition 12, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.816+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 10.0 in stage 15.0 (TID 126) in 66 ms on 172.18.0.5 (executor 0) (14/50)
[2024-10-25T12:37:10.860+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_12 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.861+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_11 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.883+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 13.0 in stage 15.0 (TID 129) (172.18.0.5, executor 0, partition 13, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.885+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 14.0 in stage 15.0 (TID 130) (172.18.0.5, executor 0, partition 14, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.886+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 12.0 in stage 15.0 (TID 128) in 71 ms on 172.18.0.5 (executor 0) (15/50)
[2024-10-25T12:37:10.887+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 11.0 in stage 15.0 (TID 127) in 75 ms on 172.18.0.5 (executor 0) (16/50)
[2024-10-25T12:37:10.922+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_13 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.926+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_14 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.943+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 15.0 in stage 15.0 (TID 131) (172.18.0.5, executor 0, partition 15, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.943+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 13.0 in stage 15.0 (TID 129) in 60 ms on 172.18.0.5 (executor 0) (17/50)
[2024-10-25T12:37:10.947+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Starting task 16.0 in stage 15.0 (TID 132) (172.18.0.5, executor 0, partition 16, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:10.948+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO TaskSetManager: Finished task 14.0 in stage 15.0 (TID 130) in 63 ms on 172.18.0.5 (executor 0) (18/50)
[2024-10-25T12:37:10.982+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_15 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:10.986+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:10 INFO BlockManagerInfo: Added rdd_43_16 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.011+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 17.0 in stage 15.0 (TID 133) (172.18.0.5, executor 0, partition 17, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.011+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 15.0 in stage 15.0 (TID 131) in 69 ms on 172.18.0.5 (executor 0) (19/50)
[2024-10-25T12:37:11.016+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 18.0 in stage 15.0 (TID 134) (172.18.0.5, executor 0, partition 18, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.017+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 16.0 in stage 15.0 (TID 132) in 70 ms on 172.18.0.5 (executor 0) (20/50)
[2024-10-25T12:37:11.055+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_17 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.058+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_18 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.072+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 19.0 in stage 15.0 (TID 135) (172.18.0.5, executor 0, partition 19, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.073+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 17.0 in stage 15.0 (TID 133) in 63 ms on 172.18.0.5 (executor 0) (21/50)
[2024-10-25T12:37:11.076+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 20.0 in stage 15.0 (TID 136) (172.18.0.5, executor 0, partition 20, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.077+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 18.0 in stage 15.0 (TID 134) in 60 ms on 172.18.0.5 (executor 0) (22/50)
[2024-10-25T12:37:11.109+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_19 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.111+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_20 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.127+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 21.0 in stage 15.0 (TID 137) (172.18.0.5, executor 0, partition 21, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.128+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 19.0 in stage 15.0 (TID 135) in 56 ms on 172.18.0.5 (executor 0) (23/50)
[2024-10-25T12:37:11.129+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 22.0 in stage 15.0 (TID 138) (172.18.0.5, executor 0, partition 22, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.130+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 20.0 in stage 15.0 (TID 136) in 54 ms on 172.18.0.5 (executor 0) (24/50)
[2024-10-25T12:37:11.164+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_22 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.166+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_21 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.182+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 23.0 in stage 15.0 (TID 139) (172.18.0.5, executor 0, partition 23, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.183+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 22.0 in stage 15.0 (TID 138) in 55 ms on 172.18.0.5 (executor 0) (25/50)
[2024-10-25T12:37:11.184+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 24.0 in stage 15.0 (TID 140) (172.18.0.5, executor 0, partition 24, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.185+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 21.0 in stage 15.0 (TID 137) in 58 ms on 172.18.0.5 (executor 0) (26/50)
[2024-10-25T12:37:11.219+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_24 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.222+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_23 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.238+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 25.0 in stage 15.0 (TID 141) (172.18.0.5, executor 0, partition 25, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.239+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 24.0 in stage 15.0 (TID 140) in 55 ms on 172.18.0.5 (executor 0) (27/50)
[2024-10-25T12:37:11.242+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 26.0 in stage 15.0 (TID 142) (172.18.0.5, executor 0, partition 26, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.243+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 23.0 in stage 15.0 (TID 139) in 61 ms on 172.18.0.5 (executor 0) (28/50)
[2024-10-25T12:37:11.272+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_25 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.274+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_26 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.292+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 27.0 in stage 15.0 (TID 143) (172.18.0.5, executor 0, partition 27, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.293+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 26.0 in stage 15.0 (TID 142) in 50 ms on 172.18.0.5 (executor 0) (29/50)
[2024-10-25T12:37:11.293+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 28.0 in stage 15.0 (TID 144) (172.18.0.5, executor 0, partition 28, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.294+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 25.0 in stage 15.0 (TID 141) in 55 ms on 172.18.0.5 (executor 0) (30/50)
[2024-10-25T12:37:11.328+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_28 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.330+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_27 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.355+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 29.0 in stage 15.0 (TID 145) (172.18.0.5, executor 0, partition 29, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.356+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 28.0 in stage 15.0 (TID 144) in 63 ms on 172.18.0.5 (executor 0) (31/50)
[2024-10-25T12:37:11.359+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 30.0 in stage 15.0 (TID 146) (172.18.0.5, executor 0, partition 30, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.360+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 27.0 in stage 15.0 (TID 143) in 67 ms on 172.18.0.5 (executor 0) (32/50)
[2024-10-25T12:37:11.400+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_29 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.402+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_30 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.420+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 31.0 in stage 15.0 (TID 147) (172.18.0.5, executor 0, partition 31, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.421+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 29.0 in stage 15.0 (TID 145) in 66 ms on 172.18.0.5 (executor 0) (33/50)
[2024-10-25T12:37:11.424+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 32.0 in stage 15.0 (TID 148) (172.18.0.5, executor 0, partition 32, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.425+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 30.0 in stage 15.0 (TID 146) in 66 ms on 172.18.0.5 (executor 0) (34/50)
[2024-10-25T12:37:11.460+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_32 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.466+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_31 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.480+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 33.0 in stage 15.0 (TID 149) (172.18.0.5, executor 0, partition 33, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.481+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 32.0 in stage 15.0 (TID 148) in 56 ms on 172.18.0.5 (executor 0) (35/50)
[2024-10-25T12:37:11.485+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 35.0 in stage 15.0 (TID 150) (172.18.0.5, executor 0, partition 35, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.486+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 31.0 in stage 15.0 (TID 147) in 66 ms on 172.18.0.5 (executor 0) (36/50)
[2024-10-25T12:37:11.516+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_33 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.518+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_35 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.533+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 36.0 in stage 15.0 (TID 151) (172.18.0.5, executor 0, partition 36, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.534+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 33.0 in stage 15.0 (TID 149) in 55 ms on 172.18.0.5 (executor 0) (37/50)
[2024-10-25T12:37:11.537+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 37.0 in stage 15.0 (TID 152) (172.18.0.5, executor 0, partition 37, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.538+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 35.0 in stage 15.0 (TID 150) in 52 ms on 172.18.0.5 (executor 0) (38/50)
[2024-10-25T12:37:11.567+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_36 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.569+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_37 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.584+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 38.0 in stage 15.0 (TID 153) (172.18.0.5, executor 0, partition 38, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.584+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 36.0 in stage 15.0 (TID 151) in 51 ms on 172.18.0.5 (executor 0) (39/50)
[2024-10-25T12:37:11.587+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 39.0 in stage 15.0 (TID 154) (172.18.0.5, executor 0, partition 39, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.588+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 37.0 in stage 15.0 (TID 152) in 50 ms on 172.18.0.5 (executor 0) (40/50)
[2024-10-25T12:37:11.616+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_38 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.623+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_39 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.634+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 40.0 in stage 15.0 (TID 155) (172.18.0.5, executor 0, partition 40, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.635+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 38.0 in stage 15.0 (TID 153) in 51 ms on 172.18.0.5 (executor 0) (41/50)
[2024-10-25T12:37:11.641+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 41.0 in stage 15.0 (TID 156) (172.18.0.5, executor 0, partition 41, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.642+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 39.0 in stage 15.0 (TID 154) in 55 ms on 172.18.0.5 (executor 0) (42/50)
[2024-10-25T12:37:11.678+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_40 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.684+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_41 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.694+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 43.0 in stage 15.0 (TID 157) (172.18.0.5, executor 0, partition 43, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.694+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 40.0 in stage 15.0 (TID 155) in 60 ms on 172.18.0.5 (executor 0) (43/50)
[2024-10-25T12:37:11.700+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 44.0 in stage 15.0 (TID 158) (172.18.0.5, executor 0, partition 44, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.701+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 41.0 in stage 15.0 (TID 156) in 60 ms on 172.18.0.5 (executor 0) (44/50)
[2024-10-25T12:37:11.740+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_43 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.747+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_44 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.758+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 45.0 in stage 15.0 (TID 159) (172.18.0.5, executor 0, partition 45, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.759+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 43.0 in stage 15.0 (TID 157) in 66 ms on 172.18.0.5 (executor 0) (45/50)
[2024-10-25T12:37:11.764+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 46.0 in stage 15.0 (TID 160) (172.18.0.5, executor 0, partition 46, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.765+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 44.0 in stage 15.0 (TID 158) in 64 ms on 172.18.0.5 (executor 0) (46/50)
[2024-10-25T12:37:11.789+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_45 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.797+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_46 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.805+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 48.0 in stage 15.0 (TID 161) (172.18.0.5, executor 0, partition 48, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.806+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 45.0 in stage 15.0 (TID 159) in 48 ms on 172.18.0.5 (executor 0) (47/50)
[2024-10-25T12:37:11.814+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 49.0 in stage 15.0 (TID 162) (172.18.0.5, executor 0, partition 49, PROCESS_LOCAL, 7356 bytes)
[2024-10-25T12:37:11.815+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 46.0 in stage 15.0 (TID 160) in 51 ms on 172.18.0.5 (executor 0) (48/50)
[2024-10-25T12:37:11.838+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_48 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.846+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added rdd_43_49 in memory on 172.18.0.5:39707 (size: 46.0 B, free: 1048.6 MiB)
[2024-10-25T12:37:11.854+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 48.0 in stage 15.0 (TID 161) in 49 ms on 172.18.0.5 (executor 0) (49/50)
[2024-10-25T12:37:11.863+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 49.0 in stage 15.0 (TID 162) in 49 ms on 172.18.0.5 (executor 0) (50/50)
[2024-10-25T12:37:11.864+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2024-10-25T12:37:11.864+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: ShuffleMapStage 15 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 1.601 s
[2024-10-25T12:37:11.865+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: looking for newly runnable stages
[2024-10-25T12:37:11.866+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: running: Set()
[2024-10-25T12:37:11.866+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: waiting: Set()
[2024-10-25T12:37:11.867+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: failed: Set()
[2024-10-25T12:37:11.889+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128
[2024-10-25T12:37:11.891+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Got job 10 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) with 1 output partitions
[2024-10-25T12:37:11.891+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Final stage: ResultStage 18 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128)
[2024-10-25T12:37:11.892+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)
[2024-10-25T12:37:11.893+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Missing parents: List()
[2024-10-25T12:37:11.893+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[49] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128), which has no missing parents
[2024-10-25T12:37:11.897+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 452.3 KiB, free 432.2 MiB)
[2024-10-25T12:37:11.900+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 432.0 MiB)
[2024-10-25T12:37:11.901+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 84076e7a47fe:44219 (size: 108.1 KiB, free: 434.0 MiB)
[2024-10-25T12:37:11.901+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1540
[2024-10-25T12:37:11.902+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[49] at $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) (first 15 tasks are for partitions Vector(0))
[2024-10-25T12:37:11.903+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2024-10-25T12:37:11.903+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 163) (172.18.0.5, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2024-10-25T12:37:11.913+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.18.0.5:39707 (size: 108.1 KiB, free: 1048.5 MiB)
[2024-10-25T12:37:11.923+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.18.0.5:35124
[2024-10-25T12:37:11.950+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 163) in 47 ms on 172.18.0.5 (executor 0) (1/1)
[2024-10-25T12:37:11.951+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2024-10-25T12:37:11.951+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: ResultStage 18 ($anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128) finished in 0.057 s
[2024-10-25T12:37:11.952+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2024-10-25T12:37:11.953+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2024-10-25T12:37:11.953+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO DAGScheduler: Job 10 finished: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:128, took 0.061842 s
[2024-10-25T12:37:11.961+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:11 INFO Snapshot: [tableId=27869bd8-0dad-499e-87a1-62767741a16b] DELTA: Done
[2024-10-25T12:37:12.008+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO OptimisticTransaction: [tableId=27869bd8,txnId=61d67e1c] Committed delta #2 to s3a://lakehouse/sliver/credit/_delta_log
[2024-10-25T12:37:12.014+0000] {spark_submit.py:579} INFO - 2024-10-25 12:37:12,014 INFO: Data cleaning and saving process completed successfully.
[2024-10-25T12:37:12.015+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-10-25T12:37:12.027+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO SparkUI: Stopped Spark web UI at http://84076e7a47fe:4040
[2024-10-25T12:37:12.033+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO StandaloneSchedulerBackend: Shutting down all executors
[2024-10-25T12:37:12.034+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2024-10-25T12:37:12.060+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-10-25T12:37:12.078+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO MemoryStore: MemoryStore cleared
[2024-10-25T12:37:12.078+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO BlockManager: BlockManager stopped
[2024-10-25T12:37:12.084+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-10-25T12:37:12.094+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-10-25T12:37:12.136+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO SparkContext: Successfully stopped SparkContext
[2024-10-25T12:37:12.375+0000] {spark_submit.py:579} INFO - 2024-10-25 12:37:12,374 INFO: Closing down clientserver connection
[2024-10-25T12:37:12.465+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO ShutdownHookManager: Shutdown hook called
[2024-10-25T12:37:12.466+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed1774e3-c0b6-43e5-a18b-cd4a7955d310
[2024-10-25T12:37:12.470+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac63235e-418e-4c0e-85e8-07e5750958c2/pyspark-3ad9aab9-0177-404d-983b-06b8150308a3
[2024-10-25T12:37:12.475+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac63235e-418e-4c0e-85e8-07e5750958c2
[2024-10-25T12:37:12.483+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2024-10-25T12:37:12.484+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2024-10-25T12:37:12.484+0000] {spark_submit.py:579} INFO - 24/10/25 12:37:12 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2024-10-25T12:37:12.554+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=task_credits_clean, task_id=credits_cleaned, execution_date=20241025T123632, start_date=20241025T123634, end_date=20241025T123712
[2024-10-25T12:37:12.600+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2024-10-25T12:37:12.615+0000] {taskinstance.py:2776} INFO - 0 downstream tasks scheduled from follow-on schedule check
